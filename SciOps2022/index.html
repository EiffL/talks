<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Going Beyond Common Deep Learning Limitations with Deep Probabilistic Modeling</title>

	<meta name="description" content="SciOps2022, May 20th 2022">
	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<link rel="stylesheet" href="reveal.js/dist/theme/darkenergy.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section data-background-iframe="background.html">
				<div class="container">
					<div class="title" style="border-radius: 20px; background-color:rgba(0, 0, 0, 0.4);">
						<h1>Going Beyond Common Deep Learning Limitations with Deep
Probabilistic Modeling</h1>
					</div>
				</div>
				<hr>
				<div style="border-radius: 20px; background-color:rgba(0, 0, 0, 0);">
					<div class="container">
						<div class="col">
							<div align="left" style="margin-left: 20px;">
								<h2>François Lanusse</h2>
								<br>
								<img src="assets/CosmoStatDarkBK.png" class="plain"></img>
								<br>
							</div>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<br>
							<img src="assets/logo_cnrs.png" class="plain" height="150"></img>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<img src="assets/aim.png" class="plain" height="150"></img>
						</div>
					</div>
					<div> slides at <a href="https://eiffl.github.io/SciOps2022">eiffl.github.io/SciOps2022</a> </div>
				</div>
			</section>

						<!-- <section>
			        <section data-background="assets/gal_hsc.png">
								<div class="fragment">
									<div style="float:right; font-size: 20px">Branched GAN model for deblending <a href="https://arxiv.org/abs/1810.10098">(Reiman & Göhre, 2018)</a></div>

									<img class="plain" data-src="assets/Reiman2018_1.png" />
								</div>

								<div class="block fragment">
									<div class="block-title">
										The issue with using deep learning as a <i>black-box</i>
									</div>
									<div class="block-content">
										<ul>
											<li> No explicit control of noise, PSF, number of sources.
												<ul>
													<li> Model would have to be retrained for all observing configurations
													</li>
												</ul>
											</li>
											<br>
											<li class="fragment"> No guarantees on the network output (e.g. flux preservation, artifacts)
											</li>
											<br>
											<li class="fragment"> No proper uncertainty quantification.
											</li>
										</ul>
									</div>
								</div>
							</section>

							<section>
								<img class="plain" data-src="assets/Reiman2018_3.png" />
							</section>
						</section>

			      			<section class="inverted" data-background="#000">
			      				<h2>Can we understand how a Neural Network solves an Inverse Problem?</h2>
			      			</section> -->

			      			<section>
			      				<section>
			      					<h3 class="slide-title" style="position:absolute;top:0;">A Motivating Example: Image Deconvolution</h3>
			      					<br>
			      					<br>

			      					$ y = P \ast x + n $

			      					<div class="container">

			      						<div class="col">
			      							<p> <b class="alert"> Observed $y$</b></p>
			      							<img class="plain" data-src="assets/cosmos_gal_ground.png" style="width: 250px" />
			      							<br>Ground-Based Telescope
			      						</div>

			      						<div class="col fragment fade-up" data-fragment-index='0'>
			      							<p> <b class="alert">$f_\theta$</b> </p>
			      							<img class="plain " data-src="assets/generic_network_inv.png" style="height: 250px; width:500px" />
			      							<br>some deep Convolutional Neural Network
			      						</div>

			      						<div class="col">
			      							<p><b class="alert"> Unknown $x$</b> </p>
			      							<img class="plain" data-src="assets/cosmos_gal.png" style="width: 250px" />
			      							<br>Hubble Space Telescope
			      						</div>
			      					</div>
			      					<br>
			      					<br>
			      					<ul>
			      						<li class="fragment fade-up" data-fragment-index='0'> A standard approach would be to train a neural network $f_\theta$ to <b class="alert">estimate $x$ given $y$</b>.
			      						</li>
			      					</ul>
			      				</section>

			      				<section>
			      					<ul>
			      						<li> <i>Step I</i>: Assemble from <b>data</b> or <b>simulations</b> a <b class="alert">training set</b> of images
			      							$$\mathcal{D} = \{(x_0, y_0),
			      							(x_1, y_1), \ldots, (x_N, y_N) \}$$
			      							$\Longrightarrow$ the dataset contains <b class="alert">hardcoded assumptions</b> about PSF $P$
			      							noise $n$, and galaxy morphology $x$.
			      						</li>
			      						<li class="fragment fade-up"> Step II: Train the neural network $f_\theta$ under a <b class="alert">regression loss</b> of the type:
			      							$$ \mathcal{L} = \sum_{i=0}^N \parallel x_i - f_\theta(y_i)\parallel^2 $$
			      						</li>
			      					</ul>
			      					<div class="container fragment">
			      						<div class="col">
			      							<img class="plain" data-src="assets/cosmos_gal_ground.png" style="width: 250px" />
			      							<p>$$ y $$</p>
			      						</div>

			      						<div class="col">
			      							<img class="plain " data-src="assets/generic_network_inv.png" style="height: 250px; width:500px" />
			      							<p>$$f_\theta$$</p>
			      						</div>

			      						<div class="col">
			      							<img class="plain" data-src="assets/rec_median.png" style="width: 250px" />
			      							$$f_\theta(y)$$
			      						</div>

			      						<div class="col fragment fade-up" style="float:center;">
			      							<img class="plain" data-src="assets/cosmos_gal.png" style="width: 250px" />
			      							<br>
			      							<p>$$ \mbox{True } x $$</p>
			      						</div>
			      					</div>
			      					<div class="fragment">$\Longrightarrow$Why is the network output different from the truth? If it's not the truth, then <b>what is $f_\theta(y)$?</b></div>
			      				</section>

			      				<section>
			      					<p>Let's try to understand the neural network output by looking at the <b class="alert">loss function</b></p>
			      					$$ \mathcal{L} = \sum_{(x_i, y_i) \in \mathcal{D}} \parallel x_i - f_\theta(y_i)\parallel^2 \quad \simeq \quad \int \parallel x - f_\theta(y) \parallel^2 \ p(x,y) \ dx dy $$

			      					<div class="fragment" data-fragment-index="1">$$\Longrightarrow \int \left[ \int \parallel x - f_\theta(y) \parallel^2 \ p(x|y) \ dx \right] p(y) dy $$ </div>

			      					<div class="block fragment" data-fragment-index="2">
			      						<div class="block-content">
			      							$\mathcal{L}$ minimized when $f_{\theta^\star}(y) = \int x \ p(x|y) \ dx $, i.e.
			      							when <b class="alert">$f_{\theta^\star}(x)$ is predicting the mean of $p(x|y)$</b>.
			      						</div>
			      					</div>
			      					<div class="container">
			      						<div class="col">
			      							<div style="position:relative; width:500px; height:500px; margin:0 auto;">
			      								<img class="fragment current-visible plain" data-src="assets/nn_l2.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="3" />
			      								<img class="fragment current-visible plain" data-src="assets/nn_l2_mean.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="4" />
			      								<img class="fragment current-visible plain" data-src="assets/nn_l1.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="5" />
			      								<img class="fragment plain" data-src="assets/nn_l1_median.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="6" />
			      							</div>
			      						</div>
			      						<div class="col">
			      							<ul>
			      								<li class="fragment" data-fragment-index="3"> Using an <b class="alert">$\ell_2$ loss learns the mean</b> of the $p(x | y)$
			      								</li>
			      								<br>
			      								<li class="fragment" data-fragment-index="5"> Using an <b class="alert">$\ell_1$ loss learns the median</b> of $p(x|y)$
			      								</li>
			      								<br>
			      								<li class="fragment" data-fragment-index="7"> In general, training a neural network for regression doesn't
			      									achieve de mode of the distribution.<br>
			      									<br>
			      									<div style='vertical-align:middle; display:inline;'>Check this <a href="https://medium.com/cosmostat/regression-in-the-presence-of-uncertainties-with-tensorflow-probability-1b7449f1083b" target="blank_">blogpost</a> and this
			      										notebook to learn how to do that: <a href=" https://colab.research.google.com/drive/1yi_BY09LCS8qHCfJqvCIftKuW6jNe-t1" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"
			      												class="plain" style="height:25px;vertical-align:middle; display:inline;" /></a></div>
			      								</li>
			      							</ul>
			      						</div>
			      					</div>
			      				</section>
			      			</section>

			      			<section>
			      				<h3 class="slide-title">A Bayesian understanding of a regression network</h3>
			      				<div class="container">
			      					<div class="col">
			      						<div style="position:relative; width:200px; height:200px; margin:0 auto;">
			      							<img class="plain" data-src="assets/cosmos_gal_ground.png" style="position:absolute;top:0;left:0;width:200px;" />
			      						</div>
			      						<div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
			      							<div class='col ' style="position:absolute;top:0;left:0;width:200px;"> Data $y$</div>
			      						</div>
			      						<br>
			      					</div>

			      					<div class="col" data-fragment-index='0'>
			      						<div style="position:relative; width:200px; height:200px; margin:0 auto;">
			      							<div><video data-autoplay data-loop data-src="assets/rec_samples.mp4" type="video/mp4" style="height: 200px;" />
			      							</div>
			      						</div>
			      						<div>Posterior samples</div>
			      					</div>

			      					<div class="col">
			      						<div style="position:relative; width:200px; height:200px; margin:0 auto;">
			      							<img class="plain " data-src="assets/rec_median.png" style="position:absolute;top:0;left:0;width:200px;" />
			      						</div>
			      						<div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
			      							<div class='col' style="position:absolute;top:0;left:0;width:200px;">Posterior mean</div>
			      						</div>
			      					</div>

			      					<div class="col">
			      						<div style="position:relative; width:200px; height:200px; margin:0 auto;">
			      							<img class="plain " data-src="assets/cosmos_gal.png" style="position:absolute;top:0;left:0;width:200px;" />
			      						</div>

			      						<div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
			      							<div class='col' data-fragment-index='1' style="position:absolute;top:0;left:0;width:200px;"> True $x$</div>
			      						</div>
			      					</div>
			      				</div>
			      				<br>
			      				<ul>
			      					<li> The distribution $p(x|y)$ can be understood as a <b>Bayesian posterior distribution</b>:
			      						$$ p(x | y) \propto \underbrace{p(y | x)}_{\mbox{likelihood}} \quad \underbrace{p(x)}_{\mbox{prior}} $$
			      					</li>
			      					<br>
			      					<li class="fragment"> Both priors and likelihoods are <b>learned implicitly</b> by the neural network from the training set.
			      						<br>$\Longrightarrow$ <b class="alert">priors AND likelihoods are hardcoded</b> in the training set.
			      					</li>
			                <br>
			                <li class="fragment"> The network only returns a <b>point summary</b> of this posterior distribution (<b class="alert">no uncertainty quantification</b>).
			                </li>
			      				</ul>
			      			</section>


											<section>

																	     <section>
																	        <h3 class="slide-title" style="position:absolute;top:0;">A Physicist's approach: let's build a model</h3>
																					<div class="container">
																						<div class="col">
																							<div style="float:right; font-size: 20px"> <b>Lanusse</b> et al. (2020) <a href="https://arxiv.org/abs/2008.03833"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2008.03833-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
																						</div>
																					</div>
																	        <div class="container">
																	          <div class="col">
																	              <img class="plain fragment" data-src="assets/rand_z_square.png" style="height: 150px" data-fragment-index="4"/>
																	          </div>
																	          <div class="col">
																	              <img class="plain fragment" data-src="assets/cosmos_gal.png" style="width: 200px" data-fragment-index="3"/>
																	          </div>
																	          <div class="col">
																	            <img class="plain fragment" data-src="assets/cosmos_gal_psf.png" style="width: 200px" data-fragment-index="2"/>
																	          </div>

																	          <div class="col">
																	            <img class="plain fragment" data-src="assets/cosmos_gal_pix.png" style="width: 200px" data-fragment-index="1"/>
																	          </div>

																	          <div class="col">
																	            <img class="plain fragment" data-src="assets/cosmos_gal_ground.png" style="width: 200px" data-fragment-index="0"/>
																	          </div>
																	        </div>

																	      <div class="container" style="position:relative; width:1000px; height:50px; margin:0 auto;">
																	        <div class='col fragment' data-fragment-index='4'> <font size="10"> $\longrightarrow$ </font> <br> $g_\theta$ </div>
																	        <div class='col fragment' data-fragment-index='3'> <font size="10"> $\longrightarrow$ </font> <br> PSF </div>
																	        <div class='col fragment' data-fragment-index='2'> <font size="10"> $\longrightarrow$ </font> <br> Pixelation</div>
																	        <div class='col fragment' data-fragment-index='1'> <font size="10"> $\longrightarrow$ </font> <br> Noise </div>
																	      </div>

																	      <div class="container">
																	          <div class="col">
																	            <div style="position:relative; width:400px; height:300px; margin:0 auto;">
																	            <img data-src="assets/pgm_0.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="0"/>
																	            <img data-src="assets/pgm_1.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="1"/>
																	            <img data-src="assets/pgm_1.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:350px;" data-fragment-index="2"/>
																	            <img data-src="assets/pgm_2.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:350px;" data-fragment-index="3"/>
																	            <img data-src="assets/pgm_3.png" class="plain fragment " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="4"/>
																	            </div>
																	          </div>
																	          <div class=" col">
																	            <div class="block fragment" data-fragment-index="0">
																	            <div class="block-title">
																	             Probabilistic model
																	            </div>
																	            <div class="block-content">
																	            <div style="position:relative; width:400px; height:100px; margin:0 auto;">
																	              <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="0"> $$ x \sim ? $$ </div>
																	              <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="1"> $$ x \sim \mathcal{N}(z, \Sigma) \quad z \sim ? $$<br>latent $z$ is a denoised galaxy image</div>
																	              <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="2"> $$ x \sim \mathcal{N}( \mathbf{P} z, \Sigma) \quad z \sim ?$$<br>latent $z$ is a super-resolved and denoised galaxy image</div>
																	              <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="3"> $$ x \sim \mathcal{N}( \mathbf{P} (\Pi \ast z), \Sigma) \quad z \sim ? $$<br>latent $z$ is a deconvolved, super-resolved, and denoised galaxy image </div>
																	              <div class="plain fragment " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="4"> $$ x \sim \mathcal{N}( \mathbf{P} (\Pi \ast g_\theta(z)), \Sigma) \quad z \sim \mathcal{N}(0, \mathbf{I}) $$ <br>latent $z$ is a Gaussian sample<br> <b class="alert"> $\theta$ are parameters of the model</b> </div>
																	            </div>
																	            <br>
																	            <br>
																	            <br>
																	          </div>
																	          </div>
																	          </div>
																	      </div>
																	      <div class="fragment"> $\Longrightarrow$ <b class="alert"> Decouples the morphology model from the observing conditions</b>.</div>
																	     </section>

																	    <section>
																	      <h3 class="slide-title">Bayesian Inference a.k.a. Uncertainty Quantification</h3>
																	      <div class="container">
																	          <div class="col">
																	            <img data-src="assets/pgm.png" class="plain" style="height: 250px;" ></img>
																	          </div>
																	          <div class="col">
																	            The Bayesian view of the problem:
																	                 $$ p(z | x ) \propto p_\theta(x | z, \Sigma, \mathbf{\Pi}) p(z)$$
																	             where:
																	             <br>
																	               <ul>
																	                 <li>$p( z | x )$ is the <b class="alert">posterior</b></li>
																	                 <li>$p( x | z )$ is the data likelihood, <b class="alert">contains the physics</b></li>
																	                 <li>$p( z )$ is the <b>prior</b> </li>
																	               </ul>
																	          </div>
																	      </div>

																	      <div class="container">
																	          <div class="col">
																	            <div style="position:relative; width:200px; height:200px; margin:0 auto;">
																	              <img class="plain fragment current-visible" data-src="assets/cosmos_gal_ground.png" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="0" />
																	              <img class="plain fragment" data-src="assets/cosmos_gal.png" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="1"/>
																	            </div>
																	            <div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
																	              <div class='col fragment current-visible' data-fragment-index='0' style="position:absolute;top:0;left:0;width:200px;"> Data<br> $x_n$</div>
																	              <div class='col fragment' data-fragment-index='1' style="position:absolute;top:0;left:0;width:200px;"> Truth<br> $x_0$ </div>
																	            </div>
																	            <br>
																	          </div>

																	          <div class="col fragment" data-fragment-index='0' >
																	            <div style="position:relative; width:200px; height:200px; margin:0 auto;">
																	              <div><video data-autoplay data-loop data-src="assets/rec_samples.mp4" type="video/mp4" style="height: 200px;"/>
																	              </div>
																	            </div>
																	            <div>Posterior samples<br> $g_\theta(z)$</div>
																	          </div>

																	          <div class="col">
																	            <div style="position:relative; width:200px; height:200px; margin:0 auto;">
																	              <div><video class="fragment current-visible" data-autoplay data-loop data-src="assets/rec_lsst.mp4" type="video/mp4" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="0"/></div>
																	              <img class="plain fragment " data-src="assets/rec_median.png" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="1"/>
																	            </div>

																	            <div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
																	              <div class='col fragment current-visible' data-fragment-index='0' style="position:absolute;top:0;left:0;width:200px;">  <br> $\mathbf{P} (\Pi \ast g_\theta(z))$</div>
																	              <div class='col fragment' data-fragment-index='1' style="position:absolute;top:0;left:0;width:200px;"> Median </div>
																	            </div>
																	          </div>

																	          <div class="col">
																	            <div style="position:relative; width:200px; height:200px; margin:0 auto;">
																	              <div><video class="fragment current-visible" data-autoplay data-loop data-src="assets/res_lsst.mp4" type="video/mp4" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="0"/></div>
																	              <img class="plain fragment " data-src="assets/rec_std.png" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="1"/>
																	            </div>

																	            <div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
																	              <div class='col fragment current-visible' data-fragment-index='0' style="position:absolute;top:0;left:0;width:200px;"> Data residuals <br> $x_n - \mathbf{P} (\Pi \ast g_\theta(z))$</div>
																	              <div class='col fragment' data-fragment-index='1' style="position:absolute;top:0;left:0;width:200px;"> Standard Deviation </div>
																	            </div>
																	          </div>
																	      </div>
																	      <div class="fragment"> $\Longrightarrow$ <b class="alert">Uncertainties are fully captured by the posterior</b>.</div>
																	    </section>
																	  </section>


            <section>
              <h3 class="slide-title">Focus of this talk</h3>
              <div class=container>
                <div class="col">
                  <div class="fig-container" data-file="venn.html" data-style="height: 600px;"></div>
                </div>

                <div class="col">

                  <br>
                  <br>
                  <div class="block fragment">
                    <div class="block-title">
                      This talk
                    </div>
                    <div class="block-content">
                      Generic approach to <b>uncertainty quantification</b> and <b>interpretability</b>:
                      <br>
                      <ul>
                        <li>(Differentiable) Physical Forward Models</li>
                        <li>Deep Generative Models</li>
                        <li>Bayesian Inference</li>
                      </ul>
                    </div>
                  </div>
                </div>
              </div>
            </section>



												<section>
						              <h1>Generative Modeling</h1>
													<h2>The Key to Manipulating Implicit Distributions</h2>
												</section>



												      			<section>
												      				<section>
												      					<h3 class="slide-title"> What is generative modeling?</h3>
												      					<br>
												      					<ul>
												      						<li>The goal of generative modeling is to <b>learn an <b class="alert">implicit</b> distribution $\mathbb{P}$</b>
												      							from which the <b>training set $X = \{x_0, x_1, \ldots, x_n \}$</b> is drawn.
												      						</li>
												      						<br>
												      						<li class='fragment'> Usually, this means building a parametric model $\mathbb{P}_\theta$
												      							that tries to be close to $\mathbb{P}$.
												      						</li>
												      					</ul>

												      					<br>
												      					<div class="container">
												      						<div class="col fragment fade-up">
												      							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756538/pasted-from-clipboard.png" class="plain"></img>
												      							<br>
												      							True $\mathbb{P}$
												      						</div>

												      						<div class="col  fragment fade-up">
												      							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756539/pasted-from-clipboard.png" class="plain"></img>
												      							<br>
												      							Samples $x_i \sim \mathbb{P}$
												      						</div>

												      						<div class="col  fragment fade-up">
												      							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756554/pasted-from-clipboard.png" class="plain"></img>
												      							<br>
												      							Model $\mathbb{P}_\theta$
												      						</div>
												      					</div>
												      					<br>
												      					<br>
												      					<ul>
												      						<li class="fragment"> Once trained, you can typically <b>sample from $\mathbb{P}_\theta$</b> and/or <b class="alert">evaluate the likelihood $p_\theta(x)$</b>.
												      						</li>
												      					</ul>

												      				</section>

												      				<section>
												      					<h3 class="slide-title">Why isn't it easy?</h3>
												      					<br>
												      					<ul>
												      						<li> The <b class="alert">curse of dimensionality</b> put all points far apart in high dimension
												      						</li>
												      					</ul>
												      					<div class="container">
												      						<div class="col fragment fade-up">
												      							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756597/pasted-from-clipboard.png" class="plain"></img>
												      						</div>

												      						<div class="col fragment fade-up">
												      							<img style="height:350px;" data-src="https://developers.google.com/machine-learning/clustering/images/CurseofDimensionality.svg" class="plain"></img>
												      							<br>Distance between pairs of points drawn from a Gaussian distribution.
												      						</div>
												      					</div>

												      					<br>
												      					<ul>
												      						<li class="fragment"><b>Classical methods</b> for estimating probability densities, i.e. Kernel Density Estimation (KDE) start to <b>fail in high dimension</b> because of all the gaps
												      						</li>
												      					</ul>
												      				</section>
												      			</section>

												      			<section>
												      				<h3 class="slide-title"> The evolution of generative models </h3>

												      				<br> <br> <br>
												      				<div class='container'>
												      					<div class='col'>
												      						<div style="position:relative; width:500px; height:500px; margin:0 auto;">
												      							<img class="fragment current-visible plain" data-src="assets/DBN.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="0" />
												      							<img class="fragment current-visible plain" data-src="assets/vae_faces.jpg" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="1" />
												      							<img class="fragment current-visible plain" data-src="assets/gan-samples-1.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="2" />
												      							<img class="fragment plain" data-src="assets/karras2017.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="3" />
												      						</div>
												      					</div>

												      					<div class='col'>
												      						<ul>
												      							<li class="fragment" data-fragment-index="0"> Deep Belief Network <br> (Hinton et al. 2006) </li>
												      							<br>
												      							<li class="fragment" data-fragment-index="1"> Variational AutoEncoder <br> (Kingma & Welling 2014) </li>
												      							<br>
												      							<li class="fragment" data-fragment-index="2"> Generative Adversarial Network <br> (Goodfellow et al. 2014)</li>
												      							<br>
												      							<li class="fragment" data-fragment-index="3"> Wasserstein GAN <br> (Arjovsky et al. 2017) </li>
												      						</ul>
												      					</div>
												      				</div>
												      				<br> <br> <br>
												      			</section>

												      			<section>
												      				<h3 class="slide-title"> A visual Turing test </h3>
												      				<div class="container">
												      					<div class="col">
												      						<img data-src="assets/samples_pixel_cnn.png" class="plain" style="height: 500px;"></img>
												      						<br>
												      						<div class="fragment fade-up" data-fragment-index="0"> Fake PixelCNN samples </div>
												      					</div>
												      					<div class="col">
												      						<img data-src="assets/sdss5.png" class="plain" style="height: 500px;"></img>
												      						<br>
												      						<div class="fragment fade-up" data-fragment-index="0"> Real galaxies from SDSS </div>
												      					</div>
												      				</div>
												      			</section>

																		<!-- <section data-vertical-align-top>
																			<h3 class="slide-title" >Not all generative models are created equal</h3>
																						<img data-src="assets/generative_models_table.png" class="plain"></img>
																	 						<div style="float:right; font-size: 20px">Grathwohl et al. 2018</div>
																				<br>
																				<br>
																			<ul>
																				<li> Of particular interest are models with an <b class="alert">explicit $\log p(x)$</b> (not the case of VAEs and GANs).</li>
																				<br>
																			</ul>
																		</section> -->

								    <section>
								       <h3 class="slide-title">Why are these generative models useful?</h3>

											<b class="alert">Implicit distributions are everywhere!</b>
											<br>
											<br>
											 <div class="container">
												 <div class="col fragment">
													 	<b>Case I</b>: Examples from data, no accurate physical model<br>
													 	<img data-src="assets/real_gal-inv-small.png" style="height:400px;"/><br>
															<div style="float:right; font-size: 20px">Mandelbaum et al. 2014</div>
															<br>
												 </div>

												 <div class="col fragment">
													 <b>Case II</b>: Physical model only available as a simulator<br>
													 <img data-src='assets/convergence.png' style="height:400px;"/><br>
														 <div style="float:right; font-size: 20px">Osato et al. 2020</div>
														 <br>
												 </div>
											 </div>
											 <br>
											 <div class="fragment">$\Longrightarrow$ Generative models <b class="alert">will enable Bayesian inference</b> in cases where
												 implicit distributions are involved, by providing a tractable $p_\theta(x)$.
											 </div>
							      </section>




																														      <section>
																														      	<h2> Data-driven priors for astronomical inverse problems</h2>

																																		<a href="https://arxiv.org/abs/1912.03980"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1912.03980-B31B1B.svg" class="plain" style="height:25px;" /></a>
																										                <a href="https://www.youtube.com/watch?v=oWOU3qNHoL0"><img src="https://img.shields.io/badge/-youtube-red?logo=youtube&labelColor=grey" class="plain" style="height:25px;" /></a>
																														                  <hr>
																														                  <br>
																														                  <div align="left" style="margin-left: 20px;">
																																								<div class="container">
																																									<div class="col">
																														                  <h3>Work in collaboration with <br>
																														                  Peter Melchior, Fred Moolekamp, Remy Joseph</h3>
																																							<br>
																																						</div>
																																						<div class="col">
																																						</div>
																																						<div class="col">
																																							<img data-src="assets/scarlet_data.png" style="height:450px;"/>
																																						</div>
																																						</div>
																														                  </div>
																														                  <br>
																														                  <br>
																														      </section>

																														      <section>
																														      <section data-background="assets/gal_hsc.png">

																														      </section>
																														 				<section>
																														 					<h3 class="slide-title">The challenge of galaxy blending</h3>
																																					<div class="container">
																																							<div class="col">
																																								<div style="position:relative; width:480px; height:500px; margin:0 auto;">
																																									<img class="fragment current-visible plain"  data-src="assets/hsc_deblending_success.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
																																									<img class="fragment plain" data-src="assets/hsc_shredded.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
																																								</div>
																																								<div class="fragment" data-fragment-index="0" style="float:left; font-size: 20px">Bosch et al. 2017</div>
																																							</div>
																																							<div class="col">
																																							<ul>
																																								<li class="fragment fade-up" data-fragment-index="0"> In HSC over 60% of all galaxies are blended</li>
																																								<br>
																																								<li class="fragment fade-up" data-fragment-index="0"> Important impact on our main cosmological probes</li>
																																								<br>
																																								<li class="fragment fade-up" data-fragment-index="1"> Current generation of deblenders does not meet our target requirements</li>
																																								<br>
																																								<ul class="fragment fade-up" data-fragment-index="2">
																																									<li> Existing methods rely on simple assumptions about galaxy profiles, like <i>symmetry</i> or <i>monotonicity</i></li>
																																								</ul>
																																							</ul>
																																							</div>
																																					</div>

																																					<div class="fragment fade-up"data-fragment-index="3" >
																																						Deblending is an ill-posed inverse problem, akin to <i>Blind Source Separation</i>. The is no <b>single solution</b>.<br>
																																						$\Longrightarrow$ Intuitively, <b class="alert">the key will be to leverage an understanding of how individual galaxies look like</b>.
																																					</div>
																														 				</section>

																																			<section>
																																			<h3 class="slide-title">Deep Learning applied to deblending (Reiman & Gohre 2018)</h3>
																																						<div>
																																						<img class="plain" data-src="assets/Reiman2018_1.png" />
																																							Branched GAN model for deblending
																																						</div>

																																					<div class="block fragment">
																																						<div class="block-title">
																																							The issue with <i>black-box</i> models
																																						</div>
																																						<div class="block-content">
																																							<ul>
																																								<li> No explicit control of noise, PSF, depth, number of sources.
																																										<ul>
																																											<li> Model would have to be retrained for all observing configurations
																																											</li>
																																										</ul>
																																								</li>
																																								<br>
																																								<li> No guarantees on the network output (e.g. flux preservation, artifacts)
																																								</li>
																																							</ul>
																																					</div>
																																				</div>
																																			</section>

																																			<section>
																																						<img class="plain" data-src="assets/Reiman2018_3.png"/>
																																			</section>
																																		</section>

																																		<section>
																																		<section>
																																			<h3 class="slide-title">Linear inverse problems</h3>

																																			$\boxed{y =  \mathbf{A}x + n}$
																																			<br>
																																			<br>
																																			$\mathbf{A}$ is known and encodes our physical understanding of the problem.
																																			<span class="fragment"><br>$\Longrightarrow$ When non-invertible or ill-conditioned, the inverse problem is ill-posed with no unique solution $x$</span>
																																			<div class="container fragment fade-up">
																																					<div class="col">
																																						<img data-src="assets/pluto_smooth.png" class="plain"></img>
																																						Deconvolution
																																					</div>
																																					<div class="col">
																																						<img data-src="assets/pluto_missing.png" class="plain"></img>
																																						Inpainting
																																					</div>
																																					<div class="col">
																																						<img data-src="assets/plutoNoise.png" class="plain"></img>
																																						Denoising
																																					</div>
																																			</div>

																																		</section>

																																		<section data-vertical-align-top>
																																			<h3 class="slide-title">A Bayesian view of the problem</h3>
																																			$\boxed{y =  \mathbf{A}x + n}$
																																			<br>

																																			<br>
																																			<div class="fragment">
																																			$$ p(x | y) \propto p(y | x) \ p(x) $$
																																			</div>
																																			<br>

																																			<ul>
																																				<li class="fragment fade-up">$p(y | x)$ is the data likelihood, which <b>contains the physics</b><br>
																																				</li>
																																				<br>
																																				<li class="fragment fade-up">$p(x)$ is our prior knowledge on the solution.</li>
																																			</ul>
																																			<br>
																																			<br>
																																			<div class="fragment fade-up">
																																			With these concepts in hand, we want to estimate the Maximum A Posteriori solution:
																																			<br>
																																			<br>
																																			$$\hat{x} = \arg\max\limits_x \ \log p(y \ | \ x)  + \log p(x)$$
																																			<br>
																																			For instance, if $n$ is Gaussian, $\hat{x} = \arg\max\limits_x \ - \frac{1}{2} \parallel y - \mathbf{A} x \parallel_{\mathbf{\Sigma}}^2 + \log p(x)$
																																			</div>
																																			<br>
																																			<div class="fragment fade-up">
																																				<h3>How do you choose the prior ?</h3>
																																			</div>
																																		</section>

																																		<section>
																																			<h3 class="slide-title"> Classical examples of signal priors </h3>
																																				<div class="container">
																																					<div class="col">
																																						Sparse
																																						<img data-src="assets/wavelet.png" height="400" class="plain"></img><br>
																																						$$ \log p(x) = \parallel \mathbf{W} x \parallel_1 $$
																																					</div>
																																					<div class="col">
																																						Gaussian
																																						<img data-src="assets/zknj8.jpg" height="400" class="plain"></img>
																																						$$ \log p(x) = x^t \mathbf{\Sigma^{-1}} x $$
																																					</div>
																																					<div class="col">
																																						Total Variation
																																						<img data-src="assets/shepp-Logan.ppm" class="plain"></img>
																																						$$ \log p(x) = \parallel \nabla x \parallel_1 $$

																																					</div>
																																			</div>
																																		</section>

																																		<section data-background="assets/hsc_screen.png">
																																				<h2>But what about this?</h2>

																																		</section>
																																	</section>


																																	<section>
																																		<h3  class="slide-title">PixelCNN: Likelihood-based Autoregressive generative model</h3>
																																		<br>
																																		<br>
																																		<div class="container">
																																		<div class="col">
																																				Models the probability $p(x)$ of an image $x$ as:
																																				$$ p_{\theta}(x) = \prod_{i=0}^{n} p_{\theta}(x_i | x_{i-1} \ldots x_0) $$
																																				<ul>
																																						<li class="fragment">$p_\theta(x)$ is explicit! We get a number out.</li>
																																						<br>
																																						<li class="fragment">We can train the model to learn a distribution of isolated galaxy images.</li>
																																						<br>
																																						<li class="fragment">We can then evaluate its gradient $\frac{\color{orange} d \color{orange}\log \color{orange}p\color{orange}(\color{orange}x\color{orange})}{\color{orange} d \color{orange}x}$.</li>
																																				</ul>
																																				<br>
																																				<br>
																																		</div>

																																		<div class="col">
																																				<img data-src="assets/pixel_cnn_conv.png" class="plain"></img>
																																 				<div style="float:right; font-size: 20px">van den Oord et al. 2016</div>
																																		</div>
																																	 </div>

																																	 <br>
																																	 <ul>
																																		 <li class="fragment"> Check out another application of these models to <b>discrimination between real (SDSS) and simulated (Illustris TNG)</b>
																																			 galaxy populations: <a href="https://arxiv.org/abs/2007.00039">Zanisi, Huertas-Company, <b>Lanusse</b> et al. 2020 arXiv:2007.00039</a></li>
																																	 </ul>
																																	</section>

																														  		<section>
																														  				<h3 class="slide-title">Getting started with Deep Priors: deep denoising example</h3>
																														  				$$ \boxed{{\color{Orchid} y}  = {\color{SkyBlue} x} + n} $$
																														  													<div class="container">
																														  														<div class="col">
																														  															<div style="position:relative; width:550px; height:550px; margin:0 auto;">
																														  																		<img class="fragment current-visible plain" data-src="assets/points.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
																														  																		<div class="fig-container fragment" data-file="dgm_prior_denoising.html" data-style="height: 550px;width: 550px;" style="position:absolute;top:0;left:0;" data-fragment-index="1"></div>
																														  															</div>
																														  															<!-- <img data-src="assets/points.png"/>
																														  															<div class="fig-container" data-file="dgm_prior_denoising.html" data-style="height: 550px;"></div> -->

																														  														</div>

																														  														<div class="col">
																														  															<ul>
																														  																<li class="fragment" data-fragment-index="0" > Let us assume we have access to examples of $ {\color{SkyBlue} x}$ without noise.</li>
																														  																<br>
																														  																<li class="fragment"  data-fragment-index="1">We learn the <b class="alert">distribution of noiseless data $\log p_\theta(x)$</b> from samples using a deep generative model.</li>
																														  																<br>
																														  																<!-- <li class="fragment"> We measure a noisy ${\color{Orchid} y}$ and we want to estimate a denoised ${\color{SkyBlue} x}$</li>
																														  																<br> -->
																														  																<li class="fragment">The solution should lie on the <b class="alert">realistic data manifold</b>, symbolized by the two-moons distribution.
																														  																	<div class="fragment">
																														  																	<p> We want to solve for the Maximum A Posterior solution: </p>
																														  																	$$\arg \max - \frac{1}{2} \parallel {\color{Orchid} y} - {\color{SkyBlue} x} \parallel_2^2 + \log p_\theta({\color{SkyBlue} x})$$

																														  																	This can be done by <b>gradient descent</b> as long as one has access to the <b class="alert">score function</b> $\frac{\color{orange} d \color{orange}\log \color{orange}p\color{orange}(\color{orange}x\color{orange})}{\color{orange} d \color{orange}x}$.
																														  																</div>
																														  																</li>
																														  															</ul>
																														  													</div>
																														  										</div>
																														  										</section>

																																	<section>
																																		<h3 class="slide-title"> The Scarlet algorithm: deblending as an optimization problem</h3>
																																				<div style="float:right; font-size: 20px">Melchior et al. 2018</div>

																																				$$ \mathcal{L} = \frac{1}{2} \parallel \mathbf{\Sigma}^{-1/2} (\ Y - P \ast A S \ ) \parallel_2^2 - \sum_{i=1}^K \log p_{\theta}(S_i) + \sum_{i=1}^K g_i(A_i) +  \sum_{i=1}^K f_i(S_i)$$

																																		<div class="container">
																																		<div class="col">
																																				<img data-src="assets/scarlet_data.png" height=450 class="plain"></img>
																																		</div>

																																		<div class="col">

																																			Where for a $K$ component blend:
																																			<br>
																																				<ul>
																																				<li>$P$ is the convolution with the instrumental response</li>
																																				<br>
																																				<li>$A_i$ are channel-wise galaxy SEDs, $S_i$ are the morphology models</li>
																																				<br>
																																				<li>$\mathbf{\Sigma}$ is the noise covariance</li>
																																				<br>
																																				<li>$\log p_\theta$ is a PixelCNN prior</li>
																																				<br>
																																				<li>$f_i$ and $g_i$ are arbitrary additional non-smooth consraints, e.g. positivity, monotonicity...</li>
																																				</ul>
																																		</div>
																																	</div>

																																	<span class="fragment fade-up">$\Longrightarrow$ Explicit physical modeling of the observed sky</span>
																																	</section>

																																	 <section>
																																		<h3  class="slide-title">Training the morphology prior</h3>

																																		<div class="container">
																																			<div class="col">
																																				<img data-src="assets/cosmos_training.png" height=450 class="plain"></img>
																																				<div> Postage stamps of isolated COSMOS galaxies used for training, at WFIRST resolution and fixed fiducial PSF</div>
																																		</div>

																																		<div class="col">
																																		<div class="container fragment fade-in">
																																			<div class="col">
																																				isolated galaxy
																																			<img data-src="assets/gal_1.png" class="plain"></img>
																																			<span> $\log p_\theta(x) = 3293.7$ </span>
																																		</div>

																																			<div class="col">
																																				artificial blend
																																			<img data-src="assets/gal_2.png" class="plain"></img>
																																			<span> $\log p_\theta(x) = 3100.5 $ </span>
																																		</div>
																																			</div>
																																		</div>
																																	</section>

																																	<section>
																																		<section>
																																		<h3 class="slide-title">Scarlet in action</h3>

																																		<div class="container">
																																			<div class="col">
																																				Input blend
																																			<div style="position:relative; width:480px; height:480px; margin:0 auto;">
																																			<img data-src="assets/scar_input.png" class="plain"></img>
																																		</div>
																																			</div>

																																		<div class="col">
																																			<span class="fragment" data-fragment-index="0">Solution</span>
																																			<div style="position:relative; width:480px; height:480px; margin:0 auto;">
																																					  <img class="fragment current-visible plain" data-src="assets/old_rec.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
																																					  <img class="fragment  plain" data-src="assets/pix_rec.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
																																			</div>
																																		</div>

																																		<div class="col">
																																			<span class="fragment" data-fragment-index="0">Residuals</span>
																																			<div style="position:relative; width:480px; height:480px; margin:0 auto;">
																																					  <img class="fragment current-visible plain" data-src="assets/old_res.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
																																					  <img class="fragment  plain" data-src="assets/pix_res.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
																																			</div>
																																		</div>
																																		</div>

																																		<ul>
																																				<li class="fragment fade-up" data-fragment-index="0">Classic priors (monotonicity, symmetry).</li>
																																				<br>

																																				<li class="fragment fade-up" data-fragment-index="1">Deep Morphology prior.</li>
																																		</ul>

																																	</section>
																																	<section>
																																		<div class="container">
																																			<div class="col">
																																				True Galaxy
																																			<img data-src="assets/true_input.png" class="plain"></img>
																																		</div>

																																		<div class="col">
																																			Deep Morphology Prior Solution
																																						<img class=" plain" data-src="assets/pix_rec2.png"  />
																																		</div>

																																		<div class="col">
																																			Monotonicity + Symmetry Solution
																																						<img class=" plain" data-src="assets/scar_rec2.png" />
																																			</div>
																																		</div>
																																	</section>
																																	</section>


																														      <section>
																																		<h3 class="slide-title"> Extending to multi-band images</h3>

																																				<img class=" plain" data-src="assets/scarlet_hsc.png" />

																														      </section>

																						<section>
																																		<h3 class="slide-title"> Takeaway message</h3>

																																		<br>
																																		<br>
																																		<br>

																														        <ul>
																														          <li> We have introduce an <b class="alert">hybrid physical/deep learning model for inverse problems</b>
																														            <ul>
																														              <br>
																														              <li class="fragment"> Incorporate prior astrophysical knowledge as a data-driven prior
																														              </li>
																														              <br>

																														              <li class="fragment"> Interpretable in terms of physical components of the astronomical scene
																														              </li>
																														              <br>
																														              <br>

																														              <li class="fragment"> Can accomodate different observing conditions and instruments
																														                <br> $\Longrightarrow$ For instance, for the joint modeling of  LSST/Euclid data
																														              </li>
																														            </ul>
																														          </li>

																														          <br>
																														          <br>
																														              <br>
																																		<br>

																																	</section>



											 <section>
												<h2>Probabilistic Mapping of Dark Matter by<br> Neural Score Matching</h2>
												<a href="https://arxiv.org/abs/2011.08271"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2011.08271-B31B1B.svg" class="plain" style="height:25px;" /></a>
												<a href="https://arxiv.org/abs/2011.08698"><img src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
												<hr>
												<div class="container">
													<div class="col">
														<div align="left" style="margin-left: 20px;">
															<h3>Work in collaboration with: <br>
																Benjamin Remy, Zaccharie Ramzi
															</h3>
															<img data-src="assets/benjamin.png" style='width:200px; height:200px;'></img>
															<img data-src="http://www.cosmostat.org/wp-content/uploads/2019/03/Portrait-2-1600x2000.jpg" style='width:200px; height:200px;object-fit: cover;'></img>

															<br>

															$\Longrightarrow$ <b class="alert">Learn complex priors</b> by Neural Score Estimation and <b class="alert">sample from posterior</b> with gradient-based MCMC.
														</div>
													</div>
													<div class="col">
														<img class="plain" data-src="assets/cropped.gif" style="width:450px;" />
													</div>
												</div>
												<br>
											</section>

															<section>
																<section data-background-image="assets/gravitational-lensing-diagram.jpg">
																	<h3 class="slide-title"> Gravitational lensing</h3>
																	<div class="fragment fade-up">
																		<img class="plain" data-src="assets/great.jpg" />

																		<div class="block ">
																			<div class="block-title">
																				Galaxy shapes as estimators for gravitational shear
																			</div>
																			<div class="block-content">
																				$$ e = \gamma + e_i \qquad \mbox{ with } \qquad e_i \sim \mathcal{N}(0, I)$$
																				<ul>
																					<li> We are trying the measure the <b class="alert"> ellipticity $e$</b> of
																						galaxies as an estimator for the <b class="alert">gravitational shear $\gamma$ </b>
																					</li>
																				</ul>
																			</div>
																		</div>
																	</div>
																</section>

																<section>
																	<h3 class="slide-title">Weak Lensing Mass-Mapping as an Inverse Problem</h3>
																	<div class="container">
																		<div class="col">
																			Shear <b class="alert">$\gamma$</b><br>
																			<img data-src="assets/shear_cat1.png" style="width:450px;"></img>
																		</div>

																		<div class="col fragment fade-up">
																			Convergence <b class="alert">$\kappa$</b><br>
																			<img data-src="assets/kappa.png" style="width:450px;"></img>
																		</div>
																	</div>

																	<div style="position:relative; width:1000px; height:100px; margin:0 auto;">
																		<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
																			$$\gamma_1 = \frac{1}{2} (\partial_1^2 - \partial_2^2) \ \Psi \quad;\quad \gamma_2 = \partial_1 \partial_2 \ \Psi \quad;\quad \kappa = \frac{1}{2} (\partial_1^2 + \partial_2^2) \ \Psi$$
																		</div>
																		<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
																			$$\boxed{\gamma = \mathbf{P} \kappa}$$
																		</div>
																	</div>
																</section>

																<section>
																	<h3 class="slide-title"> Illustration on the Dark Energy Survey (DES) Y3</h3>
																	<div style="float:right; font-size: 20px">Jeffrey, et al. (2021)
																	</div><br>
																	<img data-src="assets/DESY3map.png" style="height:600px;"></img>
																</section>
															</section>

																<section data-background-image="assets/convergence.png">
																	<h2>Where can we get our prior from?</h2>
																	<br>
																	<br>
																	<br>
																	<div class="fragment"> $\Longrightarrow$ <b>Implicit prior</b> $p(x)$ in the form of <b>cosmological simulations</b>.</div>

																	<!-- <div class="container">
																					<div class="col">
																						<img data-src="assets/gal_hsc.png" style="object-fit: cover; width:500px;height:500px;">
																						<div class="fragment"> $\Longrightarrow$ Prior in the form of existing data.</div>
																					</div>
																					<div class="col fragment fade-up">
																						<img data-src="assets/convergence.png" style="object-fit: cover; width:500px;height:500px;">
																						<div class="fragment"> $\Longrightarrow$ Prior in the form of numerical simulations.</div>
																					</div>
																				</div> -->
																</section>
															</section>

															<!-- <section class="inverted" data-background="#000">
																<h2> Can we use Deep Learning to embed simulation-driven priors within a <b>physical Bayesian model</b>?</h2>
															</section> -->
						<!--
						            <section>

						            				<section data-vertical-align-top>
						            					<h3 class="slide-title">What Would a Bayesian Do?</h3>
						            					$\boxed{y = \mathbf{A}x + n}$
						            					<br>
						            					<br>
						            					The Bayesian view of the problem:
						            					<br>
						            					$$ p(x | y) \propto p(y | x) \ p(x) $$
						            					<ul>
						            						<li class="fragment fade-up">$p(y | x)$ is the data <b>likelihood</b>, which <b class="alert">contains the physics</b><br>
						            						</li>
						            						<br>
						            						<li class="fragment fade-up">$p(x)$ is the <b>prior</b> knowledge on the solution.</li>
						            					</ul>
						            					<br>
						            					<br>
						            					<div class="fragment fade-up">
						            						<ul>With these concepts in hand we can:
						            							<br>
						            							<li class="fragment">Estimate for instance the <b>Maximum A Posteriori</b> solution:
						            								<br>
						            								$$\hat{x} = \arg\max\limits_x \ \log p(y \ | \ x) + \log p(x)$$
						            							</li>
						            							<li class="fragment">Estimate from the <b>full posterior p(x|y)</b> with MCMC or Variational Inference methods.
						            							</li>
						            						</ul>
						            					</div>
						            					<br>
						            					<div class="fragment fade-up">
						            						<h3>How do you choose the prior ?</h3>
						            					</div>
						            				</section>

						            				<section>
						            					<h3 class="slide-title"> Classical examples of signal priors </h3>
						            					<div class="container">
						            						<div class="col">
						            							Sparse
						            							<img data-src="assets/wavelet.png" height="400" class="plain"></img><br>
						            							$$ \log p(x) = \parallel \mathbf{W} x \parallel_1 $$
						            						</div>
						            						<div class="col">
						            							Gaussian
						            							<img data-src="assets/zknj8.jpg" height="400" class="plain"></img>
						            							$$ \log p(x) = x^t \mathbf{\Sigma^{-1}} x $$
						            						</div>
						            						<div class="col">
						            							Total Variation
						            							<img data-src="assets/shepp-Logan.ppm" class="plain"></img>
						            							$$ \log p(x) = \parallel \nabla x \parallel_1 $$
						            						</div>
						            					</div>
						            				</section>

						          				<section data-background="assets/hsc_screen.png">
						          					<h2>But what about this?</h2>
						          				</section>
						            </section> -->

									<!-- <section class="inverted" data-background="#000">
										<h2> Let's use <b>deep generative models</b> to build a tractable model of this high-dimensional prior distribution.</h2>
									</section> -->



									<section>
										<h3 class="slide-title">Writing down the convergence map log posterior</h3>

											$$ \log p( \kappa | e) = \underbrace{\log p(e | \kappa)}_{\simeq -\frac{1}{2} \parallel e - P \kappa \parallel_\Sigma^2} + \log p(\kappa) +cst $$

											<ul>
												<li> The likelihood term is <b class="alert">known analytically</b>.
												</li>

												<li class="fragment fade-up"> There is <b class="alert">no close form expression for the full non-Gaussian prior</b> of the convergence.
													<br> However:
													<ul>
														<li class='fragment'> <b>We do have access to samples of full  <b class="alert">implicit</b> prior</b> through simulations: $X = \{x_0, x_1, \ldots, x_n \}$ with $x_i \sim \mathbb{P}$
															<img data-src='assets/plot_massive_nu.png' />
														</li>
													</ul>
												</li>
											</ul>
											<div class="fragment">$\Longrightarrow$ Our strategy: <b class="alert">Learn the prior from simulation</b>,
												and then <b class="alert">sample the full posterior</b>.</div>
						          </section>


								<section>
									<section>
										<h3 class="slide-title">The score is all you need!</h3>
										<br>

										<div class="container">
											<div class="col">
												<ul>
													<li> Whether you are looking for the MAP or sampling with HMC or MALA, you
														<b class="alert">only need access to the score</b> of the posterior:
														$$\frac{\color{orange} d \color{orange}\log \color{orange}p\color{orange}(\color{orange}x \color{orange}|\color{orange} y\color{orange})}{\color{orange}
														d
														\color{orange}x}$$
														<ul>
															<li>Gradient descent: $x_{t+1} = x_t + \tau \nabla_x \log p(x_t | y) $</li>
															<li>Langevin algorithm: $x_{t+1} = x_t + \tau \nabla_x \log p(x_t | y) + \sqrt{2\tau} n_t$ </li>
														</ul>
													</li>
													<br>
												</ul>
											</div>
											<div class="col">
												<img data-src="assets/score_two_moons.png"></img>
											</div>
										</div>
										<br>
										<br>
										<!-- <ul>
											<li > The score of the full posterior is simply:
												$$\nabla_x \log p(x |y) = \underbrace{\nabla_x \log p(y |x)}_{\mbox{known}} \quad + \quad \underbrace{\nabla_x \log p(x)}_{\mbox{can be learned}}$$
												$\Longrightarrow$ all we have to do is <b class="alert">model/learn the score of the prior</b>.
											</li>
										</ul> -->
									</section>

										<section>
											<h3 class="slide-title">Neural Score Estimation by Denoising Score Matching</h3>

											<ul>
												<li><b>Denoising Score Matching</b>: An optimal <b class="alert">Gaussian denoiser learns the score</b> of a given distribution.
													<ul>
														<li class="fragment fade-up"> If $x \sim \mathbb{P}$ is corrupted by additional Gaussian noise $u \in \mathcal{N}(0, \sigma^2)$ to yield
															$$x^\prime = x + u$$
														</li>
														<li class="fragment fade-up"> Let's consider a denoiser $r_\theta$ trained under an $\ell_2$ loss:
															$$\mathcal{L}=\parallel x - r_\theta(x^\prime, \sigma) \parallel_2^2$$
														</li>
														<li class="fragment fade-up"> The optimal denoiser $r_{\theta^\star}$ verifies:
															$$\boxed{\boldsymbol{r}_{\theta^\star}(\boldsymbol{x}', \sigma) = \boldsymbol{x}' + \sigma^2 \nabla_{\boldsymbol{x}} \log p_{\sigma^2}(\boldsymbol{x}')}$$
														</li>
													</ul>
												</li>
											</ul>

											<div class="fragment fade-up">
												<div class="container">
													<div class="col">$\boldsymbol{x}'$
													</div>
													<div class="col">$\boldsymbol{x}$
													</div>
													<div class="col">$\boldsymbol{x}'- \boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
													</div>
													<div class="col">$\boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
													</div>
												</div>
												<img data-src="assets/denoised_mnist.png" style='width:1200px;'></img>
											</div>
										</section>

										<section>
											<h3 class="slide-title">Training a Neural Score Estimator in practice</h3>

											<div class="container">
												<div class="col">
													<br>
													<img data-src="assets/unet.png" data-fragment-index="1" /><br>
													<br> A standard UNet
												</div>

												<div class="col">
													<ul>
														<li> We use a very standard residual UNet, and we adopt a residual
															score matching loss:
															$$ \mathcal{L}_{DSM} = \underset{\boldsymbol{x} \sim P}{\mathbb{E}} \underset{\begin{subarray}{c}
															\boldsymbol{u} \sim \mathcal{N}(0, I) \\
															\sigma_s \sim \mathcal{N}(0, s^2)
															\end{subarray}}{\mathbb{E}} \parallel \boldsymbol{u} + \sigma_s \boldsymbol{r}_{\theta}(\boldsymbol{x} + \sigma_s \boldsymbol{u}, \sigma_s) \parallel_2^2$$
															$\Longrightarrow$ direct estimator of the score $\nabla \log p_\sigma(x)$
														</li>
														<br>
														<li class="fragment fade-up"> Lipschitz regularization to improve robustness:
															<br><br>
															<div class="container">
																<div class="col">
																	Without regularization
																</div>

																<div class="col">
																	With regularization
																</div>
															</div>
															<img data-src='assets/reg_score.png' />
														</li>
													</ul>
												</div>
											</div>
										</section>

										 <section>
														<h3 class="slide-title">Efficient sampling by Annealed HMC</h3>

														<ul>
															<li> Even with gradients, <b class="alert">sampling in high number of dimensions is difficult!</b><br>
																$\Longrightarrow$ Use a parallel annealing strategy to effectively sample from full distribution.
															</li>
															<br>
															<li class="fragment fade-up"> We use the fact that our score network $\mathbf{r}_\theta(x, \sigma)$ is learning a noise-convolved distribution $\nabla \log p_\sigma$

																<div>
																	$$\sigma_1 > \sigma_2 > \sigma_3 > \sigma_4 $$
																	<img data-src="assets/annealing.png" />
																</div>

															</li>

															<li class="fragment fade-up"> Run many HMC chains in parallel, progressively annealing the $\sigma$ to 0, <b class="chain">keep last point in the chain as independent sample</b>.
															</li>
														</ul>
													</section>

													<section>
															<h3 class="slide-title">Example of one chain during annealing</h3>
															<img data-src="assets/hmc-annealing.gif"/>
													</section>

									</section>

									<section>
										<section>
											<h3 class="slide-title">Illustration on $\kappa$-TNG simulations</h3>
											<div class="container">
												<div class="col">
														<div style="float:right; font-size: 20px"> Remy, Lanusse, et al. (2022) <a href="https://arxiv.org/abs/2201.05561"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2201.05561-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
												</div>
											</div>
											<div class="container">
												<div class="col">
													<img data-src='assets/ref_ktng.png' style="width:350px; height:350px;" />
													<br>
													True convergence map
												</div>
												<div class="col">
													<div class="block-content">
														<div style="position:relative; width:350px; height:350px; top:10px; left:40px;">
															<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="0">
																<img data-src='assets/ks_ktng.png' style="width:350px; height:350px;" />
															</div>
															<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="1">
																<img data-src='assets/wiener_ktng.png' style="width:350px; height:350px;" />
															</div>
															<div class="plain fragment" style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="2">
																<img data-src='assets/mean_post_ktng.png' style="width:350px; height:350px;" />
															</div>
														</div>
														<div class="block-content">
															<div style="position:relative; width:350px; height:20px; top:50px; left:10px;">
																<div class="fragment current-visible " data-fragment-index="0" style="position:absolute;top:0;left:0;width:350px;">Traditional Kaiser-Squires</div>
																<div class="fragment current-visible " data-fragment-index="1" style="position:absolute;top:0;left:0;width:350px;">Wiener Filter</div>
																<div class="fragment" data-fragment-index="2" style="position:absolute;top:0;left:0;width:350px;">Posterior Mean (ours)</div>
															</div>
														</div>
														<br>
														<br>
													</div>

												</div>
												<div class="col fragment">
													<img data-src='assets/cropped.gif' style="width:350px; height:350px;" />
													<br>
													Posterior samples
												</div>
											</div>
										</section>


										<section>
											<h3 class="slide-title">Comparison with a direct inversion method</h3>

											<div class="container">
												<div class="col">
												  <img data-src='assets/mean_post_ktng.png' style="width:500px; height:500px;" />
													<div >Posterior Mean (ours)</div>

												</div>

												<div class="col">
												  <img data-src='assets/deepmass-map.png' style="width:500px; height:500px;" />
													<div>U-net recontruction unde l2 loss (DeepMass)</div>

												</div>

											</div>
										</section>


										<section>
											<h3 class="slide-title">Reconstruction of the <b class="alert">HST/ACS COSMOS field</b></h3>

											<ul>
											<li> COSMOS shear data from <a href=https://ui.adsabs.harvard.edu/abs/2010A%26A...516A..63S/abstract>Schrabback et al. 2010</a>
											</li>
											<li> Prior learned from $\kappa$-TNG simulation from <a href=https://arxiv.org/abs/2010.09731>Osato et al. 2021</a>.
											</li>
										</ul>
											<br>
											<div class="container">
												<div class="col">
													<div class="block-content">
														<div style="position:relative; height:570px; top:0px; left:0px;">
															Massey et al. (2007)
															<img data-src="assets/massey.png" style="height:500px;"></img>
														</div>
													</div>
												</div>

												<div class="col">
													<div class="block-content">
														<div style="position:relative; height:570px; top:0px; left:0px;">
															<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0">
																Remy et al. (2022) <b class="alert">Posterior mean</b>
																<img data-src='assets/remy.png' style="height:500px;" />
															</div>

															<div class="plain fragment" style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="1">
																Remy et al. (2022) <b class="alert">Posterior samples</b>
																<img data-src='assets/cosmos_samples.gif' style="height:500px;" />
															</div>

														</div>
													</div>
												</div>

											</div>
										</section>
									</section>


															<section>
																<section>
																	<h3 class="slide-title">Uncertainty quantification in Magnetic Resonance Imaging (MRI)</h3>
																	<div style="float:right; font-size: 20px">Ramzi, Remy, <b>Lanusse</b> et al. 2020 <a href="https://arxiv.org/abs/2011.08698" style='vertical-align:middle; display:inline;'><img
																				src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
																	</div>
																	<br>
																	<br>
																	$$\boxed{y = \mathbf{M} \mathbf{F} x + n}$$
																	<div><video data-autoplay loop="loop" data-src="assets/knee.mp4" type="video/mp4" style="width: 1280px;" />
																	</div>
																	<br>

																	<br>

																	<br>

																	<p class="fragment">$\Longrightarrow$ We can see which parts of the image are well constrained by data, and which regions are <b class="alert">uncertain</b>.</p>
																</section>
															</section>


		<section>
			<h1> Conclusion </h1>
		</section>

		<section>
			<h3 class="slide-title"> Conclusion </h3>
			<div class="block ">
				<div class="block-title">
					Going Beyond Common Deep Learning Limitations
				</div>
				<div class="block-content">
					<br>
					<ul>
						<li class="fragment"> Key to interpretability and uncertainty quantification is <b class="alert">Bayesian inference over a forward model</b>.
							<ul>
								<li> Allows you to explicitly <b>embed your existing physical knowledge</b> into the model (e.g. knowledge of PSF, noise,...).
								</li>
							</ul>
						</li>
						<br>

						<li class="fragment"> <b class="alert">Generative models</b> complement known physical models with data-driven components
							<ul>
								<li>They allow you to <b>manipulate implicit distributions</b> provided by data itself or simulations.</li>
							</ul>
						</li>
						<br>
						<li class="fragment"> Modern inference techniqes allow you to <b class="alert">sample the full Bayesian posterior at scale</b>
							<ul>
								<li> Lifts most of the restrictions on using Bayesian approaches for large scale problems.
								</li>
							</ul>
						</li>
						<br>
					</ul>
				</div>
			</div>
			<br>

			<div class="fragment">
			<b>Free advertisement:</b><br><br>
			<ul>
				<li class="fragment"> <a href="https://ml4astro.github.io/icml2022/">ICML 2022 Workshop on Machine Learning for Astrophysics</a>, extended abstract submission deadline May 23rd.
					<br> $\Longrightarrow$ The theme is <b>how to make machine learning useful for science</b>.
				</li>
			</ul>
		</div>
			<br>
			<p class="fragment">Thank you ! </p>
			<br> <br> <br>
		</section>


		</div>
	</div>

	<style>
		/* .reveal .slides {
			border: 5px solid red;
			min-height: 100%;
			width: 128mm;
			height: 96mm;
		} */

		.reveal .block {
			background-color: #191919;
			margin-left: 20px;
			margin-right: 20px;
			text-align: left;
			padding-bottom: 0.1em;
		}

		.reveal .block-title {
			background-color: #333333;
			padding: 8px 35px 8px 14px;
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .block-content {
			padding: 8px 35px 8px 14px;
		}

		.reveal .slide-title {
			border-left: 5px solid white;
			text-align: left;
			margin-left: 20px;
			padding-left: 20px;
		}

		.reveal .alert {
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .inverted {
			filter: invert(100%);
		}

		/*
	/* .reveal .alert {
	padding:8px 35px 8px 14px; margin-bottom:18px;
	text-shadow:0 1px 0 rgba(255,255,255,1);
	border:5px solid #FFAA7F;
	-webkit-border-radius: 14px; -moz-border-radius: 14px;
	border-radius:14px
	background-position: 10px 10px;
	background-repeat: no-repeat;
	background-size: 38px;
	padding-left: 30px; /* 55px; if icon
	}
	.reveal .alert-block {padding-top:14px; padding-bottom:14px}
	.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
	/*.reveal .alert li {margin-top: 1em}
	.reveal .alert-block p+p {margin-top:5px} */
	</style>


	<script src="reveal.js/dist/reveal.js"></script>
	<script src="reveal.js/plugin/notes/notes.js"></script>
	<script src="reveal.js/plugin/markdown/markdown.js"></script>
	<script src="reveal.js/plugin/highlight/highlight.js"></script>
	<script src="reveal.js/plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			controls: true,

			//center: false,
			hash: true,

			// Visibility rule for backwards navigation arrows; "faded", "hidden"
			// or "visible"
			controlsBackArrows: 'hidden',

			// Display a presentation progress bar
			progress: true,

			// Display the page number of the current slide
			slideNumber: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// The "normal" size of the presentation, aspect ratio will be preserved
			// when the presentation is scaled to fit different resolutions. Can be
			// specified using percentage units.
			width: 1280,
			height: 720,

			// Factor of the display size that should remain empty around the content
			margin: 0.1,

			// Bounds for smallest/largest possible scale to apply to content
			minScale: 0.2,
			maxScale: 1.5,

			autoPlayMedia: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],

			dependencies: [{
					src: 'reveal.js/plugin/markdown/marked.js'
				},
				{
					src: 'reveal.js/plugin/markdown/markdown.js'
				},
				{
					src: 'reveal.js/plugin/notes/notes.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/math/math.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/reveal.js-d3/reveald3.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/Chart.min.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/csv2chart.js'
				},
				{
					src: 'reveal.js/plugin/highlight/highlight.js',
					async: true
				},
			]

		});
	</script>
</body>

</html>
