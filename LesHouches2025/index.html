<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Simulation-Based Bayesian Inference for Cosmology</title>

	<meta name="description" content="Les Houches Summer School, July 2025">
	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<link rel="stylesheet" href="reveal.js/dist/theme/darkenergy.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section data-background-iframe="background.html">
				<div class="container">
					<div class="title" style="border-radius: 20px; background-color:rgba(0, 0, 0, 0.4);">
						<h1>Simulation-Based Bayesian Inference for Cosmology</h1>
						<h3>Dark Universe Summer School, Les Houches, July 2025</h3>
					</div>
				</div>
				<hr>
				<div style="border-radius: 20px; background-color:rgba(0, 0, 0, 0);">
					<div class="container">
						<div class="col">
							<div align="left" style="margin-left: 20px;">
								<h2>Fran√ßois Lanusse</h2>
								<br>
								<img src="/talks/assets/CosmoStatDarkBK.png" class="plain"></img>
								<br>
							</div>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<br>
							<img src="/talks/assets/cnrs_logo.svg" class="plain" height="150"></img>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<img src="/talks/assets/aim.png" class="plain" height="150"></img>
						</div>
					</div>
					<div> slides at <a href="https://eiffl.github.io/talks/LesHouches2025">eiffl.github.io/talks/LesHouches2025</a> </div>
				</div>
			</section>
	<section>
	<section>
		<h3 class='slide-title'> The limits of traditional cosmological inference </h3>
		<br>
		<div class='container'>
			<div class='col'>
				<div style="position:relative; width:480px; height:30px; margin:0 auto;">
					<div class="fragment current-visible" style="position:absolute;top:0;" data-fragment-index="1">HSC cosmic shear power spectrum</div>
					<div class="fragment" style="position:absolute;top:0;" data-fragment-index="2">HSC Y1 constraints on $(S_8, \Omega_m)$</div>
				</div>
				<div style="position:relative; width:480px; height:300px; margin:0 auto;">
					<div class="fragment current-visible" style="position:absolute;top:0;left:0;" data-fragment-index="0">
						<img class="plain" data-src="/talks/assets/alonso_g1.png" />
						<img class="plain" data-src="/talks/assets/alonso_g2.png" />
					</div>
					<img class="fragment current-visible plain" data-src="/talks/assets/hsc_correlation_function.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
					<img class="fragment  plain" data-src="/talks/assets/hsc_constraints.png" style="position:absolute;top:0;left:0;" data-fragment-index="2" />
				</div>
				<div class="fragment" data-fragment-index="1" style="float:right; font-size: 20px">(Hikage et al. 2018)</div>
			</div>
	
			<div class='col'>
				<ul>
					<li class="fragment" data-fragment-index="0"> Measure the ellipticity $\epsilon = \epsilon_i + \gamma$ of all galaxies<br>
						$\Longrightarrow$ Noisy tracer of the weak lensing shear $\gamma$ </li>
					<br>
					<li class="fragment" data-fragment-index="1"> Compute <b class="alert">summary statistics</b> based on 2pt functions, <br>e.g. the <b>power spectrum</b> </li>
					<br>
					<li class="fragment" data-fragment-index="2"> Run an MCMC to recover a posterior on model parameters, using an <b class="alert">analytic likelihood</b>
						$$ p(\theta | x ) \propto \underbrace{p(x | \theta)}_{\mathrm{likelihood}} \ \underbrace{p(\theta)}_{\mathrm{prior}}$$
					</li>
				</ul>
			</div>
		</div>
	
		<div class="block fragment">
			<div class="block-title">
				Main limitation: the need for an explicit likelihood
			</div>
			<div class="block-content">
				We can only compute <b>from theory</b> the likelihood for <b class="alert">simple summary statistics</b> and on <b class="alert">large scales</b>
				<br>
				<br>
				<div class="fragment"> $\Longrightarrow$ We are dismissing a significant fraction of the information! </div>
			</div>
		</div>
	</section>
	
	<section>
		<h3 class='slide-title'>Full-Field Simulation-Based Inference</h3>
	
		<div class='container'>
			<div class='col'>
				<ul>
					<li data-fragment-index="0" > Instead of trying to analytically evaluate the likelihood of
						sub-optimal summary statistics, let us build a forward model of the full observables.<br>
						$\Longrightarrow$ <b class="alert">The simulator becomes the physical model</b>.
					</li>
					
				</ul>
	
				<br>
				<br>
				<br>
				<br>
	
				<div class="block fragment">
					<div class="block-title">
						Benefits of a forward modeling approach
					</div>
					<div class="block-content">
						<ul>
							<li> Fully exploits the information content of the data
								(aka "full field inference").
							</li>
	
							<br>
							<li> Easy to incorporate systematic effects.
							</li>
							<br>
							<li> Easy to combine multiple cosmological probes by joint simulations.
							</li>
						</ul>
					</div>
				</div>
			</div>
	
			<div class='col'>
				<div style="position:relative; width:600px; height:600px; margin:0 auto;">
					<img class=" plain" data-src="/talks/assets/forward_model.png" style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0" />
				</div>
			</div>
		</div>
	</section>
	
	<section>
		<h3 class="slide-title">...so why is this not mainstream?</h3>
			<img class="plain" data-src="/talks/assets/lfi_sim.png" style="width:1000px;"/>
	
				<div class="r-stack">
	
					<img class="plain fragment" data-src="/talks/assets/plot_massive_nu.png" style="width:1000px;"/>
	
						<div class="block fragment">
							<div class="block-title">
								The Challenge of Simulation-Based Inference
							</div>
							<div class="block-content">
								$$ p(x|\theta) = \int p(x, z | \theta) dz = \int p(x | z, \theta) p(z | \theta) dz $$
								Where $z$ are <b>stochastic latent variables</b> of the simulator.<br><br>
								$\Longrightarrow$ This <b class="alert">marginal likelihood is intractable</b>!
							</div>
						</div>
					</div>
	</section>
	</section>

	<section class="inverted" data-background="#000">
		<h2>How can we perform Bayesian inference if we cannot evaluate the likelihood?</h2>
	</section>

	<section>
		<h3 class="slide-title">Learning objectives for this lecture</h3>

		<ul>
		</ul>
	</section>

	<section>
		<h1>The Many Flavors of Bayesian Inference</h1>
		<hr>
	</section>


	<section>

	<section>
		<h3 class="slide-title">Back to the basics</h3>
		<br>
		<div class="block">
		<div class="block-title">
			Bayes' theorem
		</div>
		<div class="block-content">
			<div class="bayes-equation" style="margin: 10px 0;">
			$$\overbrace{p(\theta|d, \mathcal{M})}^{\text{posterior}} = \frac{ \overbrace{p(d|\theta, \mathcal{M})}^{\text{likelihood}} \ \overbrace{p(\theta|\mathcal{M})}^{\text{prior}}}{\underbrace{p(d|\mathcal{M})}_{\text{evidence}}}$$
			</div>
		<ul style="margin-top: 40px;">
			<li class="fragment"> The <strong class="prior">prior</strong> is our belief about the model parameters before observing the data.</li>
			<li class="fragment"> The <strong class="likelihood">likelihood</strong> is the probability distribution of data given particular model parameters $\theta$.</li>
			<li class="fragment"> The <strong class="posterior">posterior</strong> is the probability distribution of model parameters given particular data $d$.</li>
		</ul>
		</div>
	</div>
	</section>

	<section>
		<h3 class="slide-title">Classical Bayesian inference relies on tractable <b>posterior evaluation</b></h3>
		<iframe class="plain" data-src="https://flanusse.net/LesHouches2025/mcmc_banana" style="width: 1200px; height: 650px;"></iframe>
	</section>
	</section>
	

	<section>
		<h3 class="slide-title">Where do likelihoods come from?</h3>
	</section>


	<section>
		<h3 class="slide-title"> Simulators as Hierarchical Bayesian Models</h3>

		<img class="plain" data-src="/talks/assets/lfi_sim.png" style="width:1000px;"/>
		<ul>
			<li>If we have access to all latent variables $z$ of the simulator,
				then the <b class="alert">joint log likelihood $p(x | z, \theta)$ is explicit</b>.
			</li>

			<br>

			<li class="fragment"> We need to infer the joint posterior $p(\theta, z | x)$ before marginalization to
				yield $p(\theta | x) = \int p(\theta, z | x) dz$.<br>
				$\Longrightarrow$ Extremely difficult problem as <b>$z$ is typically very high-dimensional</b>.
			</li>

			<br>

			<li class="fragment"> Necessitates inference strategies with <b class="alert">access to gradients of the likelihood</b>.
				$$\frac{d \log p(x | z, \theta)}{d \theta} \quad ; \quad \frac{d \log p(x | z, \theta)}{d z}  $$
				For instance: Maximum A Posterior estimation, Hamiltonian Monte-Carlo, Variational Inference.
			</li>
			<br>
		</ul>

		<div class="fragment">$\Longrightarrow$ The only hope for explicit cosmological inference is to have <b class="alert">fully-differentiable cosmological simulations</b>!</div>
	</section>

	<section>
				<br>
				<br>
						<div class="block">
							<div class="block-title">
								How to perform inference over forward simulation models?
							</div>
							<div class="block-content">
								<br>
								<ul>
									<li class="fragment"> <b class="alert">Implicit Inference</b>: Treat the simulator as a black-box with only the ability to sample from the joint distribution 
										$$(x, \theta) \sim p(x, \theta)$$
										a.k.a.<br><ul>
											<li> <b>Simulation-Based Inference</b> (SBI)
											</li>
											<li> <b>Likelihood-free inference</b> (LFI)
											</li>
											<li> <b>Approximate Bayesian Computation</b> (ABC)
											</li>
										</ul>
									</li>

									<br>

									<li class="fragment"> <b class="alert">Explicit Inference</b>: Treat the simulator as a probabilistic model and perform inference over the joint posterior 
										$$p(\theta, z | x) \propto p(x | z, \theta) p(z, \theta) p(\theta) $$
										a.k.a.<br><ul>
											<li> <b>Bayesian Hierarchical Modeling</b> (BHM)
											</li>
										</ul>
									</li>
									<br>
								</ul>

							</div>
						</div>
						<div class="fragment">$\Longrightarrow$ For a given simulation model, both methods <b class="alert">should converge to the same posterior!</b></div>
	</section>



	<section>
		<h1>Neural Density Estimation</h1>
		<hr>
		<h3>The key to manipulating <b>Implicit Distributions</b></h3>
	</section>


	<!-- 
	What we are going to cover here:
		- What is a neural network fundamentally?
		- How can we use neural networks to model distributions?
		- How can we train a neural network to learn conditional distributions?
		- Flexible neural network models for learning distributions
		   = <b>Normalizing Flows</b>
	-->


			<section>
			<section>
				<h3 class="slide-title">What is a neural network?</h3>
			
					<p>Simplest architecture: Multilayer Perceptron (MLP)</p>
					
					<div class="container">
					<div class="col">
					<img data-src="/talks/assets/fnn.png" style="height:300px;"/>
					<img data-src="https://studymachinelearning.com/wp-content/uploads/2019/10/summary_activation_fn.png"</img>
					</div>
					<div class="col">
				<ul>
					<li> Series of <b>Dense</b> a.k.a <b>Fully connected</b> layers:
						$$ h = \sigma(W x + b)$$ 
						where:
						<ul>
							<li>$\sigma$ is the activation function (e.g. ReLU, Sigmoid, etc.)</li>
							<li>$W$ is a multiplicative weight matrix</li>
							<li>$b$ is an additive bias parameter</li>
						</ul>
					</li>
					<br>
					<li class="fragment"> This defines a <b class="alert">parametric non-linear function $f_\theta(x)$</b></li>
					</li>
					<br>
					<li class="fragment"> MLPs are <b class="alert">universal function approximators</b> 
						<br> <b>Nota bene</b>: only asymptotically true! 
					</li>
				</ul>
			</div>
		</section>

			<section>
				<h3 class="slide-title">How do you use it to approximate functions?</h3>

				<br>
				<br>
				<br>
				<br>
				<div class="container">

					<div class="col">
						<img data-src="/talks/assets/gradient_descent.webp" style="height:300px;"/>
					</div>

					<div class="col">

						<ul>
							<li>Assume a <b class="alert">loss function</b> that should be small for good approximations on a training set of data points $(x_i, y_i)$

								$$ \mathcal{L} = \sum_{i} ( y_i - f_\theta(x_i))^2 $$
							</li>
						
							<br>

							<li>Optimize the parameters $\theta$ to minimize the loss function by <b class="alert">gradient descent</b>
								$$ \theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L} $$
							</li>
						</ul>
					</div>
				</div>
				<br>
				<br>
				<br>
			</section>

			<section data-background-iframe="https://playground.tensorflow.org"></section>
			</section>

			<section>
			<section>
				<h3 class="slide-title">The task at hand: Density Estimation</h3>
				<br>
				<ul>
					<li>The goal of density estimation is to <b>estimate an <b class="alert">implicit</b> distribution $\mathbb{P}$</b>
						from which a <b>training set $X = \{x_0, x_1, \ldots, x_n \}$</b> is drawn.
					</li>
					<br>
					<li class='fragment'> Usually, this means building a parametric model $\mathbb{P}_\theta$
						that tries to be close to $\mathbb{P}$.
					</li>
				</ul>

				<br>
				<div class="container">
					<div class="col fragment fade-up">
						<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756538/pasted-from-clipboard.png" class="plain"></img>
						<br>
						True $\mathbb{P}$
					</div>

					<div class="col  fragment fade-up">
						<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756539/pasted-from-clipboard.png" class="plain"></img>
						<br>
						Samples $x_i \sim \mathbb{P}$
					</div>

					<div class="col  fragment fade-up">
						<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756554/pasted-from-clipboard.png" class="plain"></img>
						<br>
						Model $\mathbb{P}_\theta$
					</div>
				</div>
				<br>
				<ul>
					<li class="fragment"> Once trained, you can typically <b>sample from $\mathbb{P}_\theta$</b> and/or <b class="alert">evaluate the likelihood $p_\theta(x)$</b>.
					</li>
				</ul>

			</section>


			<section>
				<div class="container">
					<div class="col fragment" data-fragment-index="1">
							<img class="plain" data-src="/talks/assets/kl_divergence.gif"/>
							The <b>Kullback-Leibler divergence</b>:
							\begin{align}
							D_{KL}(p||q_\phi) &= \mathbb{E}_{x \sim p(x)} \left[ \log \frac{p(x)}{q_\phi(x)} \right] \\ \\
							&= - \mathbb{E}_{x \sim p(x)} \left[ \log q_\phi(x) \right] + \underbrace{H(p)}_{\text{const}}
							\end{align}								
					</div>

					<div class="col">
						<ul>
							<li><b>Step I</b>: We need a <b>parametric density model</b> $q_\phi(x)$<br><br>
								Simplest possible model, a <b class="alert">parametric Gaussian</b>:
								$$ q_\phi(x) = \mathcal{N}(x ; \mu, \Sigma) $$
								with parameters $\phi = \{\mu, \Sigma\}$
							</li>
							<br>
							<br>
							<li class="fragment" data-fragment-index="1"><b>Step II</b>: We need a tool to <b>compare distributions</b><br>
								
							</li>
							<br>
							<br>
							<li class="fragment" data-fragment-index="2"><b>Step III</b>: We optimize the parameters $\phi$ to minimize the NLL over a training set $X = \{x_1, x_2, \ldots, x_N\} \sim \mathbb{P}$ :
								$$ \mathcal{L}(\phi) = - \frac{1}{N} \sum_{i=1}^N \log q_\phi(x_i) $$
								$\Longrightarrow$ This will minimize the KL divergence and fit the model to the data.
							</li>
							<br>
							
						</ul>
					</div>
				</div>
				</section>

				<section>
					<iframe class="plain" data-src="https://flanusse.net/LesHouches2025/gaussian_mle_optimization" style="width: 1200px; height: 650px;"></iframe>				
				</section>
			</section>

			  <section>
			<section>
				<h3 class="slide-title">Your goto NDE in low dimensions (<100): The Normalizing Flows</h3>
				<br>
		<div class="container">
		<div class="col">
		  <img data-src="/talks/assets/flow_dinh_1.png" class="plain fragment fade-up" data-fragment-index="1"></img>
		  <img data-src="/talks/assets/flow_dinh_2.png" class="plain fragment fade-up" data-fragment-index="3"></img>

		  <br>
				 <div class="fragment fade-up" style="float:right; font-size: 20px" data-fragment-index="1">Dinh et al. 2016</div>
		</div>
		<div class="col">
				  <div class="block fragment fade-up" data-fragment-index="1">
				  <div class="block-title">
				   Normalizing Flows
				  </div>
				  <div class="block-content">
					<ul>
					  <li> Assumes a <b class="alert">bijective</b> mapping between
						data space $x$ and latent space $z$ with prior $p(z)$:
						$$ z = f_{\theta} ( x ) \qquad \mbox{and} \qquad x = f^{-1}_{\theta}(z)$$
					  </li>
					  <li class="fragment" data-fragment-index="2"> Admits an explicit marginal likelihood:
						$$ \log p_\theta(x) = \log p(z) + \log \left| \frac{\partial f_\theta}{\partial x}  \right|(x)    $$
					  </li>
					</ul>
				</div>
				</div>
				<br>
				  <br>
				  <div class="fragment">
				  $\Longrightarrow$ The challenge is in designing mappings $f_\theta$ that are both: 
				  <b>easy to invert, easy to compute the jacobian of</b>.</div>
				  <br>
				  <br>
		</div>
	</div>

		</section>

		<section>
			<h3 class="slide-title">One example of NF: RealNVP</h3>
			<br>
			<br>

			<div class="container">
				<div class="col">
					<img data-src="/talks/assets/realNVP_jacobian.png" style="height: 300px;"/>
					<br> Jacobian of an affine coupling layer
				</div>
				<div class="col">
					In a standard affine RealNVP, one Flow layer is defined as:

					$$ \begin{matrix} y_{1\ldots d} &=& x_{1 \ldots d} \\
					y_{d+1\ldots N} &=& x_{d+1 \ldots N} ‚äô \sigma_\theta(x_{1 \ldots d}) + \mu_\theta(x_{1 \ldots d})
					\end{matrix} $$
					where $\sigma_\theta$ and $\mu_\theta$ are unconstrained neural networks.
					<br>
					<br>
					We will call this layer an <b class="alert">affine coupling</b>.

				</div>


			</div>

			<br>
			<br>
			$\Longrightarrow$ This structure has the advantage that the Jacobian of this layer will be lower triangular which makes computing its determinant easy.
		</section>

					<section>
				<h3 class="slide-title">Your Turn!</h3>

				<br>
				<br>

				You can take a look at <a href="https://colab.research.google.com/github/EiffL/Tutorials/blob/master/NormalizingFlowsInJAX.ipynb">this notebook</a> to implement a Normalizing Flow in JAX+Flax+TensorFlow Probability

				<img data-src="/talks/assets/points.png"/>
				<br>
				<br>

				<br>
				<br>
			</section>
	</section>

			</section>


	<section>
		<h1>Implicit Inference</h1>
		<hr>
	</section>

		<section>
		   <h3 class="slide-title">Black-box Simulators Define Implicit Distributions</h3>
		   <img class="plain" data-src="/talks/assets/lfi_sim.png" style="width:750px;"/>
		   <ul>
			   <li>A black-box simulator <b class="alert">defines $p(x | \theta)$ as an implicit distribution</b>, you can <b>sample from it</b> but you cannot evaluate it.
			   </li>

			   <li>This gives us a procedure to sample from the <b>Bayesian joint distribution</b> $p(x, \theta)$: 
					$$(x, \theta)  \sim p(x | \theta) \  p(\theta)$$
			   </li>

			   <li class='fragment'> <b class="alert">Key Idea</b>: Use a <b>parametric distribution model to approximate an implicit distribution</b>.

				<ul>
					<li><b>Neural Likelihood Estimation</b>:
						$\mathcal{L}  = - \mathbb{E}_{(x,\theta)}\left[ \log p_\phi( x | \theta ) \right] $
					</li>
					<br>
					<li class="fragment"><b>Neural Posterior Estimation</b>:
						$\mathcal{L}  = - \mathbb{E}_{(x,\theta)}\left[ \log p_\phi( \theta | x ) \right] $
					</li>
					<br>
					<li class="fragment"><b>Neural Ratio Estimation</b>:
						$\mathcal{L} = - \mathbb{E}_{\begin{matrix} (x,\theta)~p(x,\theta) \\
															 \ \theta^\prime \sim p(\theta) \end{matrix}} \left[  \log r_\phi(x,\theta) + \log(1 - r_\phi(x, \theta^\prime))   \right] $
					</li>
				</ul>
			   </li>
		   </ul>
	   </section>

	   <section>
	   	<section>
			<h3 class="slide-title fragment">A closer look at <b>Neural Posterior Estimation</b> (NPE)</h3>

			<div class="container">
				<div class="col">
					<ul> <li> Building conditional neural density estimators <b class="alert"> $q_\phi(\theta | x)$ to approximate the posterior</b>
						<img class="plain" data-src="/talks/assets/MDN.png" style="height:500px" />
						<br>
						<div style="float:left; font-size: 20px">Bishop (1994)</div>
					</li>
					</ul>
				</div>

				<div class="col fragment">
					<ul> <li> Bounding the <b>KL divergence between true and approximate posterior</b> 
					<br>
					<br>
					<div class="fragment">$$D_{KL}(p(x | y) ||q_\phi(x | y)) = \mathbb{E}_{x \sim p(x |y)} \left[ \log \frac{p(x | y)}{q_\phi(x | y)} \right] \geq 0 $$</div>	
					<div class="fragment">$$ \leq \mathbb{E}_{y \sim p(y)} \mathbb{E}_{x \sim p(x |y)} \left[ \log \frac{p(x | y)}{q_\phi(x | y)} \right] $$</div>
					<div class="fragment">$$ \leq \boxed{ \mathbb{E}_{(y,x) \sim p(y, x)} \left[ - \log q_\phi(x | y) \right] } + cst $$</div>
					<br>
					<br>
					<div class="fragment">
					$\Longrightarrow$ Minimizing the <b class="alert">Negative Log Likelihood</b> (NLL) over the joint distribution $p(x, y)$
					leads to minimizing the KL divergence between the model posterior $q_{\phi}(x|y)$ and true posterior $p(x | y)$.
					</div>
					<br>
					<br>
				</li>
					</ul>
				</div>
			</div>
		</section>
			<section>
				<h3 class="slide-title">The NPE recipe</h3>
				<ul>
					<li class="fragment fade-up"> I assume a forward model of the observations:
						\begin{equation}
						p( x ) = p(x | \theta) \ p(\theta) \nonumber
						\end{equation}
						All I ask is the ability to sample from the model, to obtain $\mathcal{D} = \{x_i, \theta_i \}_{i\in \mathbb{N}}$
					</li>
					<br>
					<li class="fragment fade-up"> I am going to assume $q_\phi(\theta | x)$ a <b>parametric conditional density</b>
					</li>
					<br>
					<li class="fragment fade-up">Optimize the parameters $\phi$ of $q_{\phi}$ according to
						\begin{equation}
						\min\limits_{\phi} \sum\limits_{i} - \log q_{\phi}(\theta_i | x_i) \nonumber
						\end{equation}
						In the limit of <b>large number of samples</b> and <b>sufficient flexibility</b>
						\begin{equation}
						\boxed{q_{\phi^\ast}(\theta | x) \approx p(\theta | x)} \nonumber
						\end{equation}
					</li>
				</ul>
	
				<div style="position:relative; height:30px; margin-left: 4em;">
					<div class="fragment current-visible" style="position:absolute;top:0;"> $\Longrightarrow$ One can asymptotically recover the posterior by
						optimizing a <b>parametric estimator</b> over<br> the <b>Bayesian joint distribution</b>
					</div>
					<div class="fragment" style="position:absolute;top:0;"> $\Longrightarrow$ One can asymptotically recover the posterior by
						optimizing a <b class="alert">Deep Neural Network</b> over<br> a <b class="alert">simulated training set</b>.
					</div>
				</div>
			</section>

		</section>

	   <section>
		<h3 class="slide-title">A variety of algorithms</h3>
		<div style="float:right; font-size: 20px">Lueckmann, Boelts, Greenberg, Gon√ßalves, Macke (2021) <a href="https://arxiv.org/abs/2101.04653"><img src="https://img.shields.io/badge/stat.ML-arXiv%3A2101.04653-B31B1B.svg" class="plain"
					style="height:25px;vertical-align:middle;" /></a></div>
		<img class="plain" data-src="/talks/assets/sbibm_comparison.png"/>

		<br>
		<br>
			A few important points:
			<br><br>
			<ul>
				<li class="fragment"> <b>Amortized</b> inference methods, which estimate $p(\theta | x)$, can greatly speed up posterior estimation once trained.
				</li>
				<br>

				<li class="fragment"> <b>Sequential</b> SBI algorithms can actively sample simulations needed to refine the inference.
				</li>
			</ul>
			<br>
			<br>
			<div>Checkout this excellent package: <a href="https://www.mackelab.org/sbi/">https://www.mackelab.org/sbi</a> </div>
	</section>
	

	   <section>
			<h3 class="slide-title">A Practical Recipe for Careful Simulation-Based Inference</h3>
			<figure>
				<blockquote>
					Estimating conditional densities<br> in high dimensions is hard...
				</blockquote>
				<figcaption style="align-items: right;">
					<em>@EiffL - every 2 or 3 days</em>
				</figcaption>
			</figure>
			<br><br>
			<div class="fragment">
			To be more robust, you can decompose the problem into two tasks:<br><br>
			<ul>
				<li class="fragment"><b>Step I - <b class="alert">Dimensionality Reduction</b></b>: Compress your observables $x$ to a low dimensional <b>summary statistic $y$</b>
				
					<img class="plain" data-src="/talks/assets/lfi_sim_sum.png" />
				
				</li>

				<br>

				<li class="fragment"><b>Step II - <b class="alert">Conditional Density Estimation</b></b>: Estimate the posterior $p(\theta | y)$ using SBI from the low dimensional summary statistic $y$.
				</li>
			</ul> 
			</div>
		</section>

		<section>
			<h3 class="slide-title">The Case for Dimensionality Reduction</h3> 
			<ul>
				<li>In the case of <b>Neural Posterior Estimation</b>

					<div class="container">
						<div class="col">
							<img class="plain" data-src="/talks/assets/noisy_map.png"  style="width: 200px;"/>
						</div>
						<div class="col">
							<img class="plain" data-src="/talks/assets/generic_network_inv.png" style="height: 200px;"/>
						</div>

						<div class="col">
							$p(\theta | x)$
						</div>
					</div>
<div class="fragment">$\Longrightarrow$ <b class="alert">Dimensionality reduction already happens implicitly</b> in the network.</div>
				</li>

				<li class="fragment">In the case of <b>Neural Likelihood Estimation</b>
					<div class="container">
						<div class="col">
							<img class="plain" data-src="/talks/assets/plot_massive_nu.png" />
							$x \sim p(x|\theta)$
						</div>
						<div class="col">
							<img class="plain" data-src="/talks/assets/generic_network.png" style="height: 200px;" />
						</div>

						<div class="col">
							$\theta$
						</div>
					</div>
					<div class="fragment">$\Longrightarrow$ This is equivalent to <b class="alert">learning a perfect emulator</b> for the high-dimensional outputs of your numerical simulator.</div> 
				</li>

			</ul>
		</section>


		<section class="inverted" data-background="#000">
			<h2>How can we lower the dimensionality of the problem <b>without</b> degrading our constraining power?</h2>
		</section>

		<section>
		<section>
			<h3 class="slide-title">Automated Neural Summarization</h3>
		  <img class="plain" data-src="/talks/assets/lfi_sim_sum.png" />
				<ul>
					<li> Introduce a parametric function $f_\varphi$ to <b class="alert">reduce the dimensionality of the
						data while preserving information</b>.
					</li>
				</ul>
				<div class="container">
					<div class="col">
						<div class="r-stack">
							<img class="plain fragment"  data-fragment-index="0"  data-src="/talks/assets/mutual_information.png" />
							<!-- <img class="plain fragment" data-fragment-index="1"  data-src="/talks/assets/imnn.png" /> -->
						</div>
						<!-- <div class="fragment" style="float:right; font-size: 15px"  data-fragment-index="1"> Makinen, Charnock, Alsing, Wandelt (2021) <a href="https://arxiv.org/abs/2107.07405"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2107.07405-B31B1B.svg" class="plain"
								style="height:20px;vertical-align:middle;" /></a></div> -->
	
					</div>
					<div class="col">
									<div class="block fragment" data-fragment-index="0">
										<div class="block-title">
											Information point of view
										</div>
										<div class="block-content">
											<ul>
												<li class="fragment"> Summary statistics <b>$y$ is sufficient for $\theta$</b> if
													$$ I(Y; \Theta) = I(X; \Theta) \Leftrightarrow p(\theta | x ) = p(\theta | y) $$
												</li>
												<li class="fragment" > Variational <b>Mutual Information Maximization</b>
												  $$ \mathcal{L} \ = \ \mathbb{E}_{x, \theta} [ \log q_\phi(\theta | y=f_\varphi(x)) ] \leq  I(Y; \Theta) $$
												  (Barber & Agakov variational lower bound)
														<div style="float:right; font-size: 15px"> Jeffrey, Alsing, <b>Lanusse</b> (2021) <a href="https://arxiv.org/abs/2009.08459"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2009.08459-B31B1B.svg" class="plain"
																	style="height:20px;vertical-align:middle;" /></a></div>
												</li>
	
												<!-- <li class="fragment" data-fragment-index="1" > Information Maximization Neural Network
													$$\mathcal{L} \ = \ - | \det \mathbf{F} |  \ \mbox{with} \ \mathbf{F}_{\alpha, \beta} = tr[ \mu_{\alpha}^t C^{-1} \mu_{\beta} ] $$
													<div style="float:right; font-size: 15px"> Charnock, Lavaux, Wandelt (2018) <a href="https://arxiv.org/abs/1802.03537"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1802.03537-B31B1B.svg" class="plain"
															style="height:20px;vertical-align:middle;" /></a></div>
												</li> -->
											</ul>
										</div>
									</div>
					</div>
				</div>
		</section>
		<section>
			<h3 class="slide-title">Another Approach: maximizing the Fisher information</h3>
				
			<img class="plain " data-fragment-index="1"  data-src="/talks/assets/imnn.png" />

			Information Maximization Neural Network (IMNN)
													$$\mathcal{L} \ = \ - | \det \mathbf{F} |  \ \mbox{with} \ \mathbf{F}_{\alpha, \beta} = tr[ \mu_{\alpha}^t C^{-1} \mu_{\beta} ] $$
													<div style="float:right; font-size: 15px"> Charnock, Lavaux, Wandelt (2018) <a href="https://arxiv.org/abs/1802.03537"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1802.03537-B31B1B.svg" class="plain"
															style="height:20px;vertical-align:middle;" /></a></div>


		</section>
		</section>


	<section>
		<h3 class="slide-title">Conventional Recipe for Full-Field Implicit Inference...</h3>
		<br>
		<img class="plain" data-src="/talks/assets/lfi_sim_sum.png" />
		<div class="block">
			<div class="block-title">
				A two-steps approach to Implicit Inference
			</div>
			<div class="block-content">
				<ul>
					<li> Automatically <b class="alert">learn</b> an <b>optimal low-dimensional summary statistic</b>
						$$y = f_\varphi(x) $$
					</li>

					<li class="fragment"> Use Neural Density Estimation to either:
						<ul>
							<li>build an <b>estimate $p_\phi$ of the likelihood function $p(y \ | \ \theta)$</b> (Neural Likelihood Estimation)

							</li>
							<br>

							<li>build an <b>estimate $p_\phi$ of the posterior distribution $p(\theta \ | \ y)$</b> (Neural Posterior Estimation)

							</li>
						</ul>
					</li>
				</ul>
			</div>
		</div>
	</section>

	<section>
		<section>
			<h3 class="slide-title">The proof is in the pudding: validation on log-normal lensing simulations</h3>
			<div class="container">
				<div style="float:right; font-size: 20px">
				<a href="https://arxiv.org/abs/2407.10877"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2407.10877-B31B1B.svg" class="plain" style="height:25px;" /></a>

				</div>
			</div>
			<div class="container">
				<div class="col"> 
					<!-- <div class="container"> -->
						<!-- <div class="col">
							<img data-src="https://avatars.githubusercontent.com/u/72620117?v=4" style='width:150px; height:150px;'></img>
							<img data-src="/talks/assets/justine.jpeg" style='width:150px; height:150px;'></img>
							<br>
							<small>Denise Lanzieri (left) and Justine Zeghal (right) </small>
						</div>
	
						<div class="col"> -->
							<img data-src="/talks/assets/github.png" class="plain" style="height:70px" />
							<br>
							<a href="https://github.com/DifferentiableUniverseInitiative/sbi_lens">DifferentiableUniverseInitiative/sbi_lens</a><br>
							JAX-based log-normal lensing simulation package 
						<!-- </div> -->
	
					 <!-- </div> -->
					<img data-src="/talks/assets/mass_map_tomo.png" />
					<ul>
						<li>10x10 deg$^2$ maps at LSST Y10 quality, conditioning the log-normal shift
							parameter on $(\Omega_m, \sigma_8, w_0)$
						</li>
						<br>
						<li class="fragment" data-fragment-index="0"> Infer full-field posterior on cosmology:
							<ul>
								<li> <b>explicitly</b> using an Hamiltonian-Monte Carlo (NUTS) sampler 
									</li>
								<li class="fragment" data-fragment-index="1"> <b class="alert">implicitly</b> using a <b>learned summary statistics</b> and conditional density estimation.
							</li>
						</li>
					</ul>
					
				</div>
				<div class="col r-stack">
					<img class="fragment current-visible" data-fragment-index="0" data-src="/talks/assets/compare_ff_ps_contour_plot_multi_tomo_bins.png"/>
					<img class="fragment" data-fragment-index="1" data-src="/talks/assets/compare_contour_plot_multi_tomo_bins.png"/>
				</div>
			</div>
		</section>
	
	
		<section>
			<img data-src="/talks/assets/find-the-difference.png" style="height: 700px;" />
		</section>
		</section>
				
		<section>
			<h3 class="slide-title">Note: not all compression techniques are equivalent!</h3>
			<div class="container">
				<div class="col">
					<img data-src="/talks/assets/contours_mse_vmim.png" style="height: 700px;" />
				</div>
				<div class="col">
					<ul>
						<li> Comparison to posteriors obtained with same neural compression architecture, but <b>different loss function</b>:
							$$\mathcal{L}_{MSE} = \mathbb{E}_{(x,\theta)} || f_\varphi(x) - \theta ||_2^2 $$	
						</li>
					</ul>
					<br>
					<br>
					<img data-src="/talks/assets/fom_denise.png"/>
				</div>
		</section>

	<section>
		<h3 class="slide-title">Our humble beginnings: Likelihood-Free Parameter Inference with DES SV...</h3>
		<div class="container">
			<div class="col">
				<div style="float:right; font-size: 20px"> Jeffrey, Alsing, <b>Lanusse</b> (2021) <a href="https://arxiv.org/abs/2009.08459"><img src="https://img.shields.io/badge/astro--ph.CO-2009.08459-B31B1B.svg" class="plain"
							style="height:25px;vertical-align:middle;" /></a></div>
			</div>
		</div>

		<div class="container">
			<div class="col">
				<img class="plain" data-src="/talks/assets/ks_sv.png" style="height:550px;"></img>
			</div>

			<div class="col r-stack">
				<div class="fragment current-visible">
				<img class="plain" data-src="/talks/assets/orthant.png" style="height:300px;" />
				<img class="plain" data-src="/talks/assets/sim_params.png" style="height:300px;" /><br>
				Suite of N-body + raytracing simulations: $\mathcal{D}$
			</div>

			<div class="fragment current-visible">
				<img class="plain" data-src="/talks/assets/jeffrey_model.png" style="height:550px" /><br>
			</div>

			<div class="fragment">
				<img class="plain" data-src="/talks/assets/jeffrey_s8.png" />
			</div>

			</div>
		</div>
	</section>
	
		<section>
			<h3 class="slide-title"> $w$CDM analysis of KiDS-1000 Weak Lensing (Fluri et al. 2022)</h3>

			<div class="container">
				<div class="col">
					<div style="float:right; font-size: 20px"> Fluri, Kacprzak, Lucchi, Schneider, Refregier, Hofmann (2022)
						<a href="https://arxiv.org/abs/2201.07771"><img src="https://img.shields.io/badge/astro--ph.CO-2201.07771-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
					</div>
				</div>
			</div>
			<br>
			<br>
		<div class="container">
			<div class="col">
				KiDS-1000 footprint and simulated data
				<img class="plain" data-src="/talks/assets/fluri_sky.png"  /><br>

				<ul>
					<li><b>Neural Compressor</b>: Graph Convolutional Neural Network on the Sphere <br> Trained by Fisher information maximization.
					</li>
				</ul>
			</div>
		
			<div class="col fragment">
				<img class="plain" data-src="/talks/assets/fluri_contours.png" /><br>
			</div>
		</div>
		</section>


		<section>
				<section data-background-video="/talks/assets/simbig.mp4" 
				data-background-video-loop data-background-video-muted data-background-size="contain">
				<h3 class="slide-title" style="background-color: rgba(0, 0, 0, 0.7);">SIMBIG: Field-level SBI of Large Scale Structure (Lemos et al. 2023)</h3>
				<br>
				<br>	<br>
				<br>	<br>
				<br>	<br>
				<br>	<br>
				<br>	<br>
				<br>	<br>
				<br>	<br>
				<br>	<br>
				<br>
				BOSS CMASS galaxy sample: Data vs Simulations<br>
				<ul>
					<li>20,000 simulated galaxy samples at 2,000 cosmologies </li>
				</ul>
				<div class="container">
					<div class="col">
						<div style="float:right; font-size: 20px"> Hahn et al. (2022)
							<a href="https://arxiv.org/abs/2211.00723"><img src="https://img.shields.io/badge/astro--ph.CO-2211.00723-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
						</div>
					</div>
				</div>
				</section>
		
				<section>
					<div class="container">
		
						<div class="col">
							
							<div style="float:right; font-size: 25px"> <a href="https://ml4astro.github.io/icml2023/assets/48.pdf">Lemos et al. (2023)</a>
							</div>
						</div>
					</div>
					<img class="plain" data-src="/talks/assets/simbig_method.png" style="width:1000px;"/>
		
		
					<img class="plain fragment" data-src="/talks/assets/simbig_contours.png" style="width:800px;"/>
				</section>
				</section>
		
	<section>
		<h3 class="slide-title">Finally, SBI has reached the mainstream: Official DES year 3 SBI wCDM results</h3>
		<div class="container">
			<div class="col">
				<div style="float:right; font-size: 20px"> Jeffrey et al. (2024)
					<a href="https://arxiv.org/abs/2403.02314"><img src="https://img.shields.io/badge/astro--ph.CO-2403.02314-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
				</div>
			</div>
		</div>
		<div class="container">
			<div class="col">
				<div class="r-stack">
					<img class="plain fragment fade-out" data-fragment-index="0" data-src="/talks/assets/gower_street.png" />

					<img class="plain fragment" data-fragment-index="0" data-src="/talks/assets/des_y3_gower.png" style="height:550px" />
				</div>
			</div>

			<div class="col">
					<img class="plain fragment fade-up" data-fragment-index="1" data-src="/talks/assets/des_y3_cnn.png" style="height: 500px" /><br>
				</div>
			</div>
		<br> <br>
	</section>


	<section class="inverted" data-background="#000">
		<h2>Can we just retire all conventional likelihood-based analyses?</h2>
	</section>


		<section data-background-image="https://media1.giphy.com/media/MCZ39lz83o5lC/giphy.gif?cid=ecf05e47itrwtb66ts0bcauq0659d605b2qhhn8r9q58eo75&ep=v1_gifs_related&rid=giphy.gif&ct=g">
		</section>

		<section>
			<h3 class="slide-title">Example of unforeseen impact of shortcuts in simulations</h3>
			<div class="container">
				<div class="col">
					<div style="float:right; font-size: 20px"> Gatti, Jeffrey, Whiteway et al. (2023)
						<a href="https://arxiv.org/abs/2307.13860"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2307.13860-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
					</div>
				</div>
			</div>

			<div class="container">
				<div class="col">
					<img class="plain" data-src="/talks/assets/lensing_3d.png" style="height: 450px;"/>

					<br>Is it ok to <b>distribute lensing source galaxies randomly</b> in simulations, or should they be clustered?
				</div>

				<div class="col fragment ">
					<img class="plain" data-src="/talks/assets/gatti_clustering.png" style="height: 500px;"/>
				</div>

			</div>
			<br>
			<div class="fragment">$\Longrightarrow$ An <b class="alert">SBI analysis could be biased</b> by this effect and <b>you would never know it!</b> 
			</div>
		</section>

		<section>
			<!-- <h3 class="slide-title">It's worse than you think!</h3> -->
			<div class="container">
				<div class="col">
					<iframe height="720" width="1280" src="https://www.youtube.com/embed/tSUFEIEPB3U?si=0-0000000000000000" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
				</div>
			</div>

		</section>

		<section>
			<h3 class="slide-title">How much <b>usable</b> information is there beyond the power spectrum?</h3>

			<div class="container">
				<div class="col">
					<div style="float:left; font-size: 20px"> Chisari et al. (2018)
						<a href="https://arxiv.org/abs/1801.08559"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A1801.08559-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
					</div>
					<img class="plain" data-src="/talks/assets/chisari_feedback.png" style="height: 375px;"/>
					<br> Ratio of power spectrum in hydrodynamical simulations vs. N-body simulations
				</div>
				<div class="col fragment">
					<div style="float:right; font-size: 20px"> Secco et al. (2021)
						<a href="https://arxiv.org/abs/2105.13544"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2105.13544-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
					</div>
					<img class="plain" data-src="/talks/assets/des_y3_cosmic_shear.png" style="height: 400px;"/>
					<br>DES Y3 Cosmic Shear data vector
				</div>
			</div>
			<br>
			<div class="fragment">$\Longrightarrow$ Can we find non-Gaussian information that is not affected by baryons?</div>
		</section>

        <section>
			<h3 class="slide-title">takeways</h3>
  
			<ul>
			  <li>
				SBI <b>automatizes inference</b> over
				numerical simulators.
				<ul>
				  <li>
					Turns both summary extraction and inference problems into an
					<b class="alert">optimization problems</b>
				  </li>
				  <li><b>Deep learning allows us to solve that problem!</b></li>
				</ul>
			  </li>
  
			  <br />
			  <li class="fragment">
				In the context of upcoming surveys, this techniques provides
				<b class="alert">many advantages</b>:
				<ul>
				  <li>
					<b>Amortized inference</b>: near instantaneous parameter
					inference, extremely useful for time-domain.
				  </li>
				  <li>
					<b>Optimal information extraction</b>: no longer need for
					restrictive modeling assumptions needed to obtain tractable
					likelihoods.
				  </li>
				</ul>
			  </li>
			  <br />
			</ul>
  
			<br />
			<br />
			<div class="block fragment">
			  <div class="block-title">
				Will we be able to exploit all of the information content of
				LSST, Euclid, DESI?
			  </div>
			  <div class="block-content">
				$\Longrightarrow$ Not rightaway, but <b>it is not the fault of Deep
				Learning!</b>
				<br />
				<br />
				<ul>
				  <li class="fragment">
					Deep Learning has redefined the limits of our statistical
					tools, creating
					<b>additional demand on the accuracy of simulations</b> far
					beyond the power spectrum.
				  </li>
				  <br />
				  <li class="fragment">
					Neural compression methods have the downside of being opaque.
					It is <b>much harder to detect unknown systematics</b>.
				  </li>
				  <br />
				  <li class="fragment">
					We will need a significant number of
					<b>large volume, high resolution</b> simulations.
				  </li>
				</ul>
			  </div>
			</div>
			<br />
  
			<br />
  
			<br />
			<br />
		  </section>

		<section>
			<h1> Extra slides: Examples of other applications </h1>
		</section>

		<section>
			<section>
				<h3 class="slide-title">Example of application: Constraining Dark Matter Substructures</h3>
				<div class="container">
					<div class="col">
						<div style="float:right; font-size: 20px">
							Brehmer, Mishra-Sharma, Hermans, Louppe, Cranmer (2019) <a href="https://arxiv.org/abs/1909.02005"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A1909.02005-B31B1B.svg" class="plain"
									style="height:25px;vertical-align:middle;" /></a></div>
					</div>
				</div>

				<div class="r-stack">
					<img data-src="/talks/assets/Brehmer2019a.png" style='height:500px;'/>
					<img class="fragment" data-src="/talks/assets/Brehmer2019b.png" style='height:500px;'/>
					<img class="fragment" data-src="/talks/assets/Brehmer2019.gif" style="height:500px;"/>
				</div>

			</section>

			<section>
				<h3 class="slide-title">Example of application: Infering Microlensing Event Parameters</h3>
				<div class="container">
					<div class="col">
						<div style="float:right; font-size: 20px"> Zhang, Bloom, Gaudi, <b>Lanusse</b>, Lam, Lu (2021) <a href="https://arxiv.org/abs/2102.05673"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2102.05673-B31B1B.svg" class="plain"
									style="height:25px;vertical-align:middle;" /></a></div>
					</div>
				</div>
				<div class="r-stack">
				<div class="fragment current-visible">
					<img data-src="/talks/assets/Zhang2021a.png" style="height:500px"/>
				</div>

				<div class="fragment">
					<img data-src="/talks/assets/Zhang2021b.png" style="height:500px"/>
				</div>

			</div>
			</section>
		</section>


		</div>
	</div>

	<style>
		.reveal .slides {
			border: 5px solid red;
			min-height: 100%;
			width: 128mm;
			height: 96mm;
		} 

		.reveal .block {
			background-color: #191919;
			margin-left: 20px;
			margin-right: 20px;
			text-align: left;
			padding-bottom: 0.1em;
		}

		.reveal .block-title {
			background-color: #333333;
			padding: 8px 35px 8px 14px;
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .block-content {
			padding: 8px 35px 8px 14px;
		}

		.reveal .slide-title {
			border-left: 5px solid white;
			text-align: left;
			margin-left: 20px;
			padding-left: 20px;
		}

		.reveal .alert {
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .inverted {
			filter: invert(100%);
		}

		/*
	/* .reveal .alert {
	padding:8px 35px 8px 14px; margin-bottom:18px;
	text-shadow:0 1px 0 rgba(255,255,255,1);
	border:5px solid #FFAA7F;
	-webkit-border-radius: 14px; -moz-border-radius: 14px;
	border-radius:14px
	background-position: 10px 10px;
	background-repeat: no-repeat;
	background-size: 38px;
	padding-left: 30px; /* 55px; if icon
	}
	.reveal .alert-block {padding-top:14px; padding-bottom:14px}
	.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
	/*.reveal .alert li {margin-top: 1em}
	.reveal .alert-block p+p {margin-top:5px} */
	
	/* Bayes theorem colors */
	.bayes-equation {
		font-size: 1.5em !important;
		padding: 10px;
		background-color: rgba(0, 0, 0, 0.2);
		border-radius: 10px;
	}
	</style>


	<script src="reveal.js/dist/reveal.js"></script>
	<script src="reveal.js/plugin/notes/notes.js"></script>
	<script src="reveal.js/plugin/markdown/markdown.js"></script>
	<script src="reveal.js/plugin/highlight/highlight.js"></script>
	<script src="reveal.js/plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			controls: true,

			//center: false,
			hash: true,

			// Visibility rule for backwards navigation arrows; "faded", "hidden"
			// or "visible"
			controlsBackArrows: 'hidden',

			// Display a presentation progress bar
			progress: true,

			// Display the page number of the current slide
			slideNumber: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// The "normal" size of the presentation, aspect ratio will be preserved
			// when the presentation is scaled to fit different resolutions. Can be
			// specified using percentage units.
			width: 1280,
			height: 720,

			// Factor of the display size that should remain empty around the content
			margin: 0.1,

			// Bounds for smallest/largest possible scale to apply to content
			minScale: 0.2,
			maxScale: 1.5,

			autoPlayMedia: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],

			dependencies: [{
					src: 'reveal.js/plugin/markdown/marked.js'
				},
				{
					src: 'reveal.js/plugin/markdown/markdown.js'
				},
				{
					src: 'reveal.js/plugin/notes/notes.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/math/math.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/reveal.js-d3/reveald3.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/Chart.min.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/csv2chart.js'
				},
				{
					src: 'reveal.js/plugin/highlight/highlight.js',
					async: true
				},
			]

		});
	</script>
</body>

</html>
