<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Simulation-Based Bayesian Inference for Cosmology</title>

	<meta name="description" content="Les Houches Summer School, July 2025">
	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<link rel="stylesheet" href="reveal.js/dist/theme/darkenergy.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section data-background-iframe="background.html">
				<div class="container">
					<div class="title" style="border-radius: 20px; background-color:rgba(0, 0, 0, 0.4);">
						<h1>Simulation-Based Bayesian Inference for Cosmology</h1>
						<h3>Dark Universe Summer School, Les Houches, July 2025</h3>
					</div>
				</div>
				<hr>
				<div style="border-radius: 20px; background-color:rgba(0, 0, 0, 0);">
					<div class="container">
						<div class="col">
							<div align="left" style="margin-left: 20px;">
								<h2>Fran√ßois Lanusse</h2>
								<br>
								<img src="/talks/assets/CosmoStatDarkBK.png" class="plain"></img>
								<br>
							</div>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<br>
							<img src="/talks/assets/cnrs_logo.svg" class="plain" height="150"></img>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<img src="/talks/assets/aim.png" class="plain" height="150"></img>
						</div>
					</div>
					<div> slides at <a href="https://eiffl.github.io/talks/LesHouches2025">eiffl.github.io/talks/LesHouches2025</a> </div>
				</div>
			</section>
	<section>
	<section>
		<h3 class='slide-title'> The limits of traditional cosmological inference </h3>
		<br>
		<div class='container'>
			<div class='col'>
				<div style="position:relative; width:480px; height:30px; margin:0 auto;">
					<div class="fragment current-visible" style="position:absolute;top:0;" data-fragment-index="1">HSC cosmic shear power spectrum</div>
					<div class="fragment" style="position:absolute;top:0;" data-fragment-index="2">HSC Y1 constraints on $(S_8, \Omega_m)$</div>
				</div>
				<div style="position:relative; width:480px; height:300px; margin:0 auto;">
					<div class="fragment current-visible" style="position:absolute;top:0;left:0;" data-fragment-index="0">
						<img class="plain" data-src="/talks/assets/alonso_g1.png" />
						<img class="plain" data-src="/talks/assets/alonso_g2.png" />
					</div>
					<img class="fragment current-visible plain" data-src="/talks/assets/hsc_correlation_function.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
					<img class="fragment  plain" data-src="/talks/assets/hsc_constraints.png" style="position:absolute;top:0;left:0;" data-fragment-index="2" />
				</div>
				<div class="fragment" data-fragment-index="1" style="float:right; font-size: 20px">(Hikage et al. 2018)</div>
			</div>
	
			<div class='col'>
				<ul>
					<li class="fragment" data-fragment-index="0"> Measure the ellipticity $\epsilon = \epsilon_i + \gamma$ of all galaxies<br>
						$\Longrightarrow$ Noisy tracer of the weak lensing shear $\gamma$ </li>
					<br>
					<li class="fragment" data-fragment-index="1"> Compute <b class="alert">summary statistics</b> based on 2pt functions, <br>e.g. the <b>power spectrum</b> </li>
					<br>
					<li class="fragment" data-fragment-index="2"> Run an MCMC to recover a posterior on model parameters, using an <b class="alert">analytic likelihood</b>
						$$ p(\theta | x ) \propto \underbrace{p(x | \theta)}_{\mathrm{likelihood}} \ \underbrace{p(\theta)}_{\mathrm{prior}}$$
					</li>
				</ul>
			</div>
		</div>
	
		<div class="block fragment">
			<div class="block-title">
				Main limitation: the need for an explicit likelihood
			</div>
			<div class="block-content">
				We can only compute <b>from theory</b> the likelihood for <b class="alert">simple summary statistics</b> and on <b class="alert">large scales</b>
				<br>
				<br>
				<div class="fragment"> $\Longrightarrow$ We are dismissing a significant fraction of the information! </div>
			</div>
		</div>
	</section>
	
	<section>
		<h3 class='slide-title'>Full-Field Simulation-Based Inference</h3>
	
		<div class='container'>
			<div class='col'>
				<ul>
					<li data-fragment-index="0" > Instead of trying to analytically evaluate the likelihood of
						sub-optimal summary statistics, let us build a forward model of the full observables.<br>
						$\Longrightarrow$ <b class="alert">The simulator becomes the physical model</b>.
					</li>
					
				</ul>
	
				<br>
				<br>
				<br>
				<br>
	
				<div class="block fragment">
					<div class="block-title">
						Benefits of a forward modeling approach
					</div>
					<div class="block-content">
						<ul>
							<li> Fully exploits the information content of the data
								(aka "full field inference").
							</li>
	
							<br>
							<li> Easy to incorporate systematic effects.
							</li>
							<br>
							<li> Easy to combine multiple cosmological probes by joint simulations.
							</li>
						</ul>
					</div>
				</div>
			</div>
	
			<div class='col'>
				<div style="position:relative; width:600px; height:600px; margin:0 auto;">
					<img class=" plain" data-src="/talks/assets/forward_model.png" style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0" />
				</div>
			</div>
		</div>
	</section>
	
	<section>
		<h3 class="slide-title">...so why is this not mainstream?</h3>
			<img class="plain" data-src="/talks/assets/lfi_sim.png" style="width:1000px;"/>
	
				<div class="r-stack">
	
					<img class="plain fragment" data-src="/talks/assets/plot_massive_nu.png" style="width:1000px;"/>
	
						<div class="block fragment">
							<div class="block-title">
								The Challenge of Simulation-Based Inference
							</div>
							<div class="block-content">
								$$ p(x|\theta) = \int p(x, z | \theta) dz = \int p(x | z, \theta) p(z | \theta) dz $$
								Where $z$ are <b>stochastic latent variables</b> of the simulator.<br><br>
								$\Longrightarrow$ This <b class="alert">marginal likelihood is intractable</b>!
							</div>
						</div>
					</div>
	</section>
	</section>

	<section class="inverted" data-background="#000">
		<h2>How can we perform Bayesian inference if we cannot evaluate the likelihood?</h2>
	</section>

	<section>
		<h3 class="slide-title">Learning objectives for this lecture</h3>

		<ul>
		</ul>
	</section>

	<section>
		<h1>The Many Flavors of Bayesian Inference</h1>
		<hr>
	</section>


	<section>

	<section>
		<h3 class="slide-title">Back to the basics</h3>
		<br>
		<div class="block">
		<div class="block-title">
			Bayes' theorem
		</div>
		<div class="block-content">
			<div class="bayes-equation" style="margin: 10px 0;">
			$$\overbrace{p(\theta|d, \mathcal{M})}^{\text{posterior}} = \frac{ \overbrace{p(d|\theta, \mathcal{M})}^{\text{likelihood}} \ \overbrace{p(\theta|\mathcal{M})}^{\text{prior}}}{\underbrace{p(d|\mathcal{M})}_{\text{evidence}}}$$
			</div>
		<ul style="margin-top: 40px;">
			<li class="fragment"> The <strong class="prior">prior</strong> is our belief about the model parameters before observing the data.</li>
			<li class="fragment"> The <strong class="likelihood">likelihood</strong> is the probability distribution of data given particular model parameters $\theta$.</li>
			<li class="fragment"> The <strong class="posterior">posterior</strong> is the probability distribution of model parameters given particular data $d$.</li>
		</ul>
		</div>
	</div>
	</section>

	<section>
		<h3 class="slide-title">Classical Bayesian inference relies on tractable <b>posterior evaluation</b></h3>
		<iframe class="plain" data-src="https://flanusse.net/LesHouches2025/mcmc_banana" style="width: 1200px; height: 650px;"></iframe>
	</section>
	</section>
	

	<section>
		<h3 class="slide-title">Where do likelihoods come from?</h3>
	</section>

	<section>
				<br>
				<br>
						<div class="block">
							<div class="block-title">
								How to perform inference over forward simulation models?
							</div>
							<div class="block-content">
								<br>
								<ul>
									<li class="fragment"> <b class="alert">Implicit Inference</b>: Treat the simulator as a black-box with only the ability to sample from the joint distribution 
										$$(x, \theta) \sim p(x, \theta)$$
										a.k.a.<br><ul>
											<li> <b>Simulation-Based Inference</b> (SBI)
											</li>
											<li> <b>Likelihood-free inference</b> (LFI)
											</li>
											<li> <b>Approximate Bayesian Computation</b> (ABC)
											</li>
										</ul>
									</li>

									<br>

									<li class="fragment"> <b class="alert">Explicit Inference</b>: Treat the simulator as a probabilistic model and perform inference over the joint posterior 
										$$p(\theta, z | x) \propto p(x | z, \theta) p(z, \theta) p(\theta) $$
										a.k.a.<br><ul>
											<li> <b>Bayesian Hierarchical Modeling</b> (BHM)
											</li>
										</ul>
									</li>
									<br>
								</ul>

							</div>
						</div>
						<div class="fragment">$\Longrightarrow$ For a given simulation model, both methods <b class="alert">should converge to the same posterior!</b></div>
	</section>



	<section>
		<h1>Neural Density Estimation</h1>
		<hr>
		<h3>The key to manipulating <b>Implicit Distributions</b></h3>
	</section>


	<!-- 
	What we are going to cover here:
		- What is a neural network fundamentally?
		- How can we use neural networks to model distributions?
		- How can we train a neural network to learn conditional distributions?
		- Flexible neural network models for learning distributions
		   = <b>Normalizing Flows</b>
	-->


			<section>
			<section>
				<h3 class="slide-title">What is a neural network?</h3>
			
					<p>Simplest architecture: Multilayer Perceptron (MLP)</p>
					
					<div class="container">
					<div class="col">
					<img data-src="/talks/assets/fnn.png" style="height:300px;"/>
					<img data-src="https://studymachinelearning.com/wp-content/uploads/2019/10/summary_activation_fn.png"</img>
					</div>
					<div class="col">
				<ul>
					<li> Series of <b>Dense</b> a.k.a <b>Fully connected</b> layers:
						$$ h = \sigma(W x + b)$$ 
						where:
						<ul>
							<li>$\sigma$ is the activation function (e.g. ReLU, Sigmoid, etc.)</li>
							<li>$W$ is a multiplicative weight matrix</li>
							<li>$b$ is an additive bias parameter</li>
						</ul>
					</li>
					<br>
					<li class="fragment"> This defines a <b class="alert">parametric non-linear function $f_\theta(x)$</b></li>
					</li>
					<br>
					<li class="fragment"> MLPs are <b class="alert">universal function approximators</b> 
						<br> <b>Nota bene</b>: only asymptotically true! 
					</li>
				</ul>
			</div>
		</section>

			<section>
				<h3 class="slide-title">How do you use it to approximate functions?</h3>

				<br>
				<br>
				<br>
				<br>
				<div class="container">

					<div class="col">
						<img data-src="/talks/assets/gradient_descent.webp" style="height:300px;"/>
					</div>

					<div class="col">

						<ul>
							<li>Assume a <b class="alert">loss function</b> that should be small for good approximations on a training set of data points $(x_i, y_i)$

								$$ \mathcal{L} = \sum_{i} ( y_i - f_\theta(x_i))^2 $$
							</li>
						
							<br>

							<li>Optimize the parameters $\theta$ to minimize the loss function by <b class="alert">gradient descent</b>
								$$ \theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L} $$
							</li>
						</ul>
					</div>
				</div>
				<br>
				<br>
				<br>
			</section>

			<section data-background-iframe="https://playground.tensorflow.org"></section>
			</section>

			<section>
			<section>
				<h3 class="slide-title">The task at hand: Density Estimation</h3>
				<br>
				<ul>
					<li>The goal of density estimation is to <b>estimate an <b class="alert">implicit</b> distribution $\mathbb{P}$</b>
						from which a <b>training set $X = \{x_0, x_1, \ldots, x_n \}$</b> is drawn.
					</li>
					<br>
					<li class='fragment'> Usually, this means building a parametric model $\mathbb{P}_\theta$
						that tries to be close to $\mathbb{P}$.
					</li>
				</ul>

				<br>
				<div class="container">
					<div class="col fragment fade-up">
						<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756538/pasted-from-clipboard.png" class="plain"></img>
						<br>
						True $\mathbb{P}$
					</div>

					<div class="col  fragment fade-up">
						<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756539/pasted-from-clipboard.png" class="plain"></img>
						<br>
						Samples $x_i \sim \mathbb{P}$
					</div>

					<div class="col  fragment fade-up">
						<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756554/pasted-from-clipboard.png" class="plain"></img>
						<br>
						Model $\mathbb{P}_\theta$
					</div>
				</div>
				<br>
				<ul>
					<li class="fragment"> Once trained, you can typically <b>sample from $\mathbb{P}_\theta$</b> and/or <b class="alert">evaluate the likelihood $p_\theta(x)$</b>.
					</li>
				</ul>

			</section>


			<section>
				<div class="container">
					<div class="col fragment" data-fragment-index="1">
							<img class="plain" data-src="/talks/assets/kl_divergence.gif"/>
							The <b>Kullback-Leibler divergence</b>:
							\begin{align}
							D_{KL}(p||q_\phi) &= \mathbb{E}_{x \sim p(x)} \left[ \log \frac{p(x)}{q_\phi(x)} \right] \\ \\
							&= - \mathbb{E}_{x \sim p(x)} \left[ \log q_\phi(x) \right] + \underbrace{H(p)}_{\text{const}}
							\end{align}								
					</div>

					<div class="col">
						<ul>
							<li><b>Step I</b>: We need a <b>parametric density model</b> $q_\phi(x)$<br><br>
								Simplest possible model, a <b class="alert">parametric Gaussian</b>:
								$$ q_\phi(x) = \mathcal{N}(x ; \mu, \Sigma) $$
								with parameters $\phi = \{\mu, \Sigma\}$
							</li>
							<br>
							<br>
							<li class="fragment" data-fragment-index="1"><b>Step II</b>: We need a tool to <b>compare distributions</b><br>
								
							</li>
							<br>
							<br>
							<li class="fragment" data-fragment-index="2"><b>Step III</b>: We optimize the parameters $\phi$ to minimize the NLL over a training set $X = \{x_1, x_2, \ldots, x_N\} \sim \mathbb{P}$ :
								$$ \mathcal{L}(\phi) = - \frac{1}{N} \sum_{i=1}^N \log q_\phi(x_i) $$
								$\Longrightarrow$ This will minimize the KL divergence and fit the model to the data.
							</li>
							<br>
							
						</ul>
					</div>
				</div>
				</section>

				<section>
				</section>
			</section>

			  <section>
			<section>
				<h3 class="slide-title">Your goto NDE in low dimensions (<100): The Normalizing Flows</h3>

				<p> Still a latent variable model, except the mapping $f_\theta$ is made to be <b class="alert">bijective</b>.</p>
		<div class="container">
		<div class="col">
		  <img data-src="/talks/assets/flow_dinh_1.png" class="plain fragment fade-up" data-fragment-index="1"></img>
		  <img data-src="/talks/assets/flow_dinh_2.png" class="plain fragment fade-up" data-fragment-index="3"></img>

		  <br>
				 <div class="fragment fade-up" style="float:right; font-size: 20px" data-fragment-index="1">Dinh et al. 2016</div>
		</div>
		<div class="col">
				  <div class="block fragment fade-up" data-fragment-index="1">
				  <div class="block-title">
				   Normalizing Flows
				  </div>
				  <div class="block-content">
					<ul>
					  <li> Assumes a <b class="alert">bijective</b> mapping between
						data space $x$ and latent space $z$ with prior $p(z)$:
						$$ z = f_{\theta} ( x ) \qquad \mbox{and} \qquad x = f^{-1}_{\theta}(z)$$
					  </li>
					  <li class="fragment" data-fragment-index="2"> Admits an explicit marginal likelihood:
						$$ \log p_\theta(x) = \log p(z) + \log \left| \frac{\partial f_\theta}{\partial x}  \right|(x)    $$
					  </li>
					</ul>
				</div>
				</div>
				<br>
				  <br>
				  <div class="fragment">
				  $\Longrightarrow$ The challenge is in designing mappings $f_\theta$ that are both: 
				  <b>easy to invert, easy to compute the jacobian of</b>.</div>
				  <br>
				  <br>
		</div>
	</div>

		</section>

		<section>
			<h3 class="slide-title">One example of NF: RealNVP</h3>
			<br>
			<br>

			<div class="container">
				<div class="col">
					<img data-src="/talks/assets/realNVP_jacobian.png" style="height: 300px;"/>
					<br> Jacobian of an affine coupling layer
				</div>
				<div class="col">
					In a standard affine RealNVP, one Flow layer is defined as:

					$$ \begin{matrix} y_{1\ldots d} &=& x_{1 \ldots d} \\
					y_{d+1\ldots N} &=& x_{d+1 \ldots N} ‚äô \sigma_\theta(x_{1 \ldots d}) + \mu_\theta(x_{1 \ldots d})
					\end{matrix} $$
					where $\sigma_\theta$ and $\mu_\theta$ are unconstrained neural networks.
					<br>
					<br>
					We will call this layer an <b class="alert">affine coupling</b>.

				</div>


			</div>

			<br>
			<br>
			$\Longrightarrow$ This structure has the advantage that the Jacobian of this layer will be lower triangular which makes computing its determinant easy.
			

		</section>
	</section>

			<section>
				<h3 class="slide-title">Summarizing the Probabilistic Deep Learning Recipe</h3>
				<br>
				<br>
					<ul>
						<li class="fragment "> Express the <b class="alert">output of the model as a distribution</b>, not a point estimate
							$$ q_\phi(x | y) $$
						</li>
<br>
						<li class="fragment "> Assemble a training set that <b class="alert">encodes your prior</b> on the problem
							$$ \mathcal{D} = \left\{ (x_i, y_i) \sim p(x, y) = p(y|x) p(x) \right\} $$
						</li>
<br>
						<li class="fragment "> Optimize for the <b class="alert">Negative Log Likelihood</b>
							$$\mathcal{L} = - \log q_\phi(x|y) $$ 
						</li>
<br>
						<li class="fragment "> Profit!
							<ul>
								<li>Interpretable network outputs (i.e. I know mathematically what my network is trying to approximate)</li>
								<li>Uncertainty quantification (i.e. Bayesian Inference)</li>
							</ul>
						</li>

					</ul>
			<br>
			<br>
			<br>
			<br>
			</section>


			</section>


	<section>
		<h1>Implicit Inference</h1>
		<hr>
		<h3>The land of Neural Density Estimation</h3>
	</section>

	<section>
		<section>
		   <h3 class="slide-title">Black-box Simulators Define Implicit Distributions</h3>
		   <img class="plain" data-src="/talks/assets/lfi_sim.png" style="width:750px;"/>
		   <ul>
			   <li>A black-box simulator <b class="alert">defines $p(x | \theta)$ as an implicit distribution</b>, you can <b>sample from it</b> but you cannot evaluate it.
			   </li>

			   <li>This gives us a procedure to sample from the <b>Bayesian joint distribution</b> $p(x, \theta)$: 
					$$(x, \theta)  \sim p(x | \theta) \  p(\theta)$$
			   </li>

			   <li class='fragment'> <b class="alert">Key Idea</b>: Use a <b>parametric distribution model $\mathbb{P}_\varphi$ to approximate an implicit distribution</b>.

				<ul>
					<li><b>Neural Likelihood Estimation</b>:
						$\mathcal{L}  = - \mathbb{E}_{(x,\theta)}\left[ \log p_\varphi( x | \theta ) \right] $
					</li>
					<br>
					<li class="fragment"><b>Neural Posterior Estimation</b>:
						$\mathcal{L}  = - \mathbb{E}_{(x,\theta)}\left[ \log p_\varphi( \theta | x ) \right] $
					</li>
					<br>
					<li class="fragment"><b>Neural Ratio Estimation</b>:
						$\mathcal{L} = - \mathbb{E}_{\begin{matrix} (x,\theta)~p(x,\theta) \\
															 \ \theta^\prime \sim p(\theta) \end{matrix}} \left[  \log r_\varphi(x,\theta) + \log(1 - r_\varphi(x, \theta^\prime))   \right] $
					</li>
				</ul>
			   </li>
		   </ul>
	   </section>

	   <section>
		<h3 class="slide-title">A variety of algorithms</h3>
		<div style="float:right; font-size: 20px">Lueckmann, Boelts, Greenberg, Gon√ßalves, Macke (2021) <a href="https://arxiv.org/abs/2101.04653"><img src="https://img.shields.io/badge/stat.ML-arXiv%3A2101.04653-B31B1B.svg" class="plain"
					style="height:25px;vertical-align:middle;" /></a></div>
		<img class="plain" data-src="/talks/assets/sbibm_comparison.png"/>

		<br>
		<br>
			A few important points:
			<br><br>
			<ul>
				<li class="fragment"> <b>Amortized</b> inference methods, which estimate $p(\theta | x)$, can greatly speed up posterior estimation once trained.
				</li>
				<br>

				<li class="fragment"> <b>Sequential</b> SBI algorithms can actively sample simulations needed to refine the inference.
				</li>
			</ul>
			<br>
			<br>
			<div>Checkout this excellent package: <a href="https://www.mackelab.org/sbi/">https://www.mackelab.org/sbi</a> </div>
	</section>
</section>

	   <section>
			<h3 class="slide-title">A Practical Recipe for Careful Simulation-Based Inference</h3>
			<figure>
				<blockquote>
					Estimating conditional densities<br> in high dimensions is hard...
				</blockquote>
				<figcaption style="align-items: right;">
					<em>@EiffL - every 2 or 3 days</em>
				</figcaption>
			</figure>
			<br><br>
			<div class="fragment">
			To be more robust, you can decompose the problem into two tasks:<br><br>
			<ul>
				<li class="fragment"><b>Step I - <b class="alert">Dimensionality Reduction</b></b>: Compress your observables $x$ to a low dimensional <b>summary statistic $y$</b>
				
					<img class="plain" data-src="/talks/assets/lfi_sim_sum.png" />
				
				</li>

				<br>

				<li class="fragment"><b>Step II - <b class="alert">Conditional Density Estimation</b></b>: Estimate the posterior $p(\theta | y)$ using SBI from the low dimensional summary statistic $y$.
				</li>
			</ul> 
			</div>
		</section>

		<section>
			<h3 class="slide-title">The Case for Dimensionality Reduction</h3> 
			<ul>
				<li>In the case of <b>Neural Posterior Estimation</b>

					<div class="container">
						<div class="col">
							<img class="plain" data-src="/talks/assets/noisy_map.png"  style="width: 200px;"/>
						</div>
						<div class="col">
							<img class="plain" data-src="/talks/assets/generic_network_inv.png" style="height: 200px;"/>
						</div>

						<div class="col">
							$p(\theta | x)$
						</div>
					</div>
<div class="fragment">$\Longrightarrow$ <b class="alert">Dimensionality reduction already happens implicitly</b> in the network.</div>
				</li>

				<li class="fragment">In the case of <b>Neural Likelihood Estimation</b>
					<div class="container">
						<div class="col">
							<img class="plain" data-src="/talks/assets/plot_massive_nu.png" />
							$x \sim p(x|\theta)$
						</div>
						<div class="col">
							<img class="plain" data-src="/talks/assets/generic_network.png" style="height: 200px;" />
						</div>

						<div class="col">
							$\theta$
						</div>
					</div>
					<div class="fragment">$\Longrightarrow$ This is equivalent to <b class="alert">learning a perfect emulator</b> for the high-dimensional outputs of your numerical simulator.</div> 
				</li>

			</ul>
		</section>


		<section class="inverted" data-background="#000">
			<h2>How can we lower the dimensionality of the problem <b>without</b> degrading our constraining power?</h2>
		</section>

		<section>
		<section>
			<h3 class="slide-title">Automated Neural Summarization</h3>
		  <img class="plain" data-src="/talks/assets/lfi_sim_sum.png" />
				<ul>
					<li> Introduce a parametric function $f_\varphi$ to <b class="alert">reduce the dimensionality of the
						data while preserving information</b>.
					</li>
				</ul>
				<div class="container">
					<div class="col">
						<div class="r-stack">
							<img class="plain fragment"  data-fragment-index="0"  data-src="/talks/assets/mutual_information.png" />
							<!-- <img class="plain fragment" data-fragment-index="1"  data-src="/talks/assets/imnn.png" /> -->
						</div>
						<!-- <div class="fragment" style="float:right; font-size: 15px"  data-fragment-index="1"> Makinen, Charnock, Alsing, Wandelt (2021) <a href="https://arxiv.org/abs/2107.07405"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2107.07405-B31B1B.svg" class="plain"
								style="height:20px;vertical-align:middle;" /></a></div> -->
	
					</div>
					<div class="col">
									<div class="block fragment" data-fragment-index="0">
										<div class="block-title">
											Information point of view
										</div>
										<div class="block-content">
											<ul>
												<li class="fragment"> Summary statistics <b>$y$ is sufficient for $\theta$</b> if
													$$ I(Y; \Theta) = I(X; \Theta) \Leftrightarrow p(\theta | x ) = p(\theta | y) $$
												</li>
												<li class="fragment" > Variational <b>Mutual Information Maximization</b>
												  $$ \mathcal{L} \ = \ \mathbb{E}_{x, \theta} [ \log q_\phi(\theta | y=f_\varphi(x)) ] \leq  I(Y; \Theta) $$
												  (Barber & Agakov variational lower bound)
														<div style="float:right; font-size: 15px"> Jeffrey, Alsing, <b>Lanusse</b> (2021) <a href="https://arxiv.org/abs/2009.08459"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2009.08459-B31B1B.svg" class="plain"
																	style="height:20px;vertical-align:middle;" /></a></div>
												</li>
	
												<!-- <li class="fragment" data-fragment-index="1" > Information Maximization Neural Network
													$$\mathcal{L} \ = \ - | \det \mathbf{F} |  \ \mbox{with} \ \mathbf{F}_{\alpha, \beta} = tr[ \mu_{\alpha}^t C^{-1} \mu_{\beta} ] $$
													<div style="float:right; font-size: 15px"> Charnock, Lavaux, Wandelt (2018) <a href="https://arxiv.org/abs/1802.03537"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1802.03537-B31B1B.svg" class="plain"
															style="height:20px;vertical-align:middle;" /></a></div>
												</li> -->
											</ul>
										</div>
									</div>
					</div>
				</div>
		</section>
		<section>
			<h3 class="slide-title">Another Approach: maximizing the Fisher information</h3>
				
			<img class="plain " data-fragment-index="1"  data-src="/talks/assets/imnn.png" />

			Information Maximization Neural Network (IMNN)
													$$\mathcal{L} \ = \ - | \det \mathbf{F} |  \ \mbox{with} \ \mathbf{F}_{\alpha, \beta} = tr[ \mu_{\alpha}^t C^{-1} \mu_{\beta} ] $$
													<div style="float:right; font-size: 15px"> Charnock, Lavaux, Wandelt (2018) <a href="https://arxiv.org/abs/1802.03537"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1802.03537-B31B1B.svg" class="plain"
															style="height:20px;vertical-align:middle;" /></a></div>


		</section>
		</section>


	   <section>
		<h2>Full-Field Implicit Inference <br> by Neural Summarisation and Density Estimation</h2>
		<!-- <a href="https://arxiv.org/abs/2009.08459"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2009.08459-B31B1B.svg" class="plain" style="height:25px;" /></a>
		<a href="https://github.com/NiallJeffrey/Likelihood-free_DES_SV"><img src="https://badgen.net/badge/icon/Likelihood-free_DES_SV?icon=github&label" class="plain" style="height:25px;" /></a> -->
		<a href="https://github.com/DifferentiableUniverseInitiative/sbi_lens"><img src="https://badgen.net/badge/icon/sbi_lens?icon=github&label" class="plain" style="height:25px;" /></a>
		<hr>
		<div class="container">
			<div class="col">
				<div align="left" style="margin-left: 20px;">
					<h3>Work led by Denise Lanzieri and Justine Zeghal
					</h3>
					<!-- <img data-src="/talks/assets/niall.jpg" style='width:200px; height:200px;object-fit: cover;'></img>
					<img data-src="/talks/assets/justin.jpeg" style='width:200px; height:200px;'></img> -->
					<img data-src="https://avatars.githubusercontent.com/u/72620117?v=4" style='width:200px; height:200px;'></img>
					<img data-src="/talks/assets/justine.jpeg" style='width:200px; height:200px;'></img>

					<br>
					<br>
					$\Longrightarrow$ Demonstrating that neural summary statistics exhaust the cosmological information content.
				</div>
			</div>
			<div class="col">
				<img class="plain" data-src="/talks/assets/compare_contour_plot_multi_tomo_bins.png" style="width:350px;" />
			</div>
		</div>
		<br>
	</section>

	<section>
		<section>
			<h3 class="slide-title">An easy-to-use validation testbed: log-normal lensing simulations</h3>
	
			<div class="container">
				<div class="col"> 
					<!-- <div class="container"> -->
						<!-- <div class="col">
							<img data-src="https://avatars.githubusercontent.com/u/72620117?v=4" style='width:150px; height:150px;'></img>
							<img data-src="/talks/assets/justine.jpeg" style='width:150px; height:150px;'></img>
							<br>
							<small>Denise Lanzieri (left) and Justine Zeghal (right) </small>
						</div>
	
						<div class="col"> -->
							<img data-src="/talks/assets/github.png" class="plain" style="height:70px" />
							<br>
							<a href="https://github.com/DifferentiableUniverseInitiative/sbi_lens">DifferentiableUniverseInitiative/sbi_lens</a><br>
							JAX-based log-normal lensing simulation package 
						<!-- </div> -->
	
					 <!-- </div> -->
					<img data-src="/talks/assets/mass_map_tomo.png" />
					<ul>
						<li>10x10 deg$^2$ maps at LSST Y10 quality, conditioning the log-normal shift
							parameter on $(\Omega_m, \sigma_8, w_0)$
						</li>
						<br>
						<li class="fragment" data-fragment-index="0"> Infer full-field posterior on cosmology:
							<ul>
								<li> <b>explicitly</b> using an Hamiltonian-Monte Carlo (NUTS) sampler 
									</li>
								<li class="fragment" data-fragment-index="1"> <b class="alert">implicitly</b> using a <b>learned summary statistics</b> and conditional density estimation.
							</li>
						</li>
					</ul>
					
				</div>
				<div class="col r-stack">
					<img class="fragment current-visible" data-fragment-index="0" data-src="/talks/assets/compare_ff_ps_contour_plot_multi_tomo_bins.png"/>
					<img class="fragment" data-fragment-index="1" data-src="/talks/assets/compare_contour_plot_multi_tomo_bins.png"/>
				</div>
			</div>
		</section>
	
	
		<section>
			<img data-src="/talks/assets/find-the-difference.png" style="height: 700px;" />
		</section>
		</section>
				
		<section>
			<h3 class="slide-title">Note: not all compression techniques are equivalent!</h3>
			<div class="container">
				<div class="col">
					<img data-src="/talks/assets/contours_mse_vmim.png" style="height: 700px;" />
				</div>
				<div class="col">
					<ul>
						<li> Comparison to posteriors obtained with same neural compression architecture, but <b>different loss function</b>:
							$$\mathcal{L}_{MSE} = \mathbb{E}_{(x,\theta)} || f_\varphi(x) - \theta ||_2^2 $$	
						</li>
					</ul>
					<br>
					<br>
					<img data-src="/talks/assets/fom_denise.png"/>
				</div>
		</section>

		<section>
			<h2>Likelihood-free inference with neural compression of DES SV weak lensing map</h2>
			<a href="https://arxiv.org/abs/2009.08459"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2009.08459-B31B1B.svg" class="plain" style="height:25px;" /></a>
			<a href="https://github.com/NiallJeffrey/Likelihood-free_DES_SV"><img src="https://badgen.net/badge/icon/github?icon=github&label" class="plain" style="height:25px;" /></a>
			<hr>
			<div class="container">
				<div class="col">
					<div align="left" style="margin-left: 20px;">
						<h3>Work in collaboration with Niall Jeffrey, Justin Alsing
						</h3>
						<img data-src="/talks/assets/niall.jpg" style='width:200px; height:200px;object-fit: cover;'></img>
						<img data-src="/talks/assets/justin.jpeg" style='width:200px; height:200px;'></img>

						<br>
						<br>
						$\Longrightarrow$ Deploy end-to-end SBI to the Science Verification data of the Dark Energy Survey.
					</div>
				</div>
				<div class="col">
					<img class="plain" data-src="/talks/assets/ks_sv.png" style="width:350px;" />
				</div>
			</div>
			<br>
		</section>



			  <section>
				  <section>
					  <h3 class="slide-title">End-to-end framework for likelihood-free parameter inference with DES SV</h3>
					  <div class="container">
						  <div class="col">
							  <div style="float:right; font-size: 20px"> Jeffrey, Alsing, <b>Lanusse</b> (2021) <a href="https://arxiv.org/abs/2009.08459"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2009.08459-B31B1B.svg" class="plain"
										  style="height:25px;vertical-align:middle;" /></a></div>
						  </div>
					  </div>

					  <div class="container">
						  <div class="col">
							  <img class="plain" data-src="/talks/assets/ks_sv.png" style="height:550px;"></img>
						  </div>

						  <div class="col fragment">
							  <img class="plain" data-src="/talks/assets/orthant.png" style="height:300px;" />
							  <img class="plain" data-src="/talks/assets/sim_params.png" style="height:300px;" /><br>
							  Suite of N-body + raytracing simulations: $\mathcal{D}$
						  </div>
					  </div>
				  </section>
					</section>

					<section>


				  <section>
					  <h3 class='slide-title'> deep residual networks for lensing maps compression</h3>

					  <div class="container">

						  <div class="col" style="flex: 0 0 15em;">
							  <img class="plain" data-src="/talks/assets/jeffrey_model.png" style="height:550px" /><br>
						  </div>
						  <div class="col">
							  <ul>
								  <li> Deep Residual Network $y = f_\phi(x)$ followed by neural density estimator $q_\phi(\theta | y)$
								  </li>
								  <br>
								  <li class="fragment">Training on weak lensing maps simulated for different cosmologies</li>
								  <div class="container fragment">
									  <div class="col" style="flex: 0 0 26em;">
										  <img class="plain" data-src="/talks/assets/mass_maps.png" /><br>
									  </div>
									  <div class="col">
										  <img class="plain" data-src="/talks/assets/TF_FullColor_Horizontal.png" />
										  <br>
										  <br>
										  <br>
										  <img class="plain" data-src="/talks/assets/google-cloud-platform-logo.png" />
									  </div>
								  </div>
								  <li class="fragment">Training by Variational Mutual Information Maximization:
									  $$\mathbb{E}_{(x, \theta) \in \mathcal{D}} [ \log q_\phi(\theta | y = f_\phi(x) ) ]$$
								  </li>
							  </ul>
						  </div>
					  </div>
				  </section>
					</section>

					<section>

				  <section>
					  <h3 class="slide-title">Estimating the likelihood by Neural Density Estimation</h3>
					  <br>
					  $\Longrightarrow$ We cannot assume a Gaussian likelihood for the summary $y = f_\phi(\kappa)$ but we can learn $p(y | \theta)$: Neural Likelihood Estimation.
					  <br>
					  <br>
					  <div class="container">
						  <div class="col">
							  <img data-src="/talks/assets/flow_dinh_1.png" class="plain fragment fade-up" data-fragment-index="1"></img>
							  <img data-src="/talks/assets/flow_dinh_2.png" class="plain fragment fade-up" data-fragment-index="1"></img>
							  <br>
							  <div class="fragment fade-up" style="float:right; font-size: 20px" data-fragment-index="1">Dinh et al. 2016</div>
						  </div>
						  <div class="col">
							  <div class="block fragment fade-up" data-fragment-index="1">
								  <div class="block-title">
									  Neural Likelihood Estimation by Normalizing Flow
								  </div>
								  <div class="block-content">
									  <ul>
										  <li> We use a conditional Normalizing Flow to build an explicit model for the likelihood function
											  $$ \log p_\varphi (y | \theta)$$
										  </li>
										  <br>
										  <li class="fragment"> In practice we use the pyDELFI package and an <b>ensemble of NDEs</b> for robustness.
										  </li>
										  <br>
										  <li class="fragment"> Once learned, we can use the likelihood as part of a conventional MCMC chain</li>
									  </ul>
								  </div>
							  </div>
							  <br>
							  <br>
						  </div>
					  </div>
				  </section>

						</section>

				  <section>
					  <h3 class="slide-title">Parameter constraints from DES SV data</h3>

					  <div class="container">
						  <div class="col">
							  <img class="plain" data-src="/talks/assets/results_jeffrey.png" />
						  </div>

						  <div class="col fragment">
							  <img class="plain" data-src="/talks/assets/jeffrey_s8.png" />
						  </div>
					  </div>
				  </section>

		<section class="inverted" data-background="#000">
			<h2>Can we just retire all conventional likelihood-based analyses?</h2>
		</section>

		<section>
		<section>
			<h3 class="slide-title"> $w$CDM analysis of KiDS-1000 Weak Lensing (Fluri et al. 2022)</h3>
			<div class="container">
				<div class="col">
					<div style="float:right; font-size: 20px"> Kacprzak, Fluri, Schneider, Refregier, Stadel (2022)
						<a href="https://arxiv.org/abs/2209.04662"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2209.04662-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
					</div>
				</div>
			</div>
			<div class="container">
				<div class="col">
					<img class="plain" data-src="/talks/assets/cosmogrid.png" style="height:550px" /><br>
				</div>

				<div class="col">
					<img class="plain" data-src="/talks/assets/cosmogrid_field.png" /><br>

					<b>CosmoGridV1 simulations</b>, available at <a href="http://www.cosmogrid.ai">http://www.cosmogrid.ai</a>

					<br>
					<br>
					<ul>
						<li>17,500 simulations at 2,500 cosmologies </li>
						<li>Lensing, Intrinsic Alignment, and Galaxy Density <br>maps at nside=512</li>
					</ul>
				</div>
			</div>
		</section>
		
		<section>
			<div class="container">
				<div class="col">
					<div style="float:right; font-size: 20px"> Fluri, Kacprzak, Lucchi, Schneider, Refregier, Hofmann (2022)
						<a href="https://arxiv.org/abs/2201.07771"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2201.07771-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
					</div>
				</div>
			</div>
			<br>
			<br>
		<div class="container">
			<div class="col">
				KiDS-1000 footprint and simulated data
				<img class="plain" data-src="/talks/assets/fluri_sky.png"  /><br>

				<ul>
					<li><b>Neural Compressor</b>: Graph Convolutional Neural Network on the Sphere <br> Trained by Fisher information maximization.
					</li>
				</ul>
			</div>
		
			<div class="col fragment">
				<img class="plain" data-src="/talks/assets/fluri_contours.png" /><br>
			</div>
		</div>
		</section>

		</section>

		<section>
		<section data-background-video="/talks/assets/simbig.mp4" 
		data-background-video-loop data-background-video-muted data-background-size="contain">
		<h3 class="slide-title" style="background-color: rgba(0, 0, 0, 0.7);">SIMBIG: Field-level SBI of Large Scale Structure (Lemos et al. 2023)</h3>
		<br>
		<br>	<br>
		<br>	<br>
		<br>	<br>
		<br>	<br>
		<br>	<br>
		<br>	<br>
		<br>	<br>
		<br>	<br>
		<br>
		BOSS CMASS galaxy sample: Data vs Simulations<br>
		<ul>
			<li>20,000 simulated galaxy samples at 2,000 cosmologies </li>
		</ul>
		<div class="container">
			<div class="col">
				<div style="float:right; font-size: 20px"> Hahn et al. (2022)
					<a href="https://arxiv.org/abs/2211.00723"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2211.00723-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
				</div>
			</div>
		</div>
		</section>

		<section>
			<div class="container">

				<div class="col">
					
					<div style="float:right; font-size: 25px"> <a href="https://ml4astro.github.io/icml2023/assets/48.pdf">Lemos et al. (2023)</a>
					</div>
				</div>
			</div>
			<img class="plain" data-src="/talks/assets/simbig_method.png" style="width:1000px;"/>


			<img class="plain fragment" data-src="/talks/assets/simbig_contours.png" style="width:800px;"/>
		</section>
		</section>
		
		<section data-background-image="https://media1.giphy.com/media/MCZ39lz83o5lC/giphy.gif?cid=ecf05e47itrwtb66ts0bcauq0659d605b2qhhn8r9q58eo75&ep=v1_gifs_related&rid=giphy.gif&ct=g">
		</section>

		<section>
			<h3 class="slide-title">Example of unforeseen impact of shortcuts in simulations</h3>
			<div class="container">
				<div class="col">
					<div style="float:right; font-size: 20px"> Gatti, Jeffrey, Whiteway et al. (2023)
						<a href="https://arxiv.org/abs/2307.13860"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2307.13860-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
					</div>
				</div>
			</div>

			<div class="container">
				<div class="col">
					<img class="plain" data-src="/talks/assets/lensing_3d.png" style="height: 450px;"/>

					<br>Is it ok to <b>distribute lensing source galaxies randomly</b> in simulations, or should they be clustered?
				</div>

				<div class="col fragment ">
					<img class="plain" data-src="/talks/assets/gatti_clustering.png" style="height: 500px;"/>
				</div>

			</div>
			<br>
			<div class="fragment">$\Longrightarrow$ An <b class="alert">SBI analysis could be biased</b> by this effect and <b>you would never know it!</b> 
			</div>
		</section>

		<section>
			<h3 class="slide-title">How much <b>usable</b> information is there beyond the power spectrum?</h3>

			<div class="container">
				<div class="col">
					<div style="float:left; font-size: 20px"> Chisari et al. (2018)
						<a href="https://arxiv.org/abs/1801.08559"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A1801.08559-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
					</div>
					<img class="plain" data-src="/talks/assets/chisari_feedback.png" style="height: 375px;"/>
					<br> Ratio of power spectrum in hydrodynamical simulations vs. N-body simulations
				</div>
				<div class="col fragment">
					<div style="float:right; font-size: 20px"> Secco et al. (2021)
						<a href="https://arxiv.org/abs/2105.13544"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2105.13544-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
					</div>
					<img class="plain" data-src="/talks/assets/des_y3_cosmic_shear.png" style="height: 400px;"/>
					<br>DES Y3 Cosmic Shear data vector
				</div>
			</div>
			<br>
			<div class="fragment">$\Longrightarrow$ Can we find non-Gaussian information that is not affected by baryons?</div>
		</section>

        <section>
			<h3 class="slide-title">takeways</h3>
  
			<ul>
			  <li>
				SBI <b>automatizes inference</b> over
				numerical simulators.
				<ul>
				  <li>
					Turns both summary extraction and inference problems into an
					<b class="alert">optimization problems</b>
				  </li>
				  <li><b>Deep learning allows us to solve that problem!</b></li>
				</ul>
			  </li>
  
			  <br />
			  <li class="fragment">
				In the context of upcoming surveys, this techniques provides
				<b class="alert">many advantages</b>:
				<ul>
				  <li>
					<b>Amortized inference</b>: near instantaneous parameter
					inference, extremely useful for time-domain.
				  </li>
				  <li>
					<b>Optimal information extraction</b>: no longer need for
					restrictive modeling assumptions needed to obtain tractable
					likelihoods.
				  </li>
				</ul>
			  </li>
			  <br />
			</ul>
  
			<br />
			<br />
			<div class="block fragment">
			  <div class="block-title">
				Will we be able to exploit all of the information content of
				LSST, Euclid, DESI?
			  </div>
			  <div class="block-content">
				$\Longrightarrow$ Not rightaway, but <b>it is not the fault of Deep
				Learning!</b>
				<br />
				<br />
				<ul>
				  <li class="fragment">
					Deep Learning has redefined the limits of our statistical
					tools, creating
					<b>additional demand on the accuracy of simulations</b> far
					beyond the power spectrum.
				  </li>
				  <br />
				  <li class="fragment">
					Neural compression methods have the downside of being opaque.
					It is <b>much harder to detect unknown systematics</b>.
				  </li>
				  <br />
				  <li class="fragment">
					We will need a significant number of
					<b>large volume, high resolution</b> simulations.
				  </li>
				</ul>
			  </div>
			</div>
			<br />
  
			<br />
  
			<br />
			<br />
		  </section>

	<!-- Hierarchical Bayesian Inference
			- This time we treat the entire simulator as one big bayesian model
		   - To enable inference over this large number of dimensions, you need gradients
			   with HMC or VI
		   - Autodiff gives you easy access to these gradients
		   -
	 -->

	<section>
		<h1> Explicit Inference </h1>
		<hr>
		<h2>Where the JAX things are</h2>
	</section>

	<section>
		<h3 class="slide-title"> Simulators as Hierarchical Bayesian Models</h3>

		<img class="plain" data-src="/talks/assets/lfi_sim.png" style="width:1000px;"/>
		<ul>
			<li>If we have access to all latent variables $z$ of the simulator,
				then the <b class="alert">joint log likelihood $p(x | z, \theta)$ is explicit</b>.
			</li>

			<br>

			<li class="fragment"> We need to infer the joint posterior $p(\theta, z | x)$ before marginalization to
				yield $p(\theta | x) = \int p(\theta, z | x) dz$.<br>
				$\Longrightarrow$ Extremely difficult problem as <b>$z$ is typically very high-dimensional</b>.
			</li>

			<br>

			<li class="fragment"> Necessitates inference strategies with <b class="alert">access to gradients of the likelihood</b>.
				$$\frac{d \log p(x | z, \theta)}{d \theta} \quad ; \quad \frac{d \log p(x | z, \theta)}{d z}  $$
				For instance: Maximum A Posterior estimation, Hamiltonian Monte-Carlo, Variational Inference.
			</li>
			<br>
		</ul>

		<div class="fragment">$\Longrightarrow$ The only hope for explicit cosmological inference is to have <b class="alert">fully-differentiable cosmological simulations</b>!</div>
	</section>

	   <section>
		   <h3 class="slide-title">the hammer behind the Deep Learning revolution: Automatic Differentation</h3>

		   <ul>
			   <li class="fragment"> <b>Automatic differentiation</b> allows you to compute analytic derivatives of arbitraty expressions:<br>
				   If I form the expression $y = a * x + b$, it is separated in fundamental ops:
				   $$ y = u + b \qquad u = a * x $$
				   then gradients can be obtained by the chain rule:
				   $$\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u} \frac{ \partial u}{\partial x} = 1 \times a = a$$
			   </li>
			   <br>
			   <li class="fragment"> This is a fundamental tool in Machine Learning, and autodiff frameworks include TensorFlow and PyTorch.
			   </li>
		   </ul>
		   <br>
		   <br>
		   <div class="block fragment">
			   <div class="block-title">
				   Enters JAX: NumPy + Autograd + GPU
			   </div>
			   <div class="block-content">

				   <div class="container">
					   <div class="col">
						   <ul>
							   <li>JAX follows the NumPy api!
								   <pre class="python"><code data-trim data-noescape>
						   import jax.numpy as np
					   </code></pre>
							   </li>
							   <li>Arbitrary order derivatives</li>
							   <li>Accelerated execution on GPU and TPU</li>

						   </ul>
					   </div>
					   <div class="col" align="center">

						   <img data-src="https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png" class="plain" />
					   </div>
				   </div>
	   </section>


	   <section>
		<section>
			<h3 class="slide-title"> jax-cosmo: Finally a differentiable cosmology library, and it's in JAX!</h3>
				  <div class="container">
					  <div class="col">
						  <div style="float:right; font-size: 20px"> Campagne, <b>Lanusse</b>, Zuntz et al. (2023)
							  <a href="https://arxiv.org/abs/2302.05163"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2302.05163-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
						  </div>
					  </div>
				  </div>
			<div class="container">
				<div class="col">
					<img data-src="/talks/assets/github.png" class="plain" style="height:70px" />
					<div> <a href="https://github.com/DifferentiableUniverseInitiative/jax_cosmo/">https://github.com/DifferentiableUniverseInitiative/jax_cosmo</a>
					</div>

					<pre class="python"><code data-trim data-noescape>
						import jax.numpy as np
						import jax_cosmo as jc

						# Defining a Cosmology
						cosmo = jc.Planck15()

						# Define a redshift distribution with smail_nz(a, b, z0)
						nz = jc.redshift.smail_nz(1., 2., 1.)

						# Build a lensing tracer with a single redshift bin
						probe = probes.WeakLensing([nz])

						# Compute angular Cls for some ell
						ell = np.logspace(0.1,3)
						cls = angular_cl(cosmo_jax, ell, [probe])
					</code></pre>

					<div class="block">
						<div class="block-title">
							Current main features
						</div>
						<div class="block-content">
							<ul>
								<li>Weak Lensing and Number counts probes</li>
								<li>Eisenstein & Hu (1998) power spectrum + halofit</li>
								<li>Angular $C_\ell$ under Limber approximation </li>
							</ul>
							<div>$\Longrightarrow$ 3x2pt DES Y1 capable </div>
						</div>
					</div>

				</div>

				<div class="col">
					<img class="plain" data-src="/talks/assets/jc_vs_ccl_lensing.png" />
					<img class="plain" data-src="/talks/assets/jc_vs_ccl_clustering.png" />
					<br>
					Validated against the <a href="https://github.com/LSSTDESC/CCL">DESC Core Cosmology Library</a>
				</div>
			</div>
		</section>

		<section>
			<h3 class="slide-title"> let's compute a Fisher matrix</h3>

			<br>

			$$F = - \mathbb{E}_{p(x | \theta)}[ H_\theta(\log p(x| \theta)) ] $$

			<br>

			<div class="container">
				<div class="col fragment">

					<pre class="python"><code data-trim data-noescape>
	import jax
	import jax.numpy as np
	import jax_cosmo as jc

	# .... define probes, and load a data vector

	def gaussian_likelihood( theta ):
	  # Build the cosmology for given parameters
	  cosmo = jc.Planck15(Omega_c=theta[0], sigma8=theta[1])

	  # Compute mean and covariance
	  mu, cov = jc.angular_cl.gaussian_cl_covariance_and_mean(cosmo,
														ell, probes)
	  # returns likelihood of data under model
	  return jc.likelihood.gaussian_likelihood(data, mu, cov)

	# Fisher matrix in just one line:
	F = - jax.hessian(gaussian_likelihood)(theta)
	</code></pre>
					<a href="https://colab.research.google.com/github/DifferentiableUniverseInitiative/jax_cosmo/blob/master/docs/notebooks/jax-cosmo-intro.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg"
							alt="Open In Colab" class="plain" style="height:25px;" /></a>
				</div>

				<div class="col fragment">
					<img data-src="/talks/assets/Fisher_mat.png" class="plain"><br><br>
				</div>
			</div>

			<ul>
				<li class="fragment"> <b class="alert">No derivatives were harmed by finite differences in the computation of this Fisher!</b> </li>
				<li class="fragment"> Only a small additional compute time compared to one forward evaluation of the model</li>
			</ul>

		</section>


										<section>
											<h3 class="slide-title"> Inference becomes fast and scalable</h3>

											<div class="container">
												<div class="col">

													<ul>
														<li>Current cosmological MCMC chains take <b>days</b>, and typically require access
															to large computer clusters.</li>
														<br>
														<li class="fragment" data-fragment-index="1"><b class="alert">Gradients of the log posterior are required for modern efficient and scalable inference</b> techniques:
															<ul>
																<li>Variational Inference</li>
																<li>Hamiltonian Monte-Carlo</li>
															</ul>
														</li>
														<br>
														<li class="fragment" data-fragment-index="2">In jax-cosmo, we can trivially obtain <b>exact</b> gradients:
															<pre class="python"><code data-trim data-noescape>
										def log_posterior( theta ):
											return gaussian_likelihood( theta ) + log_prior(theta)

										score = jax.grad(log_posterior)(theta)
										</code></pre>
														</li>

														<br>
														<li class="fragment" data-fragment-index="3">On a DES Y1 analysis, we find convergence in 70,000 samples with vanilla HMC, 140,000 with Metropolis-Hastings</li>
													</ul>

												</div>

												<div class="col">
													<div class="fragment" data-fragment-index="3">
														<img data-src="/talks/assets/jc_3x2pt_hmc.png" class="plain" /><br>
														DES Y1 posterior, jax-cosmo HMC vs Cobaya MH <br>(credit: Joe Zuntz)
													</div>
												</div>
											</div>
										</section>
	</section> 


	<section>
		<h3 class='slide-title'>Forward Models in Cosmology</h3>
		<div class="container">
			<div class='col'>
				<img data-src="/talks/assets/fieldinit.png" class="plain" style="height:300px;" />
				<b class="alert"> Linear Field </b>
			</div>
			<div class='col fragment' data-fragment-index='2'>
				<img data-src="/talks/assets/fieldfin.png" class="plain " style="height:300px;" />
				<b class="alert"> Final Dark Matter </b>
			</div>
			<hr style="width: 1px; height: 400px; background: white; border: none;" />
			<div class='col fragment' data-fragment-index='3'>
				<img data-src="/talks/assets/fieldhalo.png" class="plain " style="height:300px;" />
				<b class="alert"> Dark Matter Halos </b>
			</div>
			<div class='col fragment' data-fragment-index='4'>
				<img data-src="/talks/assets/fieldgal.png" class="plain " style="height:300px;" />
				<b class="alert"> Galaxies </b>
			</div>
		</div>
		<div class="container" style="position:relative; width:1000px; height:50px; margin:0 auto;">
			<div class='col fragment' data-fragment-index='2'>
				<font size="10"> $\longrightarrow$ </font> <br>
				<div class="fragment grow" data-fragment-index='5'>N-body simulations </div>
			</div>
			<div class='col fragment' data-fragment-index='3'>
				<font size="10"> $\longrightarrow$ </font> <br> Group Finding <br> algorithms
			</div>
			<div class='col fragment' data-fragment-index='4'>
				<font size="10"> $\longrightarrow$ </font> <br> Semi-analytic &amp <br> distribution models
			</div>
			<!-- 		<div class='fragment' data-fragment-index='2'> N-body simulations <div> -->
			<!-- <div class='fragment' data-fragment-index='3'> Group Finding algorithms <div> -->
			<!-- <div class='fragment' data-fragment-index='4'> Semi-analytic models <div> -->
		</div>
	</section>

	<section>
		<h3 class='slide-title'>You can try to learn the simulation...</h3>
		<div style="float:right; font-size: 25px">Learning particle displacement with a UNet. S. He, et al. (2019)</div><br><br>

		<img data-src="/talks/assets/Model-Comparison.jpg" style="height:400px;" />

		<div class="block fragment">
			<div class="block-title">
				The issue with using deep learning as a <i>black-box</i>
			</div>
			<div class="block-content">
				<ul>
					<li> No guarantees to work outside of training regime.
					</li>
					<li> No guarantees to capture dependence on cosmology accurately.
					</li>
				</ul>
			</div>
		</div>
	</section>

			   <section>
				   <h3 class='slide-title'>the Fast Particle-Mesh scheme for N-body simulations</h3>
				   <b>The idea</b>: approximate gravitational forces by estimating densities on a grid.

				   <div class='container'>
					   <div class='col'>
						   <ul>
							   <li>The numerical scheme:
								   <br>
								   <br>
								   <ul>
									   <li class="fragment" data-fragment-index="1"> Estimate the density of particles on a mesh<br>
										   => compute gravitational forces by FFT
									   </li>

									   <br>

									   <li class="fragment" data-fragment-index="2"> Interpolate forces at particle positions
									   </li>

									   <br>

									   <li class="fragment" data-fragment-index="3"> Update particle velocity and positions, and iterate
									   </li>
								   </ul>
							   </li>
							   <br>

							   <li class='fragment'> Fast and simple, at the <b class="alert">cost of approximating short range interactions</b>.
							   </li>

						   </ul>
					   </div>

					   <div class='col'>

						   <div style="position:relative; width:550px; height:550px; margin:0 auto;">
							   <img class="fragment current-visible plain" data-src="/talks/assets/particle_positions_0.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
							   <img class="fragment current-visible plain" data-src="/talks/assets/particle_density_0.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
							   <img class="fragment current-visible plain" data-src="/talks/assets/particle_positions_0.png" style="position:absolute;top:0;left:0;" data-fragment-index="2" />
							   <img class="fragment  plain" data-src="/talks/assets/particle_positions_1.png" style="position:absolute;top:0;left:0;" data-fragment-index="3" />

						   </div>

					   </div>
				   </div>

				   <div class="fragment"> $\Longrightarrow$ Only a series of FFTs and interpolations.</div>
			   </section>


												   <section>
										 <section>
												 <h3 class='slide-title'>introducing FlowPM: Particle-Mesh Simulations in TensorFlow</h3>
												 <div class="container">
													 <div class="col">
														 <div style="float:right; font-size: 20px"> Modi, <b>Lanusse</b>, Seljak (2020)
															 <a href="https://arxiv.org/abs/2010.11847"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2010.11847-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
													 </div>
												 </div>
												 <div class='container'>
													 <div class='col'>
														 <img data-src="/talks/assets/github.png" class="plain" style="height:70px" />
														 <img data-src="/talks/assets/TF_FullColor_Horizontal.png" class='plain' style="height: 70px;" />

														 <div> <a href="https://github.com/DifferentiableUniverseInitiative/flowpm">https://github.com/DifferentiableUniverseInitiative/flowpm</a>
														 </div>
														 <pre class="python"><code data-trim data-noescape>
																		 import tensorflow as tf
																		 import flowpm
																		 # Defines integration steps
																		 stages = np.linspace(0.1, 1.0, 10, endpoint=True)

																		 initial_conds = flowpm.linear_field(32,       # size of the cube
																											100,       # Physical size
																											ipklin,    # Initial powerspectrum
																											batch_size=16)

																		 # Sample particles and displace them by LPT
																		 state = flowpm.lpt_init(initial_conds, a0=0.1)

																		 # Evolve particles down to z=0
																		 final_state = flowpm.nbody(state, stages, 32)

																		 # Retrieve final density field
																		 final_field = flowpm.cic_paint(tf.zeros_like(initial_conditions),
																										final_state[0])
																	 </code></pre>
														 <ul>
															 <li> Seamless interfacing with deep learning components
															 </li>
															 <!-- <li> <b class="alert">Mesh TensorFlow</b> implementation for distribution on supercomputers
															 </li> -->
														 </ul>
														 <br>
														 <br>
														 <br>
														 <br>
														 <br>
													 </div>

													 <div class='col'>
															   <img data-src="/talks/assets/flowpm.gif"></img>
														 <!-- <div class="fig-container" data-file="flowpm_16.html" data-style="height: 550px;"></div> -->
														 <br>
														 <br>
														 <br>
														 <br>
													 </div>
												 </div>
											 </section>

												   <section>
													   <h3 class='slide-title'>Mesh FlowPM: distributed, GPU-accelerated, and automatically differentiable simulations</h3>
													   <!--
										   <img data-src="/talks/assets/mesh_flopwm.png" class="plain" style="height:450px;" /> -->

													   <div class="container">
														   <div class="col">
															   <img data-src="/talks/assets/mfpm_demo_1024.png" />
														   </div>

														   <div class="col">
															   <ul>
																   <li> We developed a <b class="alert">Mesh TensorFlow</b> implementation that can scale on GPU clusters (horovod+NCCL).
																   </li>
																   <br>
																   <li> For a $2048^3$ simulation:
																	   <ul>
																		   <li>Distributed on <b>256</b> NVIDIA V100 GPUs</li>
																		   <!-- <li>Runtime: 3 mins</li> -->
																	   </ul>
																   </li>
																   <br>
																   <li> Don't hesitate to reach out if you have a use case for model parallelism!<br>
																	   <img data-src="/talks/assets/github.png" class="plain" style="height:70px" /><br>

																	   <div> <a href="https://github.com/DifferentiableUniverseInitiative/mesh">https://github.com/DifferentiableUniverseInitiative/mesh</a>
																	   </div>
																   </li>
																   <br>
																   <li class="fragment">
																	<b>Now developing the next generation of these tools in JAX</b>
																	<ul>
																		<li><a href="https://github.com/eelregit/pmwd">pmwd</a> Differentiable PM library,  (Li et al. 2022) arXiv:2211.09958  </li>
																		<li><a href="https://github.com/DifferentiableUniverseInitiative/jaxDecomp">jaxdecomp</a>: Domain Decomposition and Parallel FFTs</li>
																	</ul>
																   </li>
															   </ul>
														   </div>
													   </div>
												   </section>
												   </section>

												   <section>
													<h2>Hybrid Physical-Neural ODEs for Fast N-body Simulations</h2>
													<a href="https://arxiv.org/abs/2207.05509"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2207.05509-B31B1B.svg" class="plain" style="height:25px;" /></a>
													<a href="https://arxiv.org/abs/2305.07531"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2305.07531-B31B1B.svg" class="plain" style="height:25px;" /></a>
													<hr>
													<div class="container">
														<div class="col">
															<div align="left" style="margin-left: 20px;">
																<h3>Work led by <br>
																	Denise Lanzieri
																</h3>
																<img data-src="https://avatars.githubusercontent.com/u/72620117?v=4" style='width:200px; height:200px;'></img>
									
																<br>
									
																$\Longrightarrow$ <b class="alert">Learn residuals to known physical equations</b> to improve accuracy of fast PM simulations.
															</div>
														</div>
														<div class="col">
															<img class="plain" data-src="/talks/assets/cluster_2D_PM_NN.png" style="width:450px;" />
														</div>
													</div>
													<br>
												</section> 
			
												<section>
													<h3 class='slide-title'>Fill the gap in the accuracy-speed space of PM simulations</h3>
														<div class='container'>
															<div class='col'>
																<div class="plain  current-visible "  data-fragment-index="0">
																	<p>Camels simulations</p>
																	<img  data-src="/talks/assets/cluster_2D_Camels.png" style="height:250px; "></img>
																</div>
																<div class="plain  current-visible "  data-fragment-index="0">
																	<p >PM simulations</p>
																	<img  data-src='/talks/assets/cluster_2D_PM.png' style="height:250px;" />
															 </div>
															</div>
															<div  class='col'>
																<img  data-fragment-index="1" data-src="/talks/assets/comparison_pk_intro.png" class='plain' style="height: 400px; width:700px;" />
														  </div>
													  </div>
												</section>
								
												   <section>
													<section>
															<h3 class="slide-title"> Hybrid physical/neural differential equations</h3>

															<div class="container">
																<div class="col">
																	<div style="float:right; font-size: 20px"> Lanzieri, <b>Lanusse</b>, Starck (2022)
																		<a href="https://arxiv.org/abs/2207.05509"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2207.05509-B31B1B.svg" class="plain"  style="height:25px;vertical-align:middle;"/></a>										
																</div>
																</div>
															</div>
																				$$\left\{ \begin{array}{ll}
																				\frac{d  \color{#6699CC}{\mathbf{x}} }{d a} & = \frac{1}{a^3 E(a)} \color{#6699CC}{\mathbf{v}} \\
																				\frac{d  \color{#6699CC}{\mathbf{v}}}{d a} & =  \frac{1}{a^2 E(a)} F_\theta( \color{#6699CC}{\mathbf{x}} , a), \\
																				F_\theta( \color{#6699CC}{\mathbf{x}}, a) &= \frac{3 \Omega_m}{2}  \nabla \left[ \color{#669900}{\phi_{PM}} (\color{#6699CC}{\mathbf{x}}) \right]
									
																				\end{array} \right. $$
																<ul>
																	<li>   <span style='color:#6699CC'>$\mathbf{x}$</span> and <span style='color:#6699CC'>$\mathbf{v}$</span> define the position and the velocity of the particles
																	</li>
																	<li><span style='color:#669900'>$\phi_{PM}$</span> is the gravitational potential in the mesh
																	</li>
																</ul>
																<br>
																<p  class='fragment' data-fragment-index="1"> $\to$ We can use this parametrisation to complement the physical ODE with neural networks.
																</p>
																<br>
																<p  class='fragment' data-fragment-index="1">
																	$$F_\theta(\mathbf{x}, a) = \frac{3 \Omega_m}{2}  \nabla \left[ \phi_{PM} (\mathbf{x}) \ast  \mathcal{F}^{-1} (1 + \color{#996699}{f_\theta(a,|\mathbf{k}|)}) \right] $$
																</p>
																<br>
																<div class="fragment" data-fragment-index="1" style="position:relative; top:0px; ">Correction integrated as a Fourier-based isotropic filter <span style='color:#996699'>$f_{\theta}$</span> $\to$ incorporates translation and rotation symmetries </div>
													</section>
<!-- 									
													<section>
																<h3 class="slide-title">Learn the Neural Filter</h3>
															<ul>
															  <li> <span style='color:#996699'>$f_{\theta}(a)$</span> is defined as B-spline functions whose coefficients are the output of the Neural Network of parameters $\theta$.
																</li>
														 </ul>
														 <div>
																 <img data-src="/talks/assets/nn_manim.png" class='plain' style="height: 600px; width:950px" />
														 </div>
													</section>
													<section>
														<h3 class="slide-title">Train and validation loss</h3>
														<div class="container">
															<div class="col">
																	<div  >
																	$$\mathcal{L} =  \sum_{i}^{snapshots} \lambda_1||   \color{#6699CC}{\mathbf{x}^{ref}_i} -  \color{#6699CC}{\mathbf{x}_i}||_2^2  + \lambda_2 || \frac{\color{#996699}{p_i(k)}}{\color{#996699}{p_i^{ref}(k)}} -1 ||_2^2 $$
																	</div>
															</div>
															<div class="col">
																<ul>
																	<li >We adopt a loss function penalizing both the <span style='color:#6699CC'>particle positions</span> and the overall <span style='color:#996699'>matter power spectrum</span> at different snapshot times
																	</li>
																	<br>
																	<li > We train and compare the model to the CAMELS simulations <a style="color:#GOLD"; href=" https://arxiv.org/pdf/2010.00619.pdf:">(Villaescusa-Navarro et al., 2021) </a>
																	</li>
																	<br>
																	<li> 	We use a single N-body simulation of $25^3$ ($h^{-1}$ Mpc)$^3$ volume, $64^3$ dark matter particles at the fiducial cosmology of $\Omega_m = 0.3$ and $\sigma_8 = 0.8$
																	</li>
																	<br>
																	<li> Whole code implemented in the Python package <span style='color:#669900'>Jax<span/>.
																	</li>
																</ul>
															</div>
														</div>
													</section>
									
									
													<section>
														<h3 class="slide-title">Backpropagation through the ODE solver</h3>
															We are following the technique from Neural ODEs to <b>backpropagate through an ODE solver</b> (<a style="color:#FFAA7F; font-size: 20px" href="https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf">Neural Ordinary Differential Equations, Chen et al. 2018</a>).
															<br><br>
												
																<div class="block">
																<div class="block-title" style='color:white'>
																 How	optimize a <span style='color:#6699CC'>loss function</span> with input the result of an ODE solver:  <span style='color:#6699CC'>$\textbf{L}$</span>(ODESolve$(\color{#996699}{z}(t_0),f,t_0,t_1,\color{#ecad60}{\theta}))$?
																</div>
																<div class="block-content">
																	<br>
																	 To optimize  <span style='color:#6699CC'>$\textbf{L}$</span>, we require gradients with respect to <span style='color:#ecad60'>$\theta$</span>:
																	<ul>
																	<ol>
																	<br>
																	<li class='fragment' data-fragment-index="0"> Determine how the gradient of the loss (the <span style='color:#669900'>adjoint</span>)  depends on the hidden state <span style='color:#996699'>$z$</span>(t) at each instant:
																		$$\color{#669900}{\textbf{a}}(t)=\frac{\partial \color{#6699CC}{L}}{\partial \color{#996699}{\textbf{z}}(t)}$$
																	</li>
																	<li class='fragment' data-fragment-index="1"> Compute the <span style='color:#669900'>adjoint</span> dynamics by solving a another ODE:
																		$$ \frac{d\color{#669900}{\textbf{a}}(t)}{dt}=\color{#669900}{\textbf{a}}(t)^{T}\frac{\partial f(\color{#996699}{\textbf{z}}(t),t,\color{#ecad60}{\theta})}{\partial \color{#996699}{\textbf{z}}}
																			 $$
																	</li>
																	<li class='fragment' data-fragment-index="2"> Compute the gradients with respect to the parameters $\theta$ evaluating a third integral:
																	$$ \frac{d\color{#6699CC}{L}}{d\color{#ecad60}{\theta}}=\int_{t_1}^{t_0}\color{#669900}{\textbf{a}}(t)^T \frac{\partial f (\color{#996699}{\textbf{z}}(t),t,\theta)}{\partial \color{#ecad60}{\theta}}dt $$
																	</li>
															  </ol>
															  </ul>
															</div>
													</section>
									 -->
													<section>
														<h3 class="slide-title"> Projections of final density field</h3>
														<br>
														<br>
														<div class="container">
															<div class="col">
																<div class="block-content">
																	<div style="position:relative; height:570px; width:700px top:0px; left:0px;">
																		Camels simulations
																		<img data-src="/talks/assets/cluster_2D_Camels.png" style="height:400px;width:1500px"></img>
																	</div>
																</div>
															</div>
															<div class="col">
																<div class="block-content">
																	<div style="position:relative; height:570px; top:0px; left:0px;">
																		<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0">
																			PM simulations
																			<img data-src='/talks/assets/cluster_2D_PM.png' style="height:400px;" />
																		</div>
							
																		<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="1">
																			PM+NN correction
																			<img data-src='/talks/assets/cluster_2D_PM_NN.png' style="height:400px;" />
																		</div>
<!-- 							
																		<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="2">
																			PM+PGD correction
																			<img data-src='/talks/assets/cluster_2D_PM_PGD.png' style="height:400px;" />
																		</div> -->
																	</div>
																</div>
															</div>
														</div>
													</section>

													<section>
														<h3 class="slide-title">Results</h3>
															<br>
															<div >
																<li>
																	Neural network trained using single CAMELS simulation of <span style='color:#996699'>$25^3$ ($h^{-1}$ Mpc)$^3$ volume</span> and <span style='color:#669900'>$64^3$ dark matter particles</span> at the fiducial cosmology of  <span style='color:#6699CC'>$\Omega_m = 0.3$</span>
																</li>
															</div>
															<br><br>
															<div class="container">
																<div class="col">
																	<img data-src="/talks/assets/camels_residual_CV_0.png"/>
																</div>
																<div class="col" >
																	<img style=" position: relative;bottom: 21px;" data-src="/talks/assets/cross_corr_CV_0.png" />
																</div>
															</div>
													</section>
												</section>	
												

								

			<section class="inverted" data-background="#000">
				<h2>How well does explicit inference work?</h2>
			</section>
			
			<section>
				<h3 class="slide-title">Explicit Field level inference for weak lensing (Porqueres et al. 2023)</h3>
				<div class="container">
					<div class="col">
						<div style="float:right; font-size: 20px"> Porqueres, Heavens, Mortlock, Lavaux, Makinen (2023)
							<a href="https://arxiv.org/abs/2304.04785"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2304.04785-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
					</div>
				</div>
				<div class="container">
					<div class="col">
						<img data-src="/talks/assets/porqueres_model.png"></img>

					</div>

					<div class="col">
						<img data-src="/talks/assets/porqueres_contours.png"></img>
					</div>
				</div>
			</section>

							
	   <section>
		   <h2>Conclusion</h2>
	   </section>

	   <section>
		   <h3 class="slide-title"> Conclusion </h3>

		   <br>
		   <br>


		   <div class="block fragment">
			   <div class="block-title">
				   Methodology for inference over simulators
			   </div>
			   <div class="block-content">

				   <ul>
					   <li> A change of paradigm <b class="alert"> from analytic likelihoods to simulators as physical model</b>.
						   <ul>
							   <br>
							   <li> State of the art Machine Learning models enable Likelihood-Free Inference over black-box simulators.
							   </li>

							   <br>

							   <li> Progress in differentiable simulators and inference methodology paves the way to full inference over probabilistic model.
							   </li>
						   </ul>

					   </li>

					   <br>

					   <li> Ultimately, promises optimal exploitation of survey data, although the <b class="alert">"information gap" against analytic likelihoods in realistic settingns remains uncertain.</b>
					   </li>
				   </ul>
			   </div>
		   </div>

		   <br>
		   <br>

		   <div class="fragment">
			   Thank you!
		   </div>
	   </section>


		</div>
	</div>

	<style>
		.reveal .slides {
			border: 5px solid red;
			min-height: 100%;
			width: 128mm;
			height: 96mm;
		} 

		.reveal .block {
			background-color: #191919;
			margin-left: 20px;
			margin-right: 20px;
			text-align: left;
			padding-bottom: 0.1em;
		}

		.reveal .block-title {
			background-color: #333333;
			padding: 8px 35px 8px 14px;
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .block-content {
			padding: 8px 35px 8px 14px;
		}

		.reveal .slide-title {
			border-left: 5px solid white;
			text-align: left;
			margin-left: 20px;
			padding-left: 20px;
		}

		.reveal .alert {
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .inverted {
			filter: invert(100%);
		}

		/*
	/* .reveal .alert {
	padding:8px 35px 8px 14px; margin-bottom:18px;
	text-shadow:0 1px 0 rgba(255,255,255,1);
	border:5px solid #FFAA7F;
	-webkit-border-radius: 14px; -moz-border-radius: 14px;
	border-radius:14px
	background-position: 10px 10px;
	background-repeat: no-repeat;
	background-size: 38px;
	padding-left: 30px; /* 55px; if icon
	}
	.reveal .alert-block {padding-top:14px; padding-bottom:14px}
	.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
	/*.reveal .alert li {margin-top: 1em}
	.reveal .alert-block p+p {margin-top:5px} */
	
	/* Bayes theorem colors */
	.bayes-equation {
		font-size: 1.5em !important;
		padding: 10px;
		background-color: rgba(0, 0, 0, 0.2);
		border-radius: 10px;
	}
	</style>


	<script src="reveal.js/dist/reveal.js"></script>
	<script src="reveal.js/plugin/notes/notes.js"></script>
	<script src="reveal.js/plugin/markdown/markdown.js"></script>
	<script src="reveal.js/plugin/highlight/highlight.js"></script>
	<script src="reveal.js/plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			controls: true,

			//center: false,
			hash: true,

			// Visibility rule for backwards navigation arrows; "faded", "hidden"
			// or "visible"
			controlsBackArrows: 'hidden',

			// Display a presentation progress bar
			progress: true,

			// Display the page number of the current slide
			slideNumber: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// The "normal" size of the presentation, aspect ratio will be preserved
			// when the presentation is scaled to fit different resolutions. Can be
			// specified using percentage units.
			width: 1280,
			height: 720,

			// Factor of the display size that should remain empty around the content
			margin: 0.1,

			// Bounds for smallest/largest possible scale to apply to content
			minScale: 0.2,
			maxScale: 1.5,

			autoPlayMedia: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],

			dependencies: [{
					src: 'reveal.js/plugin/markdown/marked.js'
				},
				{
					src: 'reveal.js/plugin/markdown/markdown.js'
				},
				{
					src: 'reveal.js/plugin/notes/notes.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/math/math.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/reveal.js-d3/reveald3.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/Chart.min.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/csv2chart.js'
				},
				{
					src: 'reveal.js/plugin/highlight/highlight.js',
					async: true
				},
			]

		});
	</script>
</body>

</html>
