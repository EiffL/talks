<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Deep Generative Models for Inverse Problems</title>

	<meta name="description" content="Seminar at Master IMA">
	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<link rel="stylesheet" href="reveal.js/dist/theme/darkenergy.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">

			<section>
				<h1>Deep Generative Models for Inverse Problems</h1>
			</section>

			<section>
				<section data-background-image="/talks/assets/gravitational-lensing-diagram.jpg">
					<h3 class="slide-title"> Gravitational lensing</h3>
					<div class="fragment fade-up">
						<img class="plain" data-src="/talks/assets/great.jpg" />

						<div class="block ">
							<div class="block-title">
								Galaxy shapes as estimators for gravitational shear
							</div>
							<div class="block-content">
								$$ e = \gamma + e_i \qquad \mbox{ with } \qquad e_i \sim \mathcal{N}(0, I)$$
								<ul>
									<li> We are trying the measure the <b class="alert"> ellipticity $e$</b> of
										galaxies as an estimator for the <b class="alert">gravitational shear $\gamma$ </b>
									</li>
								</ul>
							</div>
						</div>
					</div>
				</section>

				<section>
					<h3 class="slide-title">Weak Lensing Mass-Mapping as an Inverse Problem</h3>
					<div class="container">
						<div class="col">
							Shear <b class="alert">$\gamma$</b><br>
							<img data-src="/talks/assets/shear_cat1.png" style="width:450px;"></img>
						</div>

						<div class="col fragment fade-up">
							Convergence <b class="alert">$\kappa$</b><br>
							<img data-src="/talks/assets/kappa.png" style="width:450px;"></img>
						</div>
					</div>

					<div style="position:relative; width:1000px; height:100px; margin:0 auto;">
						<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
							$$\gamma_1 = \frac{1}{2} (\partial_1^2 - \partial_2^2) \ \Psi \quad;\quad \gamma_2 = \partial_1 \partial_2 \ \Psi \quad;\quad \kappa = \frac{1}{2} (\partial_1^2 + \partial_2^2) \ \Psi$$
						</div>
						<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
							$$\boxed{\gamma = \mathbf{P} \kappa}$$
						</div>
					</div>
				</section>

				<section>
					<h3 class="slide-title"> Illustration on the Dark Energy Survey (DES) Y3</h3>
					<div style="float:right; font-size: 20px">Jeffrey, et al. (2021)
					</div><br>
					<img data-src="/talks/assets/DESY3map.png" style="height:600px;"></img>
				</section>
			</section>

			<section>
				<h3 class="slide-title">Linear inverse problems</h3>

				$\boxed{y = \mathbf{A}x + n}$
				<br>
				<br>
				$\mathbf{A}$ is known and encodes our physical understanding of the problem.
				<span class="fragment"><br>$\Longrightarrow$ When non-invertible or ill-conditioned, the inverse problem is ill-posed <b class="alert">with no unique solution $x$</b></span>
				<div class="container fragment fade-up">
					<div class="col">
						<img data-src="/talks/assets/pluto_smooth.png" class="plain"></img>
						Deconvolution
					</div>
					<div class="col">
						<img data-src="/talks/assets/pluto_missing.png" class="plain"></img>
						Inpainting
					</div>
					<div class="col">
						<img data-src="/talks/assets/plutoNoise.png" class="plain"></img>
						Denoising
					</div>
				</div>
			</section>

			<section>
				<section data-vertical-align-top>
					<h3 class="slide-title">What Would a Bayesian Do?</h3>
					$\boxed{y = \mathbf{A}x + n}$
					<br>
					<br>
					The Bayesian view of the problem:
					<br>
					$$ p(x | y) \propto p(y | x) \ p(x) $$
					<ul>
						<li class="fragment fade-up">$p(y | x)$ is the data <b>likelihood</b>, which <b class="alert">contains the physics</b><br>
						</li>
						<br>
						<li class="fragment fade-up">$p(x)$ is the <b>prior</b> knowledge on the solution.</li>
					</ul>
					<br>
					<br>
					<div class="fragment fade-up">
						<ul>With these concepts in hand we can:
							<br>
							<li class="fragment">Estimate for instance the <b>Maximum A Posteriori</b> solution:
								<br>
								$$\hat{x} = \arg\max\limits_x \ \log p(y \ | \ x) + \log p(x)$$
							</li>
							<li class="fragment">Estimate from the <b>full posterior p(x|y)</b> with MCMC or Variational Inference methods.
							</li>
						</ul>
					</div>
					<br>
					<div class="fragment fade-up">
						<h3>How do you choose the prior ?</h3>
					</div>
				</section>

				<section>
					<h3 class="slide-title"> Classical examples of signal priors </h3>
					<div class="container">
						<div class="col">
							Sparse
							<img data-src="/talks/assets/wavelet.png" height="400" class="plain"></img><br>
							$$ \log p(x) = \parallel \mathbf{W} x \parallel_1 $$
						</div>
						<div class="col">
							Gaussian
							<img data-src="/talks/assets/zknj8.jpg" height="400" class="plain"></img>
							$$ \log p(x) = x^t \mathbf{\Sigma^{-1}} x $$
						</div>
						<div class="col">
							Total Variation
							<img data-src="/talks/assets/shepp-Logan.ppm" class="plain"></img>
							$$ \log p(x) = \parallel \nabla x \parallel_1 $$

						</div>
					</div>
				</section>

				<section data-background-image="/talks/assets/convergence.png">
					<h2>But what about this?</h2>
					<br>
					<br>
					<br>
					<div class="fragment"> $\Longrightarrow$ Prior in the form of numerical simulations.</div>

					<!-- <div class="container">
												<div class="col">
													<img data-src="/talks/assets/gal_hsc.png" style="object-fit: cover; width:500px;height:500px;">
													<div class="fragment"> $\Longrightarrow$ Prior in the form of existing data.</div>
												</div>
												<div class="col fragment fade-up">
													<img data-src="/talks/assets/convergence.png" style="object-fit: cover; width:500px;height:500px;">
													<div class="fragment"> $\Longrightarrow$ Prior in the form of numerical simulations.</div>
												</div>
											</div> -->
				</section>
			</section>

			<section class="inverted" data-background="#000">
				<h2> Can we use Deep Learning to embed simulation-driven priors within a <b>physical Bayesian model</b>?</h2>
			</section>

			<section>
				<section>
					<h3 class="slide-title"> What is generative modeling?</h3>
					<br>
					<ul>
						<li>The goal of generative modeling is to <b>learn the distribution $\mathbb{P}$</b>
							from which the <b>training set $X = \{x_0, x_1, \ldots, x_n \}$</b> is drawn.
						</li>
						<br>
						<li class='fragment'> Usually, this means building a parametric model $\mathbb{P}_\theta$
							that tries to be close to $\mathbb{P}$.
						</li>
					</ul>

					<br>
					<div class="container">
						<div class="col fragment fade-up">
							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756538/pasted-from-clipboard.png" class="plain"></img>
							<br>
							True $\mathbb{P}$
						</div>

						<div class="col  fragment fade-up">
							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756539/pasted-from-clipboard.png" class="plain"></img>
							<br>
							Samples $x_i \sim \mathbb{P}$
						</div>

						<div class="col  fragment fade-up">
							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756554/pasted-from-clipboard.png" class="plain"></img>
							<br>
							Model $\mathbb{P}_\theta$
						</div>
					</div>
					<br>
					<br>
					<ul>
						<li class="fragment"> Once trained, you can typically <b>sample from $\mathbb{P}_\theta$</b> and/or <b class="alert">evaluate the likelihood $p_\theta(x)$</b>.
						</li>
					</ul>

				</section>

				<section>
					<h3 class="slide-title">Why isn't it easy?</h3>
					<br>
					<ul>
						<li> The <b class="alert">curse of dimensionality</b> put all points far apart in high dimension
						</li>
					</ul>
					<div class="container">
						<div class="col fragment fade-up">
							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756597/pasted-from-clipboard.png" class="plain"></img>
						</div>

						<div class="col fragment fade-up">
							<img style="height:350px;" data-src="https://developers.google.com/machine-learning/clustering/images/CurseofDimensionality.svg" class="plain"></img>
							<br>Distance between pairs of points drawn from a Gaussian distribution.
						</div>
					</div>

					<br>
					<ul>
						<li class="fragment"><b>Classical methods</b> for estimating probability densities, i.e. Kernel Density Estimation (KDE) start to <b>fail in high dimension</b> because of all the gaps
						</li>
					</ul>
				</section>
			</section>

			<section>
				<h3 class="slide-title"> The evolution of generative models </h3>

				<br> <br> <br>
				<div class='container'>
					<div class='col'>
						<div style="position:relative; width:500px; height:500px; margin:0 auto;">
							<img class="fragment current-visible plain" data-src="/talks/assets/DBN.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="0" />
							<img class="fragment current-visible plain" data-src="/talks/assets/vae_faces.jpg" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="1" />
							<img class="fragment current-visible plain" data-src="/talks/assets/gan-samples-1.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="2" />
							<img class="fragment plain" data-src="/talks/assets/karras2017.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="3" />
						</div>
					</div>

					<div class='col'>
						<ul>
							<li class="fragment" data-fragment-index="0"> Deep Belief Network <br> (Hinton et al. 2006) </li>
							<br>
							<li class="fragment" data-fragment-index="1"> Variational AutoEncoder <br> (Kingma & Welling 2014) </li>
							<br>
							<li class="fragment" data-fragment-index="2"> Generative Adversarial Network <br> (Goodfellow et al. 2014)</li>
							<br>
							<li class="fragment" data-fragment-index="3"> Wasserstein GAN <br> (Arjovsky et al. 2017) </li>
						</ul>
					</div>
				</div>
				<br> <br> <br>
			</section>

			<section>
				<h3 class="slide-title"> A visual Turing test </h3>
				<div class="container">
					<div class="col">
						<img data-src="/talks/assets/samples_pixel_cnn.png" class="plain" style="height: 500px;"></img>
						<br>
						<div class="fragment fade-up" data-fragment-index="0"> Fake PixelCNN samples </div>
					</div>
					<div class="col">
						<img data-src="/talks/assets/sdss5.png" class="plain" style="height: 500px;"></img>
						<br>
						<div class="fragment fade-up" data-fragment-index="0"> Real galaxies from SDSS </div>
					</div>
				</div>
			</section>

			<section>
				<h3 class="slide-title">Getting started with Deep Priors: deep denoising example</h3>
				$$ \boxed{{\color{Orchid} y}  = {\color{SkyBlue} x} + n} $$
				<div class="container">
										<div class="col">
											<div style="position:relative; width:550px; height:550px; margin:0 auto;">
														<img class="fragment current-visible plain" data-src="/talks/assets/points.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
														<div class="fig-container fragment" data-file="dgm_prior_denoising.html" data-style="height: 550px;width: 550px;" style="position:absolute;top:0;left:0;" data-fragment-index="1"></div>
											</div>
											<!-- <img data-src="/talks/assets/points.png"/>
											<div class="fig-container" data-file="dgm_prior_denoising.html" data-style="height: 550px;"></div> -->

										</div>

										<div class="col">
											<ul>
												<li class="fragment" data-fragment-index="0" > Let us assume we have access to examples of $ {\color{SkyBlue} x}$ without noise.</li>
												<br>
												<li class="fragment"  data-fragment-index="1">We learn the <b class="alert">distribution of noiseless data $\log p_\theta(x)$</b> from samples using a deep generative model.</li>
												<br>
												<!-- <li class="fragment"> We measure a noisy ${\color{Orchid} y}$ and we want to estimate a denoised ${\color{SkyBlue} x}$</li>
												<br> -->
												<li class="fragment">The solution should lie on the <b class="alert">realistic data manifold</b>, symbolized by the two-moons distribution.
													<div class="fragment">
													<p> We want to solve for the Maximum A Posterior solution: </p>
													$$\arg \max - \frac{1}{2} \parallel {\color{Orchid} y} - {\color{SkyBlue} x} \parallel_2^2 + \log p_\theta({\color{SkyBlue} x})$$

													This can be done by <b>gradient descent</b> as long as one has access to the <b class="alert">score function</b> $\frac{\color{orange} d \color{orange}\log \color{orange}p\color{orange}(\color{orange}x\color{orange})}{\color{orange} d \color{orange}x}$.
												</div>
												</li>
											</ul>
									</div>
						</div>
						</section>

						<section>
							<h3 class="slide-title">Neural Score Estimation by Denoising Score Matching</h3>

							<ul>
								<li><b>Denoising Score Matching</b>: An optimal <b class="alert">Gaussian denoiser learns the score</b> of a given distribution.
									<ul>
										<li class="fragment fade-up" data-fragment-index="1"> If $x \sim \mathbb{P}$ is corrupted by additional Gaussian noise $u \in \mathcal{N}(0, \sigma^2)$ to yield
											$$x^\prime = x + u$$
										</li>
										<li class="fragment fade-up"> Let's consider a denoiser $r_\theta$ trained under an $\ell_2$ loss:
											$$\mathcal{L}=\parallel x - r_\theta(x^\prime, \sigma) \parallel_2^2$$
										</li>
										<li class="fragment fade-up"> The optimal denoiser $r_{\theta^\star}$ verifies:
											$$\boxed{\boldsymbol{r}_{\theta^\star}(\boldsymbol{x}', \sigma) = \boldsymbol{x}' + \sigma^2 \nabla_{\boldsymbol{x}} \log p_{\sigma^2}(\boldsymbol{x}')}$$
				</li>
				</ul>
				</li>
				</ul>

				<div class='fragment' data-fragment-index="1">
					<div class="container">
						<div class="col">$\boldsymbol{x}'$
						</div>
						<div class="col">$\boldsymbol{x}$
						</div>
						<div class="col">$\boldsymbol{x}'- \boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
						</div>
						<div class="col">$\boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
						</div>
					</div>
					<img data-src="/talks/assets/denoised_mnist.png" style='width:1200px;'></img>
				</div>
			</section>

			<section>
				<section>
					<h3 class="slide-title">Back to the Mass-Mapping Problem</h3>

					$$ \log p( \kappa | e) = \underbrace{\log p(e | \kappa)}_{\simeq -\frac{1}{2} \parallel e - P \kappa \parallel_\Sigma^2} + \log p(\kappa) +cst $$

					<ul>
						<li> The likelihood term is <b class="alert">known analytically</b>.
						</li>

						<li class="fragment fade-up"> There is <b class="alert">no close form expression for the full non-Gaussian prior</b> of the convergence.
							<br> However:
							<ul>
								<li class='fragment'> <b>We do have access to samples of full prior</b> through simulations: $X = \{x_0, x_1, \ldots, x_n \}$ with $x_i \sim \mathbb{P}$
									<img data-src='/talks/assets/plot_massive_nu.png' />
								</li>
							</ul>
						</li>
					</ul>
					<div class="fragment">$\Longrightarrow$ Our strategy: <b class="alert">Learn the prior score $\nabla \log p(\kappa)$</b> with Denoising Score Matching,
						and then <b class="alert">sample the full posterior</b> with annealed HMC.</div>
				</section>

			</section>

			<section>
				<section>
					<h3 class="slide-title">Illustration on $\kappa$-TNG simulations</h3>
					<div>
						$$\nabla_\kappa \log p_\sigma(\kappa |\gamma) = \nabla_\kappa \log p_\sigma(\gamma |\kappa) \quad + \quad \color{orange}{\nabla_\kappa \log p_\sigma(\kappa)}$$
						<img data-src='/talks/assets/hmc-annealing.gif' />
					</div>

				</section>

				<section>
					<h3 class="slide-title">Illustration on $\kappa$-TNG simulations</h3>
					<div class="container">
						<div class="col">
							<img data-src='/talks/assets/ref_ktng.png' style="width:400px; height:400px;" />
							<br>
							True convergence map
						</div>
						<div class="col">
							<div class="block-content">
								<div style="position:relative; width:400px; height:400px; top:10px; left:40px;">
									<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="0">
										<img data-src='/talks/assets/ks_ktng.png' style="width:400px; height:400px;" />
									</div>
									<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="1">
										<img data-src='/talks/assets/wiener_ktng.png' style="width:400px; height:400px;" />
									</div>
									<div class="plain fragment" style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="2">
										<img data-src='/talks/assets/mean_post_ktng.png' style="width:400px; height:400px;" />
									</div>
								</div>
								<div class="block-content">
									<div style="position:relative; width:400px; height:20px; top:50px; left:10px;">
										<div class="fragment current-visible " data-fragment-index="0" style="position:absolute;top:0;left:0;width:400px;">Traditional Kaiser-Squires</div>
										<div class="fragment current-visible " data-fragment-index="1" style="position:absolute;top:0;left:0;width:400px;">Wiener Filter</div>
										<div class="fragment" data-fragment-index="2" style="position:absolute;top:0;left:0;width:400px;">Posterior Mean (ours)</div>
									</div>
								</div>
								<br>
								<br>
							</div>

						</div>
						<div class="col fragment">
							<img data-src='/talks/assets/cropped.gif' style="width:400px; height:400px;" />
							<br>
							Posterior samples
						</div>
					</div>
				</section>
			</section>

			<section>
				<h3 class="slide-title">Probabilistic Mass-Mapping of the HST COSMOS field</h3>
				<img data-src="/talks/assets/cosmos_massmap.png"></img>
				<br>
				<ul>
					<li> COSMOS shear data from <a href=https://ui.adsabs.harvard.edu/abs/2010A%26A...516A..63S/abstract>Schrabback et al. 2010</a>
					</li>
					<br>
					<li> Prior learned from MassiveNuS at fiducial cosmology (320x320 maps at 0.4 arcsec resolution).
					</li>
					<br>
					<li> Known massive X-ray clusters indicated with crosses, along with their redshifts, right pannel shows cutouts of central
						cluster from multiple posterior samples.
					</li>
				</ul>
				<br>
				<br>
			</section>

			<section>
				<h3 class="slide-title">Uncertainty quantification in Magnetic Resonance Imaging (MRI)</h3>
				<div style="float:right; font-size: 20px">Ramzi, Remy, <b>Lanusse</b> et al. 2020 <a href="https://arxiv.org/abs/2011.08698" style='vertical-align:middle; display:inline;'><img
							src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
				</div>
				<br>
				<br>
				$$\boxed{y = \mathbf{M} \mathbf{F} x + n}$$
				<div><video data-autoplay loop="loop" data-src="/talks/assets/knee.mp4" type="video/mp4" style="width: 1280px;" />
				</div>
				<br>

				<br>
				<br>
				<p class="fragment">$\Longrightarrow$ We can see which parts of the image are well constrained by data, and which regions are <b class="alert">uncertain</b>.</p>
			</section>


			<section>
				<h2>Deep Galaxy Deblending</h2>

				<a href="https://arxiv.org/abs/1912.03980"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1912.03980-B31B1B.svg" class="plain" style="height:25px;" /></a>
				<a href="https://www.youtube.com/watch?v=oWOU3qNHoL0"><img src="https://img.shields.io/badge/-youtube-red?logo=youtube&labelColor=grey" class="plain" style="height:25px;" /></a>
				<hr>
				<div class="container">
					<div class="col">
						<div align="left" style="margin-left: 20px;">
							<h3>Work in collaboration with: <br>
								Peter Melchior, Fred Moolekamp
							</h3>

							<br> <br> <br>

							$\Longrightarrow$ Use PixelCNNs as deep priors for deblending
						</div>
					</div>
					<div class="col">
						<img src="/talks/assets/scarlet_data.png" style="width:450px;" />
					</div>
				</div>
				<br>
			</section>

			<section>
				<section data-background="/talks/assets/gal_hsc.png">

				</section>
				<section>
					<h3 class="slide-title">The challenge of galaxy blending</h3>
					<div class="container">
						<div class="col">
							<div style="position:relative; width:480px; height:500px; margin:0 auto;">
								<img class="fragment current-visible plain" data-src="/talks/assets/hsc_deblending_success.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
								<img class="fragment plain" data-src="/talks/assets/hsc_shredded.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
							</div>
							<div class="fragment" data-fragment-index="0" style="float:left; font-size: 20px">Bosch et al. 2017</div>
						</div>
						<div class="col">
							<ul>
								<li class="fragment fade-up" data-fragment-index="0"> In HSC over 60% of all galaxies are blended</li>
								<br>
								<li class="fragment fade-up" data-fragment-index="0"> Important impact on our main cosmological probes</li>
								<br>
								<li class="fragment fade-up" data-fragment-index="1"> Current generation of deblenders does not meet our target requirements</li>
								<br>
								<ul class="fragment fade-up" data-fragment-index="2">
									<li> Existing methods rely on simple assumptions about galaxy profiles, like <i>symmetry</i> or <i>monotonicity</i></li>
								</ul>
							</ul>
						</div>
					</div>

					<div class="fragment fade-up" data-fragment-index="3">
						Deblending is an ill-posed inverse problem, akin to <i>Blind Source Separation</i>. The is no <b>single solution</b>.<br>
						$\Longrightarrow$ Intuitively, the key will to leverage an understanding of how individual <i>galaxies look like</i>.
					</div>
				</section>
			</section>


			<section>
				<section data-background="/talks/assets/gal_hsc.png">
					<h3 class='slide-title'>Can AI solve all of our problems?</h3>
					<div class="fragment">
						<div style="float:right; font-size: 20px">Branched GAN model for deblending <a href="https://arxiv.org/abs/1810.10098">(Reiman & GÃ¶hre, 2018)</a></div>

						<img class="plain" data-src="/talks/assets/Reiman2018_1.png" />
					</div>

					<div class="block fragment">
						<div class="block-title">
							The issue with using deep learning as a <i>black-box</i>
						</div>
						<div class="block-content">
							<ul>
								<li> No explicit control of noise, PSF, depth, number of sources.
									<ul>
										<li> Model would have to be retrained for all observing configurations
										</li>
									</ul>
								</li>
								<li class="fragment"> No guarantees on the network output (e.g. flux preservation, artifacts)
								</li>
								<li class="fragment"> No proper uncertainty quantification.
								</li>
							</ul>
						</div>
					</div>
				</section>

				<section>
					<img class="plain" data-src="/talks/assets/Reiman2018_3.png" />
				</section>
			</section>

			<section>
				<h3 class="slide-title"> The Scarlet algorithm: deblending as an optimization problem</h3>
				<div style="float:right; font-size: 20px">Melchior et al. 2018</div>

				$$ \mathcal{L} = \frac{1}{2} \parallel \mathbf{\Sigma}^{-1/2} (\ Y - P \ast A S \ ) \parallel_2^2 - \sum_{i=1}^K \log p_{\theta}(S_i) + \sum_{i=1}^K g_i(A_i) + \sum_{i=1}^K f_i(S_i)$$

				<div class="container">
					<div class="col">
						<img data-src="/talks/assets/scarlet_data.png" height=450 class="plain"></img>
					</div>

					<div class="col">

						Where for a $K$ component blend:
						<br>
						<ul>
							<li>$P$ is the convolution with the instrumental response</li>
							<br>
							<li>$A_i$ are channel-wise galaxy SEDs, $S_i$ are the morphology models</li>
							<br>
							<li>$\mathbf{\Sigma}$ is the noise covariance</li>
							<br>
							<li><b class="alert">$\log p_\theta$ is a PixelCNN prior</b></li>
							<br>
							<li>$f_i$ and $g_i$ are arbitrary additional non-smooth consraints, e.g. positivity, monotonicity...</li>
						</ul>
					</div>
				</div>

				<span class="fragment fade-up">$\Longrightarrow$ Explicit physical modeling of the observed sky</span>
			</section>


			<section>
				<h3 class="slide-title">PixelCNN: Likelihood-based Autoregressive generative model</h3>

				<div class="container">
					<div class="col">
						Models the probability $p(x)$ of an image $x$ as:
						$$ p_{\theta}(x) = \prod_{i=0}^{n} p_{\theta}(x_i | x_{i-1} \ldots x_0) $$
						<ul>
							<li>Some of the best log-likelihoods on the market.</li>
							<li>Extremely stable during training.</li>
							<li>Slow to sample from.</li>
						</ul>
						<br>
						<br>

						<div class="fragment fade-up">
							<img data-src="/talks/assets/speedup.gif" class="plain"></img>
							<br>
							<div style="float:left; font-size: 20px">Ramachandran et al. 2017</div>
						</div>
					</div>

					<div class="col">
						<img data-src="/talks/assets/pixel_cnn_conv.png" class="plain"></img>
						<div style="float:right; font-size: 20px">van den Oord et al. 2016</div>
					</div>
				</div>
				<br>
				<ul>
					<li class="fragment"> Provides an explicit value for $\log p_\theta(x)$<br>
						$\Longrightarrow$ <b class="alert">Can be used as a Bayesian prior for inverse problems.</b>
					</li>
				</ul>
			</section>

			<section>
				<h3 class="slide-title">Training the morphology prior</h3>

				<div class="container">
					<div class="col">
						<img data-src="/talks/assets/cosmos_training.png" height=450 class="plain"></img>
						<div> Postage stamps of isolated COSMOS galaxies used for training, at WFIRST resolution and fixed fiducial PSF</div>
					</div>

					<div class="col">
						<div class="container fragment fade-in">
							<div class="col">
								isolated galaxy
								<img data-src="/talks/assets/gal_1.png" class="plain"></img>
								<span> $\log p_\theta(x) = 3293.7$ </span>
							</div>

							<div class="col">
								artificial blend
								<img data-src="/talks/assets/gal_2.png" class="plain"></img>
								<span> $\log p_\theta(x) = 3100.5 $ </span>
							</div>
						</div>
					</div>
			</section>

			<section>
				<section>
					<h3 class="slide-title">Scarlet in action</h3>

					<div class="container">
						<div class="col">
							Input blend
							<div style="position:relative; width:480px; height:480px; margin:0 auto;">
								<img data-src="/talks/assets/scar_input.png" class="plain"></img>
							</div>
						</div>

						<div class="col">
							<span class="fragment" data-fragment-index="0">Solution</span>
							<div style="position:relative; width:480px; height:480px; margin:0 auto;">
								<img class="fragment current-visible plain" data-src="/talks/assets/old_rec.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
								<img class="fragment  plain" data-src="/talks/assets/pix_rec.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
							</div>
						</div>

						<div class="col">
							<span class="fragment" data-fragment-index="0">Residuals</span>
							<div style="position:relative; width:480px; height:480px; margin:0 auto;">
								<img class="fragment current-visible plain" data-src="/talks/assets/old_res.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
								<img class="fragment  plain" data-src="/talks/assets/pix_res.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
							</div>
						</div>
					</div>

					<ul>
						<li class="fragment fade-up" data-fragment-index="0">Classic priors (monotonicity, symmetry).</li>
						<br>

						<li class="fragment fade-up" data-fragment-index="1">Deep Morphology prior.</li>
					</ul>

				</section>
				<section>
					<div class="container">
						<div class="col">
							True Galaxy
							<img data-src="/talks/assets/true_input.png" class="plain"></img>
						</div>

						<div class="col">
							Deep Morphology Prior Solution

							<img class=" plain" data-src="/talks/assets/pix_rec2.png" />

						</div>

						<div class="col">
							Monotonicity + Symmetry Solution
							<img class=" plain" data-src="/talks/assets/scar_rec2.png" />
						</div>
					</div>
				</section>
			</section>

			<section>
				<h3 class="slide-title"> Extending to multi-band images</h3>

				<img class=" plain" data-src="/talks/assets/scarlet_hsc.png" />
				<div style="float:right; font-size: 25px"><b>Lanusse</b>, Melchior, Moolekamp (2019)<br>
				</div>
			</section>

		</div>
	</div>

	<style>
		/* .reveal .slides {
			border: 5px solid red;
			min-height: 100%;
			width: 128mm;
			height: 96mm;
		} */

		.reveal .block {
			background-color: #191919;
			margin-left: 20px;
			margin-right: 20px;
			text-align: left;
			padding-bottom: 0.1em;
		}

		.reveal .block-title {
			background-color: #333333;
			padding: 8px 35px 8px 14px;
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .block-content {
			padding: 8px 35px 8px 14px;
		}

		.reveal .slide-title {
			border-left: 5px solid white;
			text-align: left;
			margin-left: 20px;
			padding-left: 20px;
		}

		.reveal .alert {
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .inverted {
			filter: invert(100%);
		}
	</style>


	<script src="reveal.js/dist/reveal.js"></script>
	<script src="reveal.js/plugin/notes/notes.js"></script>
	<script src="reveal.js/plugin/markdown/markdown.js"></script>
	<script src="reveal.js/plugin/highlight/highlight.js"></script>
	<script src="reveal.js/plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			controls: true,

			//center: false,
			hash: true,

			// Visibility rule for backwards navigation arrows; "faded", "hidden"
			// or "visible"
			controlsBackArrows: 'faded',

			// Display a presentation progress bar
			progress: true,

			// Display the page number of the current slide
			slideNumber: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// The "normal" size of the presentation, aspect ratio will be preserved
			// when the presentation is scaled to fit different resolutions. Can be
			// specified using percentage units.
			width: 1280,
			height: 720,

			// Factor of the display size that should remain empty around the content
			margin: 0.1,

			// Bounds for smallest/largest possible scale to apply to content
			minScale: 0.2,
			maxScale: 1.5,


			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],

			dependencies: [{
					src: 'reveal.js/plugin/markdown/marked.js'
				},
				{
					src: 'reveal.js/plugin/markdown/markdown.js'
				},
				{
					src: 'reveal.js/plugin/notes/notes.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/math/math.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/reveal.js-d3/reveald3.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/Chart.min.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/csv2chart.js'
				},
				{
					src: 'reveal.js/plugin/highlight/highlight.js',
					async: true
				},
			]

		});
	</script>
</body>

</html>