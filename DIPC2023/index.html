<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Automatically Differentiable Simulations for Full-Field Cosmological Inference</title>

	<meta name="description" content="DIPC LSS Workshop, May 18th. 2023">
	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<link rel="stylesheet" href="reveal.js/dist/theme/darkenergy.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section data-background-iframe="background.html">
				<div class="container">
					<div class="title" style="border-radius: 20px; background-color:rgba(0, 0, 0, 0.4);">
						<h1>Automatically Differentiable Simulations for Full-Field Cosmological Inference</h1>
					</div>
				</div>
				<hr>
				<div style="border-radius: 20px; background-color:rgba(0, 0, 0, 0);">
					<div class="container">
						<div class="col">
							<div align="left" style="margin-left: 20px;">
								<h2>François Lanusse</h2>
								<br>
								<img src="/talks/assets/CosmoStatDarkBK.png" class="plain"></img>
								<br>
							</div>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<br>
							<img src="/talks/assets/logo_cnrs.png" class="plain" height="150"></img>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<img src="/talks/assets/aim.png" class="plain" height="150"></img>
						</div>
					</div>
					<div> slides at <a href="https://eiffl.github.io/talks/DIPC2023">eiffl.github.io/talks/DIPC2023</a> </div>
				</div>
			</section>

<section>
	<section>
		<h3 class='slide-title'>Full-Field Simulation-Based Inference</h3>
	
		<div class='container'>
			<div class='col'>
				<ul>
					<li data-fragment-index="0" > Instead of trying to analytically evaluate the likelihood $p(x | \theta)$ of
						given summary statistics $x$, let us build a forward model of the full observables.<br>
						$\Longrightarrow$ <b class="alert">The simulator becomes the physical model</b>.
					</li>
					<br>
					<!-- <li class="fragment" data-fragment-index="1"> Each component of the model is now tractable, but at the
						cost of a <b>large number of latent variables</b>.
					</li> -->
				</ul>
	
				<br>
				<br>
	
				<div class="block fragment">
					<div class="block-title">
						Benefits of a forward modeling approach
					</div>
					<div class="block-content">
						<ul>
							<li> Fully exploits the information content of the data
								(aka "full field inference").
							</li>
	
							<br>
							<li> Easy to incorporate systematic effects.
							</li>
							<br>
							<li> Easy to combine multiple cosmological probes by joint simulations.
							</li>
						</ul>
					</div>
				</div>
			</div>
	
			<div class='col'>
				<div style="position:relative; width:600px; height:600px; margin:0 auto;">
					<img class=" plain" data-src="/talks/assets/forward_model.png" style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0" />
					<!-- <img class="fragment plain" data-src="/talks/assets/pgm_lensing.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="1" />
					<div class="fragment" data-fragment-index="1" style="float:right; font-size: 20px">(Schneider et al. 2015)</div> -->
				</div>
			</div>
		</div>
		<div class="fragment">For this talk, let's <b class="alert">ignore the elephant in the room</b>:<br> <b>Do we have reliable enough models for the full complexity of the data?</b></div>
	</section>
	
	<section>
		<h3 class="slide-title">...so why is this not mainstream?</h3>
			<img class="plain" data-src="/talks/assets/lfi_sim.png" style="width:1000px;"/>
	
				<div class="r-stack">
	
					<img class="plain fragment" data-src="/talks/assets/plot_massive_nu.png" style="width:1000px;"/>
	
						<div class="block fragment">
							<div class="block-title">
								The Challenge of Simulation-Based Inference
							</div>
							<div class="block-content">
								$$ p(x|\theta) = \int p(x, z | \theta) dz = \int p(x | z, \theta) p(z | \theta) dz $$
								Where $z$ are <b>stochastic latent variables</b> of the simulator.<br><br>
								$\Longrightarrow$ This <b class="alert">marginal likelihood is intractable</b>!
							</div>
						</div>
					</div>
	</section>
	</section>
	
	<section>
				<br>
				<br>

						<div class="block">
							<div class="block-title">
								How to perform inference over forward simulation models?
							</div>
							<div class="block-content">
								<br>
								<ul>
									<li class="fragment"> <b>Implicit Inference</b>: Treat the simulator as a black-box with only the ability to sample from the joint distribution $(x, \theta) \sim p(x, \theta)$. 
										<br>a.k.a.<br><ul>
											<li> <b>Simulation-Based Inference</b> (SBI)
											</li>
											<li> <b>Likelihood-free inference</b> (LFI)
											</li>
											<li> <b>Approximate Bayesian Computation</b> (ABC)
											</li>
										</ul>
									</li>

									<br>
									<br>

									<li class="fragment"> <b>Explicit Inference</b>: Treat the simulator as a probabilistic model and perform inference over the joint posterior $p(\theta, z | x)$.
										<br>a.k.a.<br><ul>
											<li> <b>Bayesian Hierarchical Modeling</b> (BHM)
											</li>
										</ul>
									</li>
									<br>
								</ul>

							</div>
						</div>
						<br>
						<br>
						<div class="fragment">$\Longrightarrow$ For a given simulation model, both methods <b class="alert">should converge to the same posterior!</b></div>
	</section>

	<section>
		<section>
		   <h3 class="slide-title">Black-box Simulators Define Implicit Distributions</h3>
		   <img class="plain" data-src="/talks/assets/lfi_sim.png" style="width:750px;"/>
		   <ul>
			   <li>A black-box simulator <b class="alert">defines $p(x | \theta)$ as an implicit distribution</b>, you can <b>sample from it</b> but you cannot evaluate it.
			   </li>
			   <li class='fragment'> <b class="alert">Key Idea</b>: Use a <b>parametric distribution model $\mathbb{P}_\varphi$ to approximate the implicit distribution $\mathbb{P}$</b>.
			   </li>
		   </ul>

		   <div class="container">
			   <div class="col fragment fade-up">
				   <img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756538/pasted-from-clipboard.png" class="plain"></img>
				   <br>
				   True $\mathbb{P}$
			   </div>

			   <div class="col  fragment fade-up">
				   <img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756539/pasted-from-clipboard.png" class="plain"></img>
				   <br>
				   Samples $x_i \sim \mathbb{P}$
			   </div>

			   <div class="col  fragment fade-up">
				   <img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756554/pasted-from-clipboard.png" class="plain"></img>
				   <br>
				   Model $\mathbb{P}_\varphi$
			   </div>
		   </div>
	   </section>
<!-- 
	   <section>
		   <h3 class="slide-title">Why isn't it easy?</h3>
		   <br>
		   <ul>
			   <li> The <b class="alert">curse of dimensionality</b> put all points far apart in high dimension
			   </li>
		   </ul>
		   <div class="container">
			   <div class="col fragment fade-up">
				   <img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756597/pasted-from-clipboard.png" class="plain"></img>
			   </div>

			   <div class="col fragment fade-up">
				   <img style="height:350px;" data-src="https://developers.google.com/machine-learning/clustering/images/CurseofDimensionality.svg" class="plain"></img>
				   <br>Distance between pairs of points drawn from a Gaussian distribution.
			   </div>
		   </div>

		   <br>
		   <ul>
			   <li class="fragment"><b>Classical methods</b> for estimating probability densities, i.e. Kernel Density Estimation (KDE) start to <b>fail in high dimension</b> because of all the gaps
			   </li>
		   </ul>
	   </section> -->

	   <!-- <section>
		   <h3 class="slide-title">Deep Learning Approaches to Likelihood-Free Inference</h3>

		   <div class="block fragment">
			   <div class="block-title">
				   A two-steps approach to Likelihood-Free Inference
			   </div>
			   <div class="block-content">
				   <ul>
					   <li> Automatically learn an <b>optimal low-dimensional summary statistic</b>
						   $$y = f_\varphi(x) $$
					   </li>

					   <li class="fragment"> Use Neural Density Estimation to either:
						   <ul>
							   <li>build an <b>estimate $p_\phi$ of the likelihood function $p(y \ | \ \theta)$</b> (Neural Likelihood Estimation)

							   </li>
							   <br>

							   <li>build an <b>estimate $p_\phi$ of the posterior distribution $p(\theta \ | \ y)$</b> (Neural Posterior Estimation)

							   </li>
						   </ul>
					   </li>
				   </ul>
			   </div>
		   </div>
	   </section> -->
	   <!-- </section>

	   <section> -->
	   <section>
		   <h3 class="slide-title">Conditional Density Estimation with Neural Networks</h3>
		   <ul>
			   <li class="fragment fade-up"> I assume a forward model of the observations:
				   \begin{equation}
				   p( x ) = p(x | \theta) \ p(\theta) \nonumber
				   \end{equation}
				   All I ask is the ability to sample from the model, to obtain $\mathcal{D} = \{x_i, \theta_i \}_{i\in \mathbb{N}}$
			   </li>
			   <br>
			   <li class="fragment fade-up"> I am going to assume $q_\phi(\theta | x)$ a <b>parametric conditional density</b>
			   </li>
			   <br>
			   <li class="fragment fade-up">Optimize the parameters $\phi$ of $q_{\phi}$ according to
				   \begin{equation}
				   \min\limits_{\phi} \sum\limits_{i} - \log q_{\phi}(\theta_i | x_i) \nonumber
				   \end{equation}
				   In the limit of <b>large number of samples</b> and <b>sufficient flexibility</b>
				   \begin{equation}
				   \boxed{q_{\phi^\ast}(\theta | x) \approx p(\theta | x)} \nonumber
				   \end{equation}
			   </li>
		   </ul>

		   <div style="position:relative; height:30px; margin-left: 4em;">
			   <div class="fragment current-visible" style="position:absolute;top:0;"> $\Longrightarrow$ One can asymptotically recover the posterior by
				   optimizing a <b>parametric estimator</b> over<br> the <b>Bayesian joint distribution</b>
			   </div>
			   <div class="fragment" style="position:absolute;top:0;"> $\Longrightarrow$ One can asymptotically recover the posterior by
				   optimizing a <b class="alert">Deep Neural Network</b> over<br> a <b class="alert">simulated training set</b>.
			   </div>
		   </div>
	   </section>

	   <!-- <section>
		   <h3 class='slide-title'>Neural Density Estimation</h3>
		   <div class="container">
			   <div class="col">
				   <img class="plain" data-src="/talks/assets/MDN.png" style="height:550px" />
				   <br>
				   <div style="float:left; font-size: 20px">Bishop (1994)</div>
			   </div>
			   <div class="col">

				   <ul>
					   <li> Mixture Density Networks (MDN)
						   \begin{equation}
						   p(\theta | x) = \prod_i \pi_i(x) \ \mathcal{N}\left(\mu_i(x), \ \sigma_i(x) \right) \nonumber
						   \end{equation}
					   </li>
					   <br>

					   <li class="fragment fade-up">Flourishing Machine Learning literature on density estimators
						   <img class="plain" data-src="/talks/assets/glow.png" />
						   <div style="float:right; font-size: 20px">GLOW, (Kingma & Dhariwal, 2018)</div>
					   </li>
				   </ul>
			   </div>
		   </div>
	   </section> -->
<!-- 
	   <section>
		   <h3 class="slide-title">A variety of algorithms</h3>
		   <div style="float:right; font-size: 20px">Lueckmann, Boelts, Greenberg, Gonçalves, Macke (2021) <a href="https://arxiv.org/abs/2101.04653"><img src="https://img.shields.io/badge/stat.ML-arXiv%3A2101.04653-B31B1B.svg" class="plain"
					   style="height:25px;vertical-align:middle;" /></a></div>
		   <img class="plain" data-src="/talks/assets/sbibm_comparison.png"/>

		   <br>
		   <br>
			   A few important points:
			   <br><br>
			   <ul>
				   <li> <b>Amortized</b> inference methods, which estimate $p(\theta | x)$, can greatly speed up posterior estimation once trained.
				   </li>
				   <br>

				   <li> <b>Sequential</b> Neural Posterior/Likelihood Estimation methods can actively sample simulations needed to refine the inference.
				   </li>
			   </ul>
	   </section> -->

		   <section>
			   <h3 class="slide-title">Automated Summary Statistics Extraction</h3>
			 <img class="plain" data-src="/talks/assets/lfi_sim_sum.png" />
				   <ul>
					   <li> Introduce a parametric function $f_\varphi$ to <b class="alert">reduce the dimensionality of the
						   data while preserving information</b>.
					   </li>
				   </ul>
				   <div class="container">
					   <div class="col">
						   <div class="r-stack">
							   <img class="plain fragment"  data-fragment-index="0"  data-src="/talks/assets/mutual_information.png" />
							   <!-- <img class="plain fragment" data-fragment-index="1"  data-src="/talks/assets/imnn.png" /> -->
						   </div>
						   <!-- <div class="fragment" style="float:right; font-size: 15px"  data-fragment-index="1"> Makinen, Charnock, Alsing, Wandelt (2021) <a href="https://arxiv.org/abs/2107.07405"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2107.07405-B31B1B.svg" class="plain"
								   style="height:20px;vertical-align:middle;" /></a></div> -->

					   </div>
					   <div class="col">
									   <div class="block fragment" data-fragment-index="0">
										   <div class="block-title">
											   Information-based loss functions
										   </div>
										   <div class="block-content">
											   <ul>
												   <li> Summary statistics $y$ is sufficient for $\theta$ if
													   $$ I(Y; \Theta) = I(X; \Theta) \Leftrightarrow p(\theta | x ) = p(\theta | y) $$
												   </li>
												   <li class="fragment" > Variational Mutual Information Maximization
													 $$ \mathcal{L} \ = \ \mathbb{E}_{x, \theta} [ \log q_\phi(\theta | f_\varphi(x)) ] \leq  I(Y; \Theta) $$

														   <div style="float:right; font-size: 15px"> Jeffrey, Alsing, <b>Lanusse</b> (2021) <a href="https://arxiv.org/abs/2009.08459"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2009.08459-B31B1B.svg" class="plain"
																	   style="height:20px;vertical-align:middle;" /></a></div>
												   </li>

												   <!-- <li class="fragment" data-fragment-index="1" > Information Maximization Neural Network
													   $$\mathcal{L} \ = \ - | \det \mathbf{F} |  \ \mbox{with} \ \mathbf{F}_{\alpha, \beta} = tr[ \mu_{\alpha}^t C^{-1} \mu_{\beta} ] $$
													   <div style="float:right; font-size: 15px"> Charnock, Lavaux, Wandelt (2018) <a href="https://arxiv.org/abs/1802.03537"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1802.03537-B31B1B.svg" class="plain"
															   style="height:20px;vertical-align:middle;" /></a></div>
												   </li> -->
											   </ul>
										   </div>
									   </div>
					   </div>
				   </div>
		   </section>

	   </section>



	<!-- Hierarchical Bayesian Inference
			- This time we treat the entire simulator as one big bayesian model
		   - To enable inference over this large number of dimensions, you need gradients
			   with HMC or VI
		   - Autodiff gives you easy access to these gradients
		   -
	 -->

		   <section>
		   <section>
			   <h3 class="slide-title"> Simulators as Hierarchical Bayesian Models</h3>

			   <img class="plain" data-src="/talks/assets/lfi_sim.png" style="width:1000px;"/>
			   <ul>
				   <li>If we have access to all latent variables $z$ of the simulator,
					   then the <b class="alert">joint log likelihood $p(x | z, \theta)$ is explicit</b>.
				   </li>

				   <br>

				   <li class="fragment"> We need to infer the joint posterior $p(\theta, z | x)$ before marginalization to
					   yield $p(\theta | x) = \int p(\theta, z | x) dz$.<br>
					   $\Longrightarrow$ Extremely difficult problem as <b>$z$ is typically very high-dimensional</b>.
				   </li>

				   <br>

				   <li class="fragment"> Necessitates inference strategies with <b class="alert">access to gradients of the likelihood</b>.
					   $$\frac{d \log p(x | z, \theta)}{d \theta} \quad ; \quad \frac{d \log p(x | z, \theta)}{d z}  $$
					   For instance: Maximum A Posterior estimation, Hamiltonian Monte-Carlo, Variational Inference.
				   </li>
				   <br>
			   </ul>
		   </section>
</section>


	<section>
	<section>
		<h3 class="slide-title">Example on log-normal lensing simulations</h3>

		<div class="container">
			<div class="col">
				<div class="container">
					<div class="col">
						<img data-src="https://avatars.githubusercontent.com/u/72620117?v=4" style='width:150px; height:150px;'></img>
						<img data-src="/talks/assets/justine.jpeg" style='width:150px; height:150px;'></img>
						<br>
						<small>Denise Lanzieri (left) and Justine Zhegal (right) </small>
					</div>

					<div class="col">
						<img data-src="/talks/assets/github.png" class="plain" style="height:70px" />
						<br>
						<a href="https://github.com/DifferentiableUniverseInitiative/sbi_lens">sbi_lens</a><br>
						JAX-based log-normal lensing simulation package 
					</div>

				</div>
				<img data-src="/talks/assets/mass_map_tomo.png" />
				<ul>
					<li>10x10 deg$^2$ maps at LSST Y10 quality, conditioning the log-normal shift
						parameter on $(\Omega_m, \sigma_8, w_0)$
					</li>
					<br>
					<li class="fragment" data-fragment-index="0"> Infer full-field posterior on cosmology:
						<ul>
							<li> <b>explicitly</b> using an Hamiltonian-Monte Carlo (NUTS) sampler 
								</li>
							<li> <b>implicitly</b> using a learned summary statistics and conditional density estimation.
						</li>
					</li>
				</ul>
				
			</div>
			<div class="col r-stack">
				<img class="fragment current-visible" data-fragment-index="0" data-src="/talks/assets/compare_ff_ps_contour_plot_multi_tomo_bins.png"/>
				<img class="fragment" data-src="/talks/assets/compare_contour_plot_multi_tomo_bins.png"/>
			</div>
		</div>
	</section>


	<section>
		<img data-src="/talks/assets/find-the-difference.png" style="height: 700px;" />
	</section>

	<section>
		<h3 class="slide-title">but explicit inference yields intermediate data products</h3>
		<div class="container">
			<div class="col">
				<img data-src="/talks/assets/noisy_map.png" style="width: 500px;" />
				<br> simulated observed data
			</div>

			<div class="col">
				<video loop="true" data-autoplay data-src="/talks/assets/recovered_field.mp4" style="width: 500px;" ></video>
				<br> <br> <br> posterior samples of the converegence field $\kappa = f(z,\theta)$ with $z \sim p(z, \theta | x)$
			</div>
		</div>
	</section>
	</section>

	<section>
		<h1> Automatically Differentiable Physics </h1>
		<h2> for large-scale explicit inference</h2>
	</section>


	   <section>
		   <h3 class="slide-title">the hammer behind the Deep Learning revolution: Automatic Differentation</h3>

		   <ul>
			   <li class="fragment"> <b>Automatic differentiation</b> allows you to compute analytic derivatives of arbitraty expressions:<br>
				   If I form the expression $y = a * x + b$, it is separated in fundamental ops:
				   $$ y = u + b \qquad u = a * x $$
				   then gradients can be obtained by the chain rule:
				   $$\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u} \frac{ \partial u}{\partial x} = 1 \times a = a$$
			   </li>
			   <br>
			   <li class="fragment"> This is a fundamental tool in Machine Learning, and autodiff frameworks include TensorFlow and PyTorch.
			   </li>
		   </ul>
		   <br>
		   <br>
		   <div class="block fragment">
			   <div class="block-title">
				   Enters JAX: NumPy + Autograd + GPU
			   </div>
			   <div class="block-content">

				   <div class="container">
					   <div class="col">
						   <ul>
							   <li>JAX follows the NumPy api!
								   <pre class="python"><code data-trim data-noescape>
						   import jax.numpy as np
					   </code></pre>
							   </li>
							   <li>Arbitrary order derivatives</li>
							   <li>Accelerated execution on GPU and TPU</li>

						   </ul>
					   </div>
					   <div class="col" align="center">

						   <img data-src="https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png" class="plain" />
					   </div>
				   </div>
	   </section>


	   <section>
		<section>
			<h3 class="slide-title"> jax-cosmo: Finally a differentiable cosmology library, and it's in JAX!</h3>
				  <div class="container">
					  <div class="col">
						  <div style="float:right; font-size: 20px"> Campagne, <b>Lanusse</b>, Zuntz et al. (2023)
							  <a href="https://arxiv.org/abs/2302.05163"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2302.05163-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
						  </div>
					  </div>
				  </div>
			<div class="container">
				<div class="col">
					<img data-src="/talks/assets/github.png" class="plain" style="height:70px" />
					<div> <a href="https://github.com/DifferentiableUniverseInitiative/jax_cosmo/">https://github.com/DifferentiableUniverseInitiative/jax_cosmo</a>
					</div>

					<pre class="python"><code data-trim data-noescape>
						import jax.numpy as np
						import jax_cosmo as jc

						# Defining a Cosmology
						cosmo = jc.Planck15()

						# Define a redshift distribution with smail_nz(a, b, z0)
						nz = jc.redshift.smail_nz(1., 2., 1.)

						# Build a lensing tracer with a single redshift bin
						probe = probes.WeakLensing([nz])

						# Compute angular Cls for some ell
						ell = np.logspace(0.1,3)
						cls = angular_cl(cosmo_jax, ell, [probe])
					</code></pre>

					<div class="block">
						<div class="block-title">
							Current main features
						</div>
						<div class="block-content">
							<ul>
								<li>Weak Lensing and Number counts probes</li>
								<li>Eisenstein & Hu (1998) power spectrum + halofit</li>
								<li>Angular $C_\ell$ under Limber approximation </li>
							</ul>
							<div>$\Longrightarrow$ 3x2pt DES Y1 capable </div>
						</div>
					</div>

				</div>

				<div class="col">
					<img class="plain" data-src="/talks/assets/jc_vs_ccl_lensing.png" />
					<img class="plain" data-src="/talks/assets/jc_vs_ccl_clustering.png" />
					<br>
					Validated against the <a href="https://github.com/LSSTDESC/CCL">DESC Core Cosmology Library</a>
				</div>
			</div>
		</section>

		<section>
			<h3 class="slide-title"> let's compute a Fisher matrix</h3>

			<br>

			$$F = - \mathbb{E}_{p(x | \theta)}[ H_\theta(\log p(x| \theta)) ] $$

			<br>

			<div class="container">
				<div class="col fragment">

					<pre class="python"><code data-trim data-noescape>
	import jax
	import jax.numpy as np
	import jax_cosmo as jc

	# .... define probes, and load a data vector

	def gaussian_likelihood( theta ):
	  # Build the cosmology for given parameters
	  cosmo = jc.Planck15(Omega_c=theta[0], sigma8=theta[1])

	  # Compute mean and covariance
	  mu, cov = jc.angular_cl.gaussian_cl_covariance_and_mean(cosmo,
														ell, probes)
	  # returns likelihood of data under model
	  return jc.likelihood.gaussian_likelihood(data, mu, cov)

	# Fisher matrix in just one line:
	F = - jax.hessian(gaussian_likelihood)(theta)
	</code></pre>
					<a href="https://colab.research.google.com/github/DifferentiableUniverseInitiative/jax_cosmo/blob/master/docs/notebooks/jax-cosmo-intro.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg"
							alt="Open In Colab" class="plain" style="height:25px;" /></a>
				</div>

				<div class="col fragment">
					<img data-src="/talks/assets/Fisher_mat.png" class="plain"><br><br>
				</div>
			</div>

			<ul>
				<li class="fragment"> <b class="alert">No derivatives were harmed by finite differences in the computation of this Fisher!</b> </li>
				<li class="fragment"> Only a small additional compute time compared to one forward evaluation of the model</li>
			</ul>

		</section>


										<section>
											<h3 class="slide-title"> Inference becomes fast and scalable</h3>

											<div class="container">
												<div class="col">

													<ul>
														<li>Current cosmological MCMC chains take <b>days</b>, and typically require access
															to large computer clusters.</li>
														<br>
														<li class="fragment" data-fragment-index="1"><b class="alert">Gradients of the log posterior are required for modern efficient and scalable inference</b> techniques:
															<ul>
																<li>Variational Inference</li>
																<li>Hamiltonian Monte-Carlo</li>
															</ul>
														</li>
														<br>
														<li class="fragment" data-fragment-index="2">In jax-cosmo, we can trivially obtain <b>exact</b> gradients:
															<pre class="python"><code data-trim data-noescape>
										def log_posterior( theta ):
											return gaussian_likelihood( theta ) + log_prior(theta)

										score = jax.grad(log_posterior)(theta)
										</code></pre>
														</li>

														<br>
														<li class="fragment" data-fragment-index="3">On a DES Y1 analysis, we find convergence in 70,000 samples with vanilla HMC, 140,000 with Metropolis-Hastings</li>
													</ul>

												</div>

												<div class="col">
													<div class="fragment" data-fragment-index="3">
														<img data-src="/talks/assets/jc_3x2pt_hmc.png" class="plain" /><br>
														DES Y1 posterior, jax-cosmo HMC vs Cobaya MH <br>(credit: Joe Zuntz)
													</div>
												</div>
											</div>
										</section>
	</section> 


	<section>
		<h3 class='slide-title'>Forward Models in Cosmology</h3>
		<div class="container">
			<div class='col'>
				<img data-src="/talks/assets/fieldinit.png" class="plain" style="height:300px;" />
				<b class="alert"> Linear Field </b>
			</div>
			<div class='col fragment' data-fragment-index='2'>
				<img data-src="/talks/assets/fieldfin.png" class="plain " style="height:300px;" />
				<b class="alert"> Final Dark Matter </b>
			</div>
			<hr style="width: 1px; height: 400px; background: white; border: none;" />
			<div class='col fragment' data-fragment-index='3'>
				<img data-src="/talks/assets/fieldhalo.png" class="plain " style="height:300px;" />
				<b class="alert"> Dark Matter Halos </b>
			</div>
			<div class='col fragment' data-fragment-index='4'>
				<img data-src="/talks/assets/fieldgal.png" class="plain " style="height:300px;" />
				<b class="alert"> Galaxies </b>
			</div>
		</div>
		<div class="container" style="position:relative; width:1000px; height:50px; margin:0 auto;">
			<div class='col fragment' data-fragment-index='2'>
				<font size="10"> $\longrightarrow$ </font> <br>
				<div class="fragment grow" data-fragment-index='5'>N-body simulations </div>
			</div>
			<div class='col fragment' data-fragment-index='3'>
				<font size="10"> $\longrightarrow$ </font> <br> Group Finding <br> algorithms
			</div>
			<div class='col fragment' data-fragment-index='4'>
				<font size="10"> $\longrightarrow$ </font> <br> Semi-analytic &amp <br> distribution models
			</div>
			<!-- 		<div class='fragment' data-fragment-index='2'> N-body simulations <div> -->
			<!-- <div class='fragment' data-fragment-index='3'> Group Finding algorithms <div> -->
			<!-- <div class='fragment' data-fragment-index='4'> Semi-analytic models <div> -->
		</div>
	</section>

			   <!-- <section>
				   <h3 class='slide-title'>the Fast Particle-Mesh scheme for N-body simulations</h3>
				   <b>The idea</b>: approximate gravitational forces by estimating densities on a grid.

				   <div class='container'>
					   <div class='col'>
						   <ul>
							   <li>The numerical scheme:
								   <br>
								   <br>
								   <ul>
									   <li class="fragment" data-fragment-index="1"> Estimate the density of particles on a mesh<br>
										   => compute gravitational forces by FFT
									   </li>

									   <br>

									   <li class="fragment" data-fragment-index="2"> Interpolate forces at particle positions
									   </li>

									   <br>

									   <li class="fragment" data-fragment-index="3"> Update particle velocity and positions, and iterate
									   </li>
								   </ul>
							   </li>
							   <br>

							   <li class='fragment'> Fast and simple, at the cost of approximating short range interactions.
							   </li>

						   </ul>
					   </div>

					   <div class='col'>

						   <div style="position:relative; width:550px; height:550px; margin:0 auto;">
							   <img class="fragment current-visible plain" data-src="/talks/assets/particle_positions_0.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
							   <img class="fragment current-visible plain" data-src="/talks/assets/particle_density_0.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
							   <img class="fragment current-visible plain" data-src="/talks/assets/particle_positions_0.png" style="position:absolute;top:0;left:0;" data-fragment-index="2" />
							   <img class="fragment  plain" data-src="/talks/assets/particle_positions_1.png" style="position:absolute;top:0;left:0;" data-fragment-index="3" />

						   </div>

					   </div>
				   </div>

				   <div class="fragment"> $\Longrightarrow$ Only a series of FFTs and interpolations.</div>
			   </section> -->


												   <section>
										 <section>
												 <h3 class='slide-title'>introducing FlowPM: Particle-Mesh Simulations in TensorFlow</h3>
												 <div class="container">
													 <div class="col">
														 <div style="float:right; font-size: 20px"> Modi, <b>Lanusse</b>, Seljak (2020)
															 <a href="https://arxiv.org/abs/2010.11847"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2010.11847-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
													 </div>
												 </div>
												 <div class='container'>
													 <div class='col'>
														 <img data-src="/talks/assets/github.png" class="plain" style="height:70px" />
														 <img data-src="/talks/assets/TF_FullColor_Horizontal.png" class='plain' style="height: 70px;" />

														 <div> <a href="https://github.com/DifferentiableUniverseInitiative/flowpm">https://github.com/DifferentiableUniverseInitiative/flowpm</a>
														 </div>
														 <pre class="python"><code data-trim data-noescape>
																		 import tensorflow as tf
																		 import flowpm
																		 # Defines integration steps
																		 stages = np.linspace(0.1, 1.0, 10, endpoint=True)

																		 initial_conds = flowpm.linear_field(32,       # size of the cube
																											100,       # Physical size
																											ipklin,    # Initial powerspectrum
																											batch_size=16)

																		 # Sample particles and displace them by LPT
																		 state = flowpm.lpt_init(initial_conds, a0=0.1)

																		 # Evolve particles down to z=0
																		 final_state = flowpm.nbody(state, stages, 32)

																		 # Retrieve final density field
																		 final_field = flowpm.cic_paint(tf.zeros_like(initial_conditions),
																										final_state[0])
																	 </code></pre>
														 <ul>
															 <li> Seamless interfacing with deep learning components
															 </li>
															 <!-- <li> <b class="alert">Mesh TensorFlow</b> implementation for distribution on supercomputers
															 </li> -->
														 </ul>
														 <br>
														 <br>
														 <br>
														 <br>
														 <br>
													 </div>

													 <div class='col'>
															   <img data-src="/talks/assets/flowpm.gif"></img>
														 <!-- <div class="fig-container" data-file="flowpm_16.html" data-style="height: 550px;"></div> -->
														 <br>
														 <br>
														 <br>
														 <br>
													 </div>
												 </div>
											 </section>

												   <section>
													   <h3 class='slide-title'>Mesh FlowPM: distributed, GPU-accelerated, and automatically differentiable simulations</h3>
													   <!--
										   <img data-src="/talks/assets/mesh_flopwm.png" class="plain" style="height:450px;" /> -->

													   <div class="container">
														   <div class="col">
															   <img data-src="/talks/assets/mfpm_demo_1024.png" />
														   </div>

														   <div class="col">
															   <ul>
																   <li> We developed a <b class="alert">Mesh TensorFlow</b> implementation that can scale on GPU clusters (horovod+NCCL).
																   </li>
																   <br>
																   <li> For a $2048^3$ simulation:
																	   <ul>
																		   <li>Distributed on <b>256</b> NVIDIA V100 GPUs</li>
																		   <!-- <li>Runtime: 3 mins</li> -->
																	   </ul>
																   </li>
																   <br>
																   <li> Don't hesitate to reach out if you have a use case for model parallelism!<br>
																	   <img data-src="/talks/assets/github.png" class="plain" style="height:70px" /><br>

																	   <div> <a href="https://github.com/DifferentiableUniverseInitiative/mesh">https://github.com/DifferentiableUniverseInitiative/mesh</a>
																	   </div>
																   </li>
																   <br>
																   <li class="fragment">
																	<b>Now developing the next generation of these tools in JAX</b>
																	<ul>
																		<li><a href="https://github.com/eelregit/pmwd">pmwd</a> Differentiable PM library,  (Li et al. 2022) arXiv:2211.09958  </li>
																		<li><a href="https://github.com/DifferentiableUniverseInitiative/jaxDecomp">jaxdecomp</a>: Domain Decomposition and Parallel FFTs</li>
																	</ul>
																   </li>
															   </ul>
														   </div>
													   </div>
												   </section>
												   </section>

												   <section>
													<section>
															<h3 class="slide-title"> Hybrid physical/neural differential equations</h3>

															<div class="container">
																<div class="col">
																	<div style="float:right; font-size: 20px"> Lanzieri, <b>Lanusse</b>, Starck (2022)
																		<a href="https://arxiv.org/abs/2207.05509"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2207.05509-B31B1B.svg" class="plain"  style="height:25px;vertical-align:middle;"/></a>										
																</div>
																</div>
															</div>
																				$$\left\{ \begin{array}{ll}
																				\frac{d  \color{#6699CC}{\mathbf{x}} }{d a} & = \frac{1}{a^3 E(a)} \color{#6699CC}{\mathbf{v}} \\
																				\frac{d  \color{#6699CC}{\mathbf{v}}}{d a} & =  \frac{1}{a^2 E(a)} F_\theta( \color{#6699CC}{\mathbf{x}} , a), \\
																				F_\theta( \color{#6699CC}{\mathbf{x}}, a) &= \frac{3 \Omega_m}{2}  \nabla \left[ \color{#669900}{\phi_{PM}} (\color{#6699CC}{\mathbf{x}}) \right]
									
																				\end{array} \right. $$
																<ul>
																	<li>   <span style='color:#6699CC'>$\mathbf{x}$</span> and <span style='color:#6699CC'>$\mathbf{v}$</span> define the position and the velocity of the particles
																	</li>
																	<li><span style='color:#669900'>$\phi_{PM}$</span> is the gravitational potential in the mesh
																	</li>
																</ul>
																<br>
																<p  class='fragment' data-fragment-index="1"> $\to$ We can use this parametrisation to complement the physical ODE with neural networks.
																</p>
																<br>
																<p  class='fragment' data-fragment-index="1">
																	$$F_\theta(\mathbf{x}, a) = \frac{3 \Omega_m}{2}  \nabla \left[ \phi_{PM} (\mathbf{x}) \ast  \mathcal{F}^{-1} (1 + \color{#996699}{f_\theta(a,|\mathbf{k}|)}) \right] $$
																</p>
																<br>
																<div class="fragment" data-fragment-index="1" style="position:relative; top:0px; ">Correction integrated as a Fourier-based isotropic filter <span style='color:#996699'>$f_{\theta}$</span> $\to$ incorporates translation and rotation symmetries </div>
													</section>
<!-- 									
													<section>
																<h3 class="slide-title">Learn the Neural Filter</h3>
															<ul>
															  <li> <span style='color:#996699'>$f_{\theta}(a)$</span> is defined as B-spline functions whose coefficients are the output of the Neural Network of parameters $\theta$.
																</li>
														 </ul>
														 <div>
																 <img data-src="/talks/assets/nn_manim.png" class='plain' style="height: 600px; width:950px" />
														 </div>
													</section>
													<section>
														<h3 class="slide-title">Train and validation loss</h3>
														<div class="container">
															<div class="col">
																	<div  >
																	$$\mathcal{L} =  \sum_{i}^{snapshots} \lambda_1||   \color{#6699CC}{\mathbf{x}^{ref}_i} -  \color{#6699CC}{\mathbf{x}_i}||_2^2  + \lambda_2 || \frac{\color{#996699}{p_i(k)}}{\color{#996699}{p_i^{ref}(k)}} -1 ||_2^2 $$
																	</div>
															</div>
															<div class="col">
																<ul>
																	<li >We adopt a loss function penalizing both the <span style='color:#6699CC'>particle positions</span> and the overall <span style='color:#996699'>matter power spectrum</span> at different snapshot times
																	</li>
																	<br>
																	<li > We train and compare the model to the CAMELS simulations <a style="color:#GOLD"; href=" https://arxiv.org/pdf/2010.00619.pdf:">(Villaescusa-Navarro et al., 2021) </a>
																	</li>
																	<br>
																	<li> 	We use a single N-body simulation of $25^3$ ($h^{-1}$ Mpc)$^3$ volume, $64^3$ dark matter particles at the fiducial cosmology of $\Omega_m = 0.3$ and $\sigma_8 = 0.8$
																	</li>
																	<br>
																	<li> Whole code implemented in the Python package <span style='color:#669900'>Jax<span/>.
																	</li>
																</ul>
															</div>
														</div>
													</section>
									
									
													<section>
														<h3 class="slide-title">Backpropagation through the ODE solver</h3>
															We are following the technique from Neural ODEs to <b>backpropagate through an ODE solver</b> (<a style="color:#FFAA7F; font-size: 20px" href="https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf">Neural Ordinary Differential Equations, Chen et al. 2018</a>).
															<br><br>
												
																<div class="block">
																<div class="block-title" style='color:white'>
																 How	optimize a <span style='color:#6699CC'>loss function</span> with input the result of an ODE solver:  <span style='color:#6699CC'>$\textbf{L}$</span>(ODESolve$(\color{#996699}{z}(t_0),f,t_0,t_1,\color{#ecad60}{\theta}))$?
																</div>
																<div class="block-content">
																	<br>
																	 To optimize  <span style='color:#6699CC'>$\textbf{L}$</span>, we require gradients with respect to <span style='color:#ecad60'>$\theta$</span>:
																	<ul>
																	<ol>
																	<br>
																	<li class='fragment' data-fragment-index="0"> Determine how the gradient of the loss (the <span style='color:#669900'>adjoint</span>)  depends on the hidden state <span style='color:#996699'>$z$</span>(t) at each instant:
																		$$\color{#669900}{\textbf{a}}(t)=\frac{\partial \color{#6699CC}{L}}{\partial \color{#996699}{\textbf{z}}(t)}$$
																	</li>
																	<li class='fragment' data-fragment-index="1"> Compute the <span style='color:#669900'>adjoint</span> dynamics by solving a another ODE:
																		$$ \frac{d\color{#669900}{\textbf{a}}(t)}{dt}=\color{#669900}{\textbf{a}}(t)^{T}\frac{\partial f(\color{#996699}{\textbf{z}}(t),t,\color{#ecad60}{\theta})}{\partial \color{#996699}{\textbf{z}}}
																			 $$
																	</li>
																	<li class='fragment' data-fragment-index="2"> Compute the gradients with respect to the parameters $\theta$ evaluating a third integral:
																	$$ \frac{d\color{#6699CC}{L}}{d\color{#ecad60}{\theta}}=\int_{t_1}^{t_0}\color{#669900}{\textbf{a}}(t)^T \frac{\partial f (\color{#996699}{\textbf{z}}(t),t,\theta)}{\partial \color{#ecad60}{\theta}}dt $$
																	</li>
															  </ol>
															  </ul>
															</div>
													</section>
									 -->
													<section>
														<h3 class="slide-title"> Projections of final density field</h3>
														<br>
														<br>
														<div class="container">
															<div class="col">
																<div class="block-content">
																	<div style="position:relative; height:570px; width:700px top:0px; left:0px;">
																		Camels simulations
																		<img data-src="/talks/assets/cluster_2D_Camels.png" style="height:400px;width:1500px"></img>
																	</div>
																</div>
															</div>
															<div class="col">
																<div class="block-content">
																	<div style="position:relative; height:570px; top:0px; left:0px;">
																		<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0">
																			PM simulations
																			<img data-src='/talks/assets/cluster_2D_PM.png' style="height:400px;" />
																		</div>
							
																		<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="1">
																			PM+NN correction
																			<img data-src='/talks/assets/cluster_2D_PM_NN.png' style="height:400px;" />
																		</div>
<!-- 							
																		<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="2">
																			PM+PGD correction
																			<img data-src='/talks/assets/cluster_2D_PM_PGD.png' style="height:400px;" />
																		</div> -->
																	</div>
																</div>
															</div>
														</div>
													</section>

													<section>
														<h3 class="slide-title">Results</h3>
															<br>
															<div >
																<li>
																	Neural network trained using single CAMELS simulation of <span style='color:#996699'>$25^3$ ($h^{-1}$ Mpc)$^3$ volume</span> and <span style='color:#669900'>$64^3$ dark matter particles</span> at the fiducial cosmology of  <span style='color:#6699CC'>$\Omega_m = 0.3$</span>
																</li>
															</div>
															<br><br>
															<div class="container">
																<div class="col">
																	<img data-src="/talks/assets/camels_residual_CV_0.png"/>
																</div>
																<div class="col" >
																	<img style=" position: relative;bottom: 21px;" data-src="/talks/assets/cross_corr_CV_0.png" />
																</div>
															</div>
													</section>
												</section>	
												
							
													<section>
													<section>
														<h3 class='slide-title'>Forward Models in Cosmology</h3>
														<div class="container">
															<div class='col'>
																<img data-src="/talks/assets/fieldinit.png" class="plain" style="height:300px;" />
																<b class="alert"> Linear Field </b>
															</div>
															<div class='col ' data-fragment-index='2'>
																<img data-src="/talks/assets/fieldfin.png" class="plain " style="height:300px;" />
																<b class="alert"> Final Dark Matter </b>
															</div>
															<hr style="width: 1px; height: 400px; background: white; border: none;" />
															<div class='col ' data-fragment-index='3'>
																<img data-src="/talks/assets/fieldhalo.png" class="plain " style="height:300px;" />
																<b class="alert"> Dark Matter Halos </b>
															</div>
															<div class='col ' data-fragment-index='4'>
																<img data-src="/talks/assets/fieldgal.png" class="plain " style="height:300px;" />
																<b class="alert"> Galaxies </b>
															</div>
														</div>
														<div class="container" style="position:relative; width:1000px; height:50px; margin:0 auto;">
															<div class='col ' data-fragment-index='2'>
																<font size="10"> $\longrightarrow$ </font> <br>
																<div class=" grow" data-fragment-index='5'>N-body simulations </div>
															</div>
															<div class='col ' data-fragment-index='3'>
																<font size="10"> $\longrightarrow$ </font> <br> Group Finding <br> algorithms
															</div>
															<div class='col fragment grow' data-fragment-index='4'>
																<font size="10"> $\longrightarrow$ </font> <br> HOD models
															</div>
															<!-- 		<div class='fragment' data-fragment-index='2'> N-body simulations <div> -->
															<!-- <div class='fragment' data-fragment-index='3'> Group Finding algorithms <div> -->
															<!-- <div class='fragment' data-fragment-index='4'> Semi-analytic models <div> -->
														</div>
													</section>
		<section>
			<h3 class="slide-title">Differentiable sampling from Halo Occupation Distributions</h3>
			<div class="container">
				<div class="col">
					<div style="float:right; font-size: 20px"> Horowitz, Hahn, <b>Lanusse</b>, Modi, Ferraro (2022)
						<a href="https://arxiv.org/abs/2211.03852"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2211.03852-B31B1B.svg" class="plain"  style="height:25px;vertical-align:middle;"/></a>										
				</div>
				</div>

			</div>

			<div class="container">
				<div class="col">
					<img data-src="/talks/assets/github.png" class="plain" style="height:70px" />
					<br>
					<a href="https://github.com/DifferentiableUniverseInitiative/DHOD">https://github.com/DifferentiableUniverseInitiative/DHOD</a><br>
				
					<br>
					<ul>
						<li>Sampling from a discrete random distribution is classically not a differentiable operation
						</li>
						
						<li class="fragment"> <b class="alert">Relaxed and reparameterized HOD sampling</b>
							<ul>
								<li>Relaxed Bernoulli distributions for centrals<br>
									$$ N_{\rm cen} = \frac{1}{1 + \exp( - (\log(\frac{p}{1 - p}) + \epsilon)/\tau) } \mbox{ with } \epsilon \sim \mathrm{Logistic}(0,1) \;. $$
									where $\tau$ is a temperature parameter
								</li>
								<li>Relaxed Binomial distribution for satelittes<br>
									$$ N_{\rm sat} \sim \mathrm{Binomial}\left(N, p=\frac{ \left\langle N_{\rm sat}\right\rangle }{N} \right)$$
								</li>
								
							</ul>
						</li>
					</ul>

				</div>

				<div class="col">
					<img data-src="/talks/assets/relaxed_hod.png" style="height:600px;"/>
				</div>
			</div>

		</section>
	</section>


	   <section>
		   <h2>Conclusion</h2>
	   </section>

	   <section>
		   <h3 class="slide-title"> Conclusion </h3>

		   <br>
		   <br>


		   <div class="block fragment">
			   <div class="block-title">
				   Methodology for inference over simulators
			   </div>
			   <div class="block-content">

				   <ul>
					   <li> A change of paradigm <b class="alert"> from analytic likelihoods to simulators as physical model</b>.
						   <ul>
							   <br>
							   <li> State of the art Machine Learning models enable Likelihood-Free Inference over black-box simulators.
							   </li>

							   <br>

							   <li> Progress in differentiable simulators and inference methodology paves the way to full inference over probabilistic model.
							   </li>
						   </ul>

					   </li>

					   <br>

					   <li> Ultimately, promises optimal exploitation of survey data, although the <b class="alert">"information gap" against analytic likelihoods in realistic settingns remains uncertain.</b>
					   </li>
				   </ul>
			   </div>
		   </div>

		   <br>
		   <br>

		   <div class="fragment">
			   Thank you!
		   </div>
	   </section>


		</div>
	</div>

	<style>
		/* .reveal .slides {
			border: 5px solid red;
			min-height: 100%;
			width: 128mm;
			height: 96mm;
		}  */

		.reveal .block {
			background-color: #191919;
			margin-left: 20px;
			margin-right: 20px;
			text-align: left;
			padding-bottom: 0.1em;
		}

		.reveal .block-title {
			background-color: #333333;
			padding: 8px 35px 8px 14px;
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .block-content {
			padding: 8px 35px 8px 14px;
		}

		.reveal .slide-title {
			border-left: 5px solid white;
			text-align: left;
			margin-left: 20px;
			padding-left: 20px;
		}

		.reveal .alert {
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .inverted {
			filter: invert(100%);
		}

		/*
	/* .reveal .alert {
	padding:8px 35px 8px 14px; margin-bottom:18px;
	text-shadow:0 1px 0 rgba(255,255,255,1);
	border:5px solid #FFAA7F;
	-webkit-border-radius: 14px; -moz-border-radius: 14px;
	border-radius:14px
	background-position: 10px 10px;
	background-repeat: no-repeat;
	background-size: 38px;
	padding-left: 30px; /* 55px; if icon
	}
	.reveal .alert-block {padding-top:14px; padding-bottom:14px}
	.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
	/*.reveal .alert li {margin-top: 1em}
	.reveal .alert-block p+p {margin-top:5px} */
	</style>


	<script src="reveal.js/dist/reveal.js"></script>
	<script src="reveal.js/plugin/notes/notes.js"></script>
	<script src="reveal.js/plugin/markdown/markdown.js"></script>
	<script src="reveal.js/plugin/highlight/highlight.js"></script>
	<script src="reveal.js/plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			controls: true,

			//center: false,
			hash: true,

			// Visibility rule for backwards navigation arrows; "faded", "hidden"
			// or "visible"
			controlsBackArrows: 'hidden',

			// Display a presentation progress bar
			progress: true,

			// Display the page number of the current slide
			slideNumber: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// The "normal" size of the presentation, aspect ratio will be preserved
			// when the presentation is scaled to fit different resolutions. Can be
			// specified using percentage units.
			width: 1280,
			height: 720,

			// Factor of the display size that should remain empty around the content
			margin: 0.1,

			// Bounds for smallest/largest possible scale to apply to content
			minScale: 0.2,
			maxScale: 1.5,

			autoPlayMedia: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],

			dependencies: [{
					src: 'reveal.js/plugin/markdown/marked.js'
				},
				{
					src: 'reveal.js/plugin/markdown/markdown.js'
				},
				{
					src: 'reveal.js/plugin/notes/notes.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/math/math.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/reveal.js-d3/reveald3.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/Chart.min.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/csv2chart.js'
				},
				{
					src: 'reveal.js/plugin/highlight/highlight.js',
					async: true
				},
			]

		});
	</script>
</body>

</html>
